{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a239eea-e414-412d-9242-d7316e9cc18d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a7fcd7-eb6b-4ee8-8f57-ff5a443c78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch==1.13.1 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6eb2fe-f515-471e-8ae3-1aae649d697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c866a9c-6192-4cf1-b8e7-1b96c98953f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1961ba0b-d202-4477-8004-63b1c5e5ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 16 16:00:55 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   33C    P8    26W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a20d7-05ea-4951-a079-08c8b8a71940",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16825bfc-8853-445c-9e6c-53395bea41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "import subprocess\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedGroupKFold, GroupKFold\n",
    "from sklearn.metrics import f1_score, mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6e65d9-657c-4b4d-a20c-ccd0dc3f5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b9b22b-0a41-44b0-beec-e050afbeaa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR\n",
    "from torch import LongTensor\n",
    "from torch import nn, optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4c3033-b975-4aac-bc6b-24346ea9c6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "# from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm, trange\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict, Dataset as HFDataset\n",
    "from datasets import concatenate_datasets, interleave_datasets\n",
    "from datasets import ClassLabel, load_metric\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a120b4-51ef-49dc-9748-873fbd66e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext watermark\n",
    "# %watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563effdb-c378-45ba-8f7e-8d7cc8bf8b08",
   "metadata": {},
   "source": [
    "# Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db742f8-2be3-47d5-bd7b-82629ce77866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_warnings(strict=False):\n",
    "\twarnings.simplefilter('ignore')\n",
    "\tif strict:\n",
    "\t\tlogging.disable(logging.WARNING)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "\trandom.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c01b0cb9-83b7-46c0-8383-c9384772e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "disable_warnings()\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7fe6b-98d6-4d75-9912-bf1bf44b6643",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe357fad-adaf-4b32-8c8a-3bfba4d57170",
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = '../masakhane-pos/data/'\n",
    "add_path = '../masakhane-pos/transfer_corpus/'\n",
    "pseudo_path = './pseudos/pos-ner-afro-xlmr-large-75L-best.csv'\n",
    "\n",
    "langs = sorted(os.listdir(path))\n",
    "add_langs = os.listdir(add_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5bd3e9-2a48-4227-af3e-24fde850a994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang2family = {\n",
    " 'bam': 'Mande',\n",
    " 'bbj': 'Grassfields',\n",
    " 'ewe': 'Kwa',\n",
    " 'fon': 'Volta-Niger',\n",
    " 'hau': 'Chadic',\n",
    " 'ibo': 'Volta-Niger',\n",
    " 'kin': 'Bantu',\n",
    " 'lug': 'Bantu',\n",
    " 'mos': 'Gur',\n",
    " 'nya': 'Bantu',\n",
    " 'pcm': 'English-Creole',\n",
    " 'sna': 'Bantu',\n",
    " 'swa': 'Bantu',\n",
    " 'twi': 'Kwa',\n",
    " 'wol': 'Senegambia',\n",
    " 'xho': 'Bantu',\n",
    " 'yor': 'Volta-Niger',\n",
    " 'zul': 'Bantu'\n",
    "}\n",
    "\n",
    "lang2region = {\n",
    "    'bam': 'West',\n",
    "    'bbj': 'Central',\n",
    "    'ewe': 'West',\n",
    "    'fon': 'West',\n",
    "    'hau': 'West',\n",
    "    'ibo': 'West',\n",
    "    'kin': 'East',\n",
    "    'lug': 'East',\n",
    "    'mos': 'West',\n",
    "    'nya': 'East',\n",
    "    'pcm': 'West',\n",
    "    'sna': 'South',\n",
    "    'swa': 'East',\n",
    "    'twi': 'South',\n",
    "    'wol': 'West',\n",
    "    'xho': 'South',\n",
    "    'yor': 'West',\n",
    "    'zul': 'South'\n",
    "}\n",
    "\n",
    "mapper = {'NOUN': 0,\n",
    " 'ADJ': 1,\n",
    " 'PUNCT': 2,\n",
    " 'CCONJ': 3,\n",
    " 'PRON': 4,\n",
    " 'ADV': 5,\n",
    " 'AUX': 6,\n",
    " 'VERB': 7,\n",
    " 'ADP': 8,\n",
    " 'PART': 9,\n",
    " 'SCONJ': 10,\n",
    " 'PROPN': 11,\n",
    " 'X': 12,\n",
    " 'NUM': 13,\n",
    " 'INTJ': 14,\n",
    " 'SYM': 15,\n",
    " 'DET': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d2ead2-2302-480b-82b6-37346e9e07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2fix = {\n",
    "    'yo': 'yor',\n",
    "    'bm': 'bam'\n",
    "}\n",
    "\n",
    "def load_lang_data(data_path, lang, split='train'):\n",
    "    data = []\n",
    "    language = lang.split('_')[0]\n",
    "    language = lang2fix.get(language, language)\n",
    "    \n",
    "    try:\n",
    "        with open(f'{data_path}{lang}/{split}.txt', 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                splits = line.split(' ')\n",
    "                if len(splits) != 2:\n",
    "                    continue\n",
    "                data.append(splits)\n",
    "        data = np.array(data).squeeze()\n",
    "        data = pd.DataFrame({\n",
    "            'Word': data[:, 0],\n",
    "            'Pos': data[:, 1],\n",
    "            'Language': language.split('_')[0]\n",
    "        })\n",
    "        data['family'] = lang2family.get(language, language)\n",
    "        data['region'] = lang2region.get(language, language)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d23f39a4-6053-418d-b865-aaf8f4af5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lang_data_ner(data_path, lang, split='train'):\n",
    "    data = []\n",
    "    language = lang.split('_')[0]\n",
    "    language = lang2fix.get(language, language)\n",
    "    \n",
    "    try:\n",
    "        with open(f'{data_path}{lang}/{split}.txt', 'r') as f:\n",
    "            sentences, labels = [], []\n",
    "            \n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                splits = line.split(' ')\n",
    "                \n",
    "                if len(splits) == 2:\n",
    "                    sentences.append(splits[0])\n",
    "                    labels.append(splits[1])\n",
    "                elif len(splits) == 1:\n",
    "                    data.append(\n",
    "                        [\n",
    "                            ' '.join(sentences),\n",
    "                            ' '.join(labels),\n",
    "                        ]\n",
    "                    )\n",
    "                    \n",
    "                    sentences, labels = [], []\n",
    "\n",
    "        data = np.array(data).squeeze()\n",
    "        data = pd.DataFrame({\n",
    "            'Word': data[:, 0],\n",
    "            'Pos': data[:, 1],\n",
    "            'Language': language.split('_')[0]\n",
    "        })\n",
    "        data['family'] = lang2family.get(language, language)\n",
    "        data['region'] = lang2region.get(language, language)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "378e3114-de98-4f91-a64f-e7ebf677b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):  \n",
    "    # pos = df.Pos.unique()\n",
    "    use_pos = ['PUNCT', 'SYM']\n",
    "    \n",
    "    df_others = df[~df.Pos.isin(use_pos)]\n",
    "    df_toclean = df[df.Pos.isin(use_pos)]\n",
    "    \n",
    "    dfs = [df_others]\n",
    "    \n",
    "    for p in use_pos:\n",
    "        df_p = df_toclean[df_toclean.Pos == p]\n",
    "        words = df_p.Word.value_counts()[df_p.Word.value_counts() < 2].index.values\n",
    "        df_p = df_p[~df_p.Word.isin(words)]\n",
    "        \n",
    "        dfs.append(df_p)\n",
    "        \n",
    "    return pd.concat(dfs).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f54e1e-1c04-40a8-b297-d7bf05d901d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_data(with_pseudo=False):\n",
    "    train = pd.concat([load_lang_data_ner(path, lang, split) for lang in langs for split in ['train', 'dev', 'test']], axis=0).dropna().reset_index(drop=True)\n",
    "    add_data = (\n",
    "        pd.concat([load_lang_data_ner(add_path, lang, split) for lang in add_langs for split in ['train', 'dev', 'test']], axis=0)\n",
    "        .dropna()\n",
    "        .drop_duplicates(subset=['Word', 'Pos'], keep='first')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    train = pd.concat([train, add_data])\n",
    "    if with_pseudo:\n",
    "        train = pd.concat([train, pd.read_csv(pseudo_path)])\n",
    "    train = train.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return train\n",
    "\n",
    "def load_pos_data():\n",
    "    train = pd.concat([load_lang_data(path, lang, split) for lang in langs for split in ['train', 'dev', 'test']], axis=0).dropna().reset_index(drop=True)\n",
    "    add_data = (\n",
    "        pd.concat([load_lang_data(add_path, lang, split) for lang in add_langs for split in ['train', 'dev', 'test']], axis=0)\n",
    "        .dropna()\n",
    "        .drop_duplicates(subset=['Word', 'Pos'], keep='first')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    train = pd.concat([train, add_data])\n",
    "    train = train[train.Pos.isin(list(mapper.keys()))]\n",
    "\n",
    "    train = train.drop_duplicates().drop_duplicates(subset=['Word', 'Language'], keep='first').reset_index(drop=True)\n",
    "    # train = train.drop_duplicates().reset_index(drop=True)\n",
    "    # train = clean_data(train)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd40436e-a4f4-4823-99b3-8ff6c05636de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muso</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ŋana</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afiriki</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tilebinyanfan</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word    Pos Language family region\n",
       "0           Muso   NOUN      bam  Mande   West\n",
       "1           ŋana    ADJ      bam  Mande   West\n",
       "2              ,  PUNCT      bam  Mande   West\n",
       "3        Afiriki   NOUN      bam  Mande   West\n",
       "4  tilebinyanfan   NOUN      bam  Mande   West"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_pseudo = True\n",
    "\n",
    "train_ner = load_ner_data(with_pseudo)\n",
    "train = load_pos_data()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fc60dcc-029f-4508-85ac-070e6a79b23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muso ŋana , Afiriki tilebinyanfan n' a cɛmancɛ...</td>\n",
       "      <td>NOUN ADJ PUNCT NOUN NOUN CCONJ PRON NOUN ADV C...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ni mɔgɔ ka dɔgɔn kojugu , i bɛ mɔgɔw juguya mi...</td>\n",
       "      <td>CCONJ NOUN PART ADJ NOUN PUNCT PRON AUX NOUN N...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A kɔrɔtalen a ka taɲɛ fɛ kow ɲɛmɔgɔyabaaraw la...</td>\n",
       "      <td>PRON VERB PRON PART NOUN PART NOUN NOUN PART C...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ale y' a ( basikɛti ) to a ka se ka bɔ a ka so...</td>\n",
       "      <td>PRON PART PRON PUNCT NOUN PUNCT VERB PRON PART...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sannayɛlɛn galabukɛnɛya a sera ka min fiyɛ Afi...</td>\n",
       "      <td>NOUN NOUN PRON VERB PART PRON VERB PROPN NOUN ...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Muso ŋana , Afiriki tilebinyanfan n' a cɛmancɛ...   \n",
       "1  Ni mɔgɔ ka dɔgɔn kojugu , i bɛ mɔgɔw juguya mi...   \n",
       "2  A kɔrɔtalen a ka taɲɛ fɛ kow ɲɛmɔgɔyabaaraw la...   \n",
       "3  Ale y' a ( basikɛti ) to a ka se ka bɔ a ka so...   \n",
       "4  Sannayɛlɛn galabukɛnɛya a sera ka min fiyɛ Afi...   \n",
       "\n",
       "                                                 Pos Language family region  \n",
       "0  NOUN ADJ PUNCT NOUN NOUN CCONJ PRON NOUN ADV C...      bam  Mande   West  \n",
       "1  CCONJ NOUN PART ADJ NOUN PUNCT PRON AUX NOUN N...      bam  Mande   West  \n",
       "2  PRON VERB PRON PART NOUN PART NOUN NOUN PART C...      bam  Mande   West  \n",
       "3  PRON PART PRON PUNCT NOUN PUNCT VERB PRON PART...      bam  Mande   West  \n",
       "4  NOUN NOUN PRON VERB PART PRON VERB PROPN NOUN ...      bam  Mande   West  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1e73767-4a99-4f9f-b4dc-7c7d6eeb8ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "      <th>Language</th>\n",
       "      <th>Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Id00qog2f11n_0</td>\n",
       "      <td>Ne</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Id00qog2f11n_1</td>\n",
       "      <td>otim</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Id00qog2f11n_2</td>\n",
       "      <td>penj</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Id00qog2f11n_3</td>\n",
       "      <td>e</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Id00qog2f11n_4</td>\n",
       "      <td>kind</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  Word Language  Pos\n",
       "0  Id00qog2f11n_0    Ne      luo  NaN\n",
       "1  Id00qog2f11n_1  otim      luo  NaN\n",
       "2  Id00qog2f11n_2  penj      luo  NaN\n",
       "3  Id00qog2f11n_3     e      luo  NaN\n",
       "4  Id00qog2f11n_4  kind      luo  NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../Test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07d27197-1217-4279-bffa-8052260878dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((255140, 5), (104039, 5), (32045, 4))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, train_ner.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2114cb1d-b750-4c81-b4e9-cdf1b4d7bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = train_ner[train_ner.Language.isin(langs)].reset_index(drop=True)\n",
    "add_train = train_ner[~train_ner.Language.isin(langs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "989646fd-faa8-4d30-92a5-db49b4e2518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['fold'] = -1\n",
    "train_ner['fold'] = -1\n",
    "\n",
    "\n",
    "# val_langs = ['fon', 'pcm', 'twi', 'xho']\n",
    "# train.loc[train.Language.isin(val_langs), 'fold'] = 0\n",
    "\n",
    "base_train['fold'] = base_train['Language'].map(dict(zip(langs, range(len(langs)))))\n",
    "base_train.loc[base_train['fold'].isna(), 'fold'] = -1\n",
    "base_train['fold'] = base_train['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12da99ae-93ad-4f4a-9f4d-87e539c55ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    10338\n",
       "1     6908\n",
       "2     6541\n",
       "3     6133\n",
       "4     7515\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_fold = 5\n",
    "# Fold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "# Fold = StratifiedGroupKFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "Fold = GroupKFold(n_splits=n_fold)\n",
    "\n",
    "base_train['fold'] = -1\n",
    "# for n, (train_index, val_index) in enumerate(Fold.split(train, train['region'], groups=train['Language'])):\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(base_train, base_train['Pos'], groups=base_train['Language'])):\n",
    "    base_train.loc[val_index, 'fold'] = int(n)\n",
    "    \n",
    "base_train['fold'] = base_train['fold'].astype(int)\n",
    "\n",
    "display(base_train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b0cabda-60df-4c10-bb88-93fcd7cddafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bam', 'bbj', 'ewe', 'fon', 'hau', 'ibo', 'kin', 'lug', 'mos',\n",
       "       'nya', 'pcm', 'sna', 'swa', 'twi', 'wol', 'xho', 'yor', 'zul',\n",
       "       'en', 'fr', 'eng-ron-wol-sna', 'ar', 'af', 'luo', 'tsn'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.Language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6f2731d-47d6-4b01-814b-bd5a8d97bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_train['fold'] = -1\n",
    "base_train.loc[base_train.Language.isin(['sna', 'wol']), 'fold'] = -1\n",
    "# base_train.loc[base_train.Language.isin(['sna', 'bam', 'pcm', 'yor', 'wol']), 'fold'] = -1\n",
    "\n",
    "train_ner = pd.concat([base_train, add_train]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef6675b1-ada8-436b-829d-7d24d62d9382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Make anibodi wey dey call say make dem impeach...</td>\n",
       "      <td>VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...</td>\n",
       "      <td>pcm</td>\n",
       "      <td>English-Creole</td>\n",
       "      <td>West</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وزير كورى : حل القضية النووية مع كوريا الشمالي...</td>\n",
       "      <td>NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mano nyalo konyi ahinya e yudo tich .</td>\n",
       "      <td>PRON VERB VERB ADV ADP VERB NOUN PUNCT</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Son nom d' espèce , composé de loyalti et de l...</td>\n",
       "      <td>DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les lemmings sont des êtres expressifs , capab...</td>\n",
       "      <td>DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Make anibodi wey dey call say make dem impeach...   \n",
       "1  وزير كورى : حل القضية النووية مع كوريا الشمالي...   \n",
       "2              Mano nyalo konyi ahinya e yudo tich .   \n",
       "3  Son nom d' espèce , composé de loyalti et de l...   \n",
       "4  Les lemmings sont des êtres expressifs , capab...   \n",
       "\n",
       "                                                 Pos Language          family  \\\n",
       "0  VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...      pcm  English-Creole   \n",
       "1  NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...       ar              ar   \n",
       "2             PRON VERB VERB ADV ADP VERB NOUN PUNCT      luo             luo   \n",
       "3  DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...       fr              fr   \n",
       "4  DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...       fr              fr   \n",
       "\n",
       "  region  fold  \n",
       "0   West     0  \n",
       "1     ar    -1  \n",
       "2    luo    -1  \n",
       "3     fr    -1  \n",
       "4     fr    -1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7a768de-2e21-4900-b470-ea68408a58ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th>Language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">-1</th>\n",
       "      <th>af</th>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng-ron-wol-sna</th>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luo</th>\n",
       "      <td>7244</td>\n",
       "      <td>7244</td>\n",
       "      <td>7244</td>\n",
       "      <td>7244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sna</th>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsn</th>\n",
       "      <td>4936</td>\n",
       "      <td>4936</td>\n",
       "      <td>4936</td>\n",
       "      <td>4936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wol</th>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>pcm</th>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>bam</th>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbj</th>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nya</th>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zul</th>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
       "      <th>ewe</th>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hau</th>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mos</th>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yor</th>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>fon</th>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kin</th>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">4</th>\n",
       "      <th>ibo</th>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lug</th>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swa</th>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twi</th>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xho</th>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Word    Pos  family  region\n",
       "fold Language                                     \n",
       "-1   af                1899   1899    1899    1899\n",
       "     ar                7534   7534    7534    7534\n",
       "     en               15532  15532   15532   15532\n",
       "     eng-ron-wol-sna  13120  13120   13120   13120\n",
       "     fr               16339  16339   16339   16339\n",
       "     luo               7244   7244    7244    7244\n",
       "     sna               1492   1492    1492    1492\n",
       "     tsn               4936   4936    4936    4936\n",
       "     wol               1563   1563    1563    1563\n",
       " 0   pcm              10338  10338   10338   10338\n",
       " 1   bam               2481   2481    2481    2481\n",
       "     bbj               1484   1484    1484    1484\n",
       "     nya               1440   1440    1440    1440\n",
       "     zul               1503   1503    1503    1503\n",
       " 2   ewe               1453   1453    1453    1453\n",
       "     hau               1484   1484    1484    1484\n",
       "     mos               1508   1508    1508    1508\n",
       "     yor               2096   2096    2096    2096\n",
       " 3   fon               1617   1617    1617    1617\n",
       "     kin               1461   1461    1461    1461\n",
       " 4   ibo               1603   1603    1603    1603\n",
       "     lug               1461   1461    1461    1461\n",
       "     swa               1383   1383    1383    1383\n",
       "     twi               1567   1567    1567    1567\n",
       "     xho               1501   1501    1501    1501"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.groupby(['fold', 'Language']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c4eea-ebba-4afe-8bde-2f37646bfb13",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b46cb191-2de4-4698-ace9-95cabd8475a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def paralellize(fct, data, verbose=0, with_tqdm=True):\n",
    "    fn = map(delayed(fct), data)\n",
    "    if with_tqdm:\n",
    "        fn = tqdm(fn, total=len(data))\n",
    "    return Parallel(n_jobs=-1, verbose=verbose, backend=\"multiprocessing\")(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccd222f9-e3e1-4398-994b-33637be80614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self._time = 0\n",
    "\n",
    "    def start(self):\n",
    "        self._time = time()\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        return (time() - self._time) / 60\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de588c-aabf-4134-b3a3-c208e25b642b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8677ddf8-cd35-411b-bb67-397c2f501348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Make anibodi wey dey call say make dem impeach...</td>\n",
       "      <td>VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...</td>\n",
       "      <td>pcm</td>\n",
       "      <td>English-Creole</td>\n",
       "      <td>West</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وزير كورى : حل القضية النووية مع كوريا الشمالي...</td>\n",
       "      <td>NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mano nyalo konyi ahinya e yudo tich .</td>\n",
       "      <td>PRON VERB VERB ADV ADP VERB NOUN PUNCT</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Son nom d' espèce , composé de loyalti et de l...</td>\n",
       "      <td>DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les lemmings sont des êtres expressifs , capab...</td>\n",
       "      <td>DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Make anibodi wey dey call say make dem impeach...   \n",
       "1  وزير كورى : حل القضية النووية مع كوريا الشمالي...   \n",
       "2              Mano nyalo konyi ahinya e yudo tich .   \n",
       "3  Son nom d' espèce , composé de loyalti et de l...   \n",
       "4  Les lemmings sont des êtres expressifs , capab...   \n",
       "\n",
       "                                                 Pos Language          family  \\\n",
       "0  VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...      pcm  English-Creole   \n",
       "1  NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...       ar              ar   \n",
       "2             PRON VERB VERB ADV ADP VERB NOUN PUNCT      luo             luo   \n",
       "3  DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...       fr              fr   \n",
       "4  DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...       fr              fr   \n",
       "\n",
       "  region  fold  \n",
       "0   West     0  \n",
       "1     ar    -1  \n",
       "2    luo    -1  \n",
       "3     fr    -1  \n",
       "4     fr    -1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4e4278d-e53b-4675-a97f-1643fac183bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': 0,\n",
       " 'ADP': 1,\n",
       " 'ADV': 2,\n",
       " 'AUX': 3,\n",
       " 'CCONJ': 4,\n",
       " 'DET': 5,\n",
       " 'INTJ': 6,\n",
       " 'NOUN': 7,\n",
       " 'NUM': 8,\n",
       " 'PART': 9,\n",
       " 'PRON': 10,\n",
       " 'PROPN': 11,\n",
       " 'PUNCT': 12,\n",
       " 'SCONJ': 13,\n",
       " 'SYM': 14,\n",
       " 'VERB': 15,\n",
       " 'X': 16,\n",
       " 'NAW': 17}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_column_name = 'Word'\n",
    "label_column_name = 'Pos'\n",
    "label_list = sorted(train[label_column_name].unique()) + ['NAW']\n",
    "\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9478b6bc-ce17-412b-9bac-ef7b581460f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ADJ',\n",
       " 1: 'ADP',\n",
       " 2: 'ADV',\n",
       " 3: 'AUX',\n",
       " 4: 'CCONJ',\n",
       " 5: 'DET',\n",
       " 6: 'INTJ',\n",
       " 7: 'NOUN',\n",
       " 8: 'NUM',\n",
       " 9: 'PART',\n",
       " 10: 'PRON',\n",
       " 11: 'PROPN',\n",
       " 12: 'PUNCT',\n",
       " 13: 'SCONJ',\n",
       " 14: 'SYM',\n",
       " 15: 'VERB',\n",
       " 16: 'X',\n",
       " 17: 'NAW'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label = {v:k for k,v in label_to_id.items()}\n",
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a78a58e7-22ef-49cc-857e-9811cf748e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_path,\n",
    "        # config=config,\n",
    "        num_labels=num_labels, id2label=id_to_label, label2id=label_to_id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04218f8f-11fc-429e-b70e-c59931217409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77c8fc398044b34a3eba24cac95ff80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aab575f522b48358fca04488ca3262e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/404 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40906171ee7a45a4a35bdad11b81ec39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6cb986783847c3982495a91731bc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5f66e888c847749097f5fe33524049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_name = 'Davlan/afro-xlmr-base'\n",
    "# model_name = 'Davlan/afro-xlmr-large-61L'\n",
    "# model_name = 'Davlan/afro-xlmr-large-75L'\n",
    "model_name = 'masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0'\n",
    "# model_name = 'google/rembert'\n",
    "# model_name = 'bonadossou/afrolm_active_learning'\n",
    "# model_name = 'castorini/afriberta_large'\n",
    "\n",
    "max_seq_length = 256\n",
    "padding = False\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "tokenizer_name_or_path = model_name\n",
    "if config.model_type in {\"gpt2\", \"roberta\"}:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name_or_path,\n",
    "        use_fast=True,\n",
    "        add_prefix_space=True,\n",
    "    )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name_or_path,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    \n",
    "# data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, max_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54b5558a-0961-4dc5-ba9e-8f4103accfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(examples):\n",
    "    is_test = examples.get(label_column_name) is None\n",
    "    \n",
    "    for idx in range(len(examples[text_column_name])):\n",
    "        if not is_test:\n",
    "            examples[label_column_name][idx] = examples[label_column_name][idx].split()\n",
    "        examples[text_column_name][idx] = examples[text_column_name][idx].split()\n",
    "        \n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length if not is_test else None,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    \n",
    "    if not is_test:\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[label_column_name]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label_to_id[label[word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(label_to_id['NAW'])\n",
    "                previous_word_idx = word_idx\n",
    "            labels.append(label_ids)\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8f08892-823c-474a-a3f1-43480ec1213a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0688f4ccb054deabc35099b32adb9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cee4234b-51b0-4f63-9050-7c226b1596d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l not in [-100, 17]]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    lengths = list(map(len, true_labels))\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l not in [-100, 17]]\n",
    "        for prediction, label, length in zip(predictions, labels, lengths)\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f668b84f-ac49-4378-8cea-b0c9c3ecad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32045/32045 [00:00<00:00, 582956.59it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "words = []\n",
    "\n",
    "last_lang = test.Language.values[0]\n",
    "for i, row in enumerate(tqdm(test.itertuples(), total=len(test))):\n",
    "    words.append(row.Word)\n",
    "    \n",
    "    if row.Word == '.' or i == len(test)-1 or row.Language != last_lang:\n",
    "        sentences.append(' '.join(words))\n",
    "        words = []\n",
    "        \n",
    "    last_lang = row.Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00797992-b49d-47d1-bc47-6cbffc86ccf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0c0f60ed9b46a492e68cb9f9de4098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1974 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1974\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_dataset = HFDataset.from_pandas(test).remove_columns(column_names=['Id', 'Language', 'Pos'])\n",
    "# test_dataset = HFDataset.from_pandas(test_ner).remove_columns(column_names=['Language', 'family', 'region'])\n",
    "test_dataset = HFDataset.from_pandas(pd.DataFrame({'Word': sentences}))\n",
    "test_dataset = test_dataset.map(\n",
    "    process_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=['Word'],\n",
    ")\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94e34428-b18d-4949-83cc-e625b0ecf5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(self, model, optimizer, *, adv_param='weight',\n",
    "                 adv_lr=0.001, adv_eps=0.001):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.backup = {}\n",
    "\n",
    "    def perturb(self):\n",
    "        \"\"\"\n",
    "        Perturb model parameters for AWP gradient\n",
    "        Call before loss and loss.backward()\n",
    "        \"\"\"\n",
    "        self._save()  # save model parameters\n",
    "        self._attack_step()  # perturb weights\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                grad = self.optimizer.state[param]['exp_avg']\n",
    "                norm_grad = torch.norm(grad)\n",
    "                norm_data = torch.norm(param.detach())\n",
    "\n",
    "                if norm_grad != 0 and not torch.isnan(norm_grad):\n",
    "                    # Set lower and upper limit in change\n",
    "                    limit_eps = self.adv_eps * param.detach().abs()\n",
    "                    param_min = param.data - limit_eps\n",
    "                    param_max = param.data + limit_eps\n",
    "\n",
    "                    # Perturb along gradient\n",
    "                    # w += (adv_lr * |w| / |grad|) * grad\n",
    "                    param.data.add_(grad, alpha=(self.adv_lr * (norm_data + e) / (norm_grad + e)))\n",
    "\n",
    "                    # Apply the limit to the change\n",
    "                    param.data.clamp_(param_min, param_max)\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.clone().detach()\n",
    "                else:\n",
    "                    self.backup[name].copy_(param.data)\n",
    "\n",
    "    def restore(self):\n",
    "        \"\"\"\n",
    "        Restore model parameter to correct position; AWP do not perturbe weights, it perturb gradients\n",
    "        Call after loss.backward(), before optimizer.step()\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data.copy_(self.backup[name])\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.awp = AWP(self.model, self.optimizer, adv_lr=0.001, adv_eps=0.001)\n",
    "    \n",
    "    def training_step(self, model: nn.Module, inputs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        # if is_sagemaker_mp_enabled():\n",
    "        #     loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n",
    "        #     return loss_mb.reduce_mean().detach().to(self.args.device)\n",
    "        \n",
    "        self.awp.perturb() # \n",
    "        \n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        if self.do_grad_scaling:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            self.accelerator.backward(loss)\n",
    "            \n",
    "        self.awp.restore()\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            if is_peft_available() and isinstance(model, PeftModel):\n",
    "                model_name = unwrap_model(model.base_model)._get_name()\n",
    "            else:\n",
    "                model_name = unwrap_model(model)._get_name()\n",
    "            if model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n",
    "                loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
    "            else:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
    "                raise ValueError(\n",
    "                    \"The model did not return a loss from the inputs, only the following keys: \"\n",
    "                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
    "                )\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c3ab5b5-1122-4b5a-bf90-b3c07e34c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(fold):\n",
    "    raw_dataset = DatasetDict({\n",
    "        'train': HFDataset.from_pandas(train_ner[train_ner.fold != fold]),\n",
    "        'validation': HFDataset.from_pandas(train_ner[train_ner.fold == fold])\n",
    "    }).remove_columns(column_names=['fold', '__index_level_0__', 'Language', 'family', 'region'])\n",
    "\n",
    "    train_dataset = raw_dataset[\"train\"]\n",
    "    column_names = train_dataset.column_names\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        process_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "    )\n",
    "    if max_seq_length is not None:\n",
    "        train_dataset = train_dataset.filter(\n",
    "            lambda example: len(example['input_ids']) <= max_seq_length\n",
    "        )\n",
    "\n",
    "    eval_dataset = raw_dataset['validation']\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        process_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "    )\n",
    "    if max_seq_length is not None:\n",
    "        eval_dataset = eval_dataset.filter(\n",
    "            lambda example: len(example['input_ids']) <= max_seq_length\n",
    "        )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'pos-tagging-ner/{model_name.replace(\"/\", \"-\")}/{fold}',\n",
    "        learning_rate=5e-5,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        group_by_length=True,\n",
    "        overwrite_output_dir=True,\n",
    "        warmup_steps=0.15,\n",
    "        num_train_epochs=5,\n",
    "        lr_scheduler_type ='cosine',\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,\n",
    "        fp16=True,\n",
    "        report_to='none'\n",
    "    )\n",
    "    \n",
    "    # model_path = glob(f'./pos-tagging-ner/{model_name.replace(\"/\", \"-\")}/{fold}/checkpoint-*')[0]\n",
    "    # model = load_model(model_path)\n",
    "    \n",
    "    model = load_model(model_name)\n",
    "\n",
    "    trainer = Trainer(\n",
    "    # trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    results = trainer.evaluate(eval_dataset)\n",
    "    \n",
    "    test_pos_ids = trainer.predict(test_dataset)\n",
    "    \n",
    "    return results['eval_accuracy'], test_pos_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65bf22a7-cfe7-454b-a739-20bfc29d1430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276e7112c0904775bfefcfdef1c1b441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/97131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce487824802454882c2ecb2de1954d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/97131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb9f057b50146e68f6b91d5963e146d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51f614287144e55a6e18e0694bce607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe037eaa97934679844b6816aa08b148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([18]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 1024]) in the checkpoint and torch.Size([18, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15180' max='15180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15180/15180 56:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.348800</td>\n",
       "      <td>0.655346</td>\n",
       "      <td>0.537482</td>\n",
       "      <td>0.510681</td>\n",
       "      <td>0.523739</td>\n",
       "      <td>0.589787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.728324</td>\n",
       "      <td>0.515254</td>\n",
       "      <td>0.485314</td>\n",
       "      <td>0.499836</td>\n",
       "      <td>0.571977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.134400</td>\n",
       "      <td>0.736456</td>\n",
       "      <td>0.523116</td>\n",
       "      <td>0.498118</td>\n",
       "      <td>0.510311</td>\n",
       "      <td>0.575550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.118900</td>\n",
       "      <td>0.752387</td>\n",
       "      <td>0.520593</td>\n",
       "      <td>0.499588</td>\n",
       "      <td>0.509874</td>\n",
       "      <td>0.575623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.789549</td>\n",
       "      <td>0.518401</td>\n",
       "      <td>0.490601</td>\n",
       "      <td>0.504118</td>\n",
       "      <td>0.572331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.809176</td>\n",
       "      <td>0.522198</td>\n",
       "      <td>0.480180</td>\n",
       "      <td>0.500308</td>\n",
       "      <td>0.578826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.090300</td>\n",
       "      <td>0.792410</td>\n",
       "      <td>0.496081</td>\n",
       "      <td>0.471255</td>\n",
       "      <td>0.483349</td>\n",
       "      <td>0.557088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>0.842487</td>\n",
       "      <td>0.501285</td>\n",
       "      <td>0.477026</td>\n",
       "      <td>0.488855</td>\n",
       "      <td>0.559269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.810978</td>\n",
       "      <td>0.511924</td>\n",
       "      <td>0.484534</td>\n",
       "      <td>0.497853</td>\n",
       "      <td>0.568404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.781387</td>\n",
       "      <td>0.505570</td>\n",
       "      <td>0.481506</td>\n",
       "      <td>0.493244</td>\n",
       "      <td>0.563808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.075800</td>\n",
       "      <td>0.797403</td>\n",
       "      <td>0.504261</td>\n",
       "      <td>0.484624</td>\n",
       "      <td>0.494247</td>\n",
       "      <td>0.560420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>0.794833</td>\n",
       "      <td>0.506748</td>\n",
       "      <td>0.477429</td>\n",
       "      <td>0.491652</td>\n",
       "      <td>0.565764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.832473</td>\n",
       "      <td>0.505260</td>\n",
       "      <td>0.483334</td>\n",
       "      <td>0.494053</td>\n",
       "      <td>0.562311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.054300</td>\n",
       "      <td>0.843328</td>\n",
       "      <td>0.516105</td>\n",
       "      <td>0.491452</td>\n",
       "      <td>0.503477</td>\n",
       "      <td>0.573184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.837455</td>\n",
       "      <td>0.514681</td>\n",
       "      <td>0.487536</td>\n",
       "      <td>0.500741</td>\n",
       "      <td>0.571373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.858656</td>\n",
       "      <td>0.512015</td>\n",
       "      <td>0.482662</td>\n",
       "      <td>0.496905</td>\n",
       "      <td>0.569627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.851041</td>\n",
       "      <td>0.513973</td>\n",
       "      <td>0.485645</td>\n",
       "      <td>0.499408</td>\n",
       "      <td>0.570544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.831140</td>\n",
       "      <td>0.508226</td>\n",
       "      <td>0.482993</td>\n",
       "      <td>0.495289</td>\n",
       "      <td>0.564621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.928779</td>\n",
       "      <td>0.508393</td>\n",
       "      <td>0.480878</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.566182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.927373</td>\n",
       "      <td>0.499013</td>\n",
       "      <td>0.480251</td>\n",
       "      <td>0.489452</td>\n",
       "      <td>0.558320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.886314</td>\n",
       "      <td>0.513853</td>\n",
       "      <td>0.488934</td>\n",
       "      <td>0.501084</td>\n",
       "      <td>0.570786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.921542</td>\n",
       "      <td>0.510592</td>\n",
       "      <td>0.485923</td>\n",
       "      <td>0.497952</td>\n",
       "      <td>0.567526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.918646</td>\n",
       "      <td>0.510020</td>\n",
       "      <td>0.488235</td>\n",
       "      <td>0.498890</td>\n",
       "      <td>0.567293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.939622</td>\n",
       "      <td>0.509095</td>\n",
       "      <td>0.484015</td>\n",
       "      <td>0.496238</td>\n",
       "      <td>0.566013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.975950</td>\n",
       "      <td>0.507875</td>\n",
       "      <td>0.483405</td>\n",
       "      <td>0.495338</td>\n",
       "      <td>0.564999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.982607</td>\n",
       "      <td>0.509809</td>\n",
       "      <td>0.486918</td>\n",
       "      <td>0.498100</td>\n",
       "      <td>0.567068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.990829</td>\n",
       "      <td>0.510843</td>\n",
       "      <td>0.485690</td>\n",
       "      <td>0.497949</td>\n",
       "      <td>0.568484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.987305</td>\n",
       "      <td>0.510331</td>\n",
       "      <td>0.485995</td>\n",
       "      <td>0.497866</td>\n",
       "      <td>0.567253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.987036</td>\n",
       "      <td>0.510091</td>\n",
       "      <td>0.485538</td>\n",
       "      <td>0.497512</td>\n",
       "      <td>0.567205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.987483</td>\n",
       "      <td>0.509886</td>\n",
       "      <td>0.485466</td>\n",
       "      <td>0.497377</td>\n",
       "      <td>0.567027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5897871313025633\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e6e82ddd5547ebbb9c420f67901e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/97498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cb6da912b7485ab87c4ec45c1b1ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/97498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ae004cefb74b9abed02132ad4808e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8acbbe82ba04a7a822c709c328f4b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([18]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 1024]) in the checkpoint and torch.Size([18, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15235' max='15235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15235/15235 57:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>0.682003</td>\n",
       "      <td>0.569740</td>\n",
       "      <td>0.555662</td>\n",
       "      <td>0.562613</td>\n",
       "      <td>0.630511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.714919</td>\n",
       "      <td>0.549538</td>\n",
       "      <td>0.526742</td>\n",
       "      <td>0.537898</td>\n",
       "      <td>0.612161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>0.552489</td>\n",
       "      <td>0.531986</td>\n",
       "      <td>0.542044</td>\n",
       "      <td>0.616299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.118700</td>\n",
       "      <td>0.721330</td>\n",
       "      <td>0.554188</td>\n",
       "      <td>0.537165</td>\n",
       "      <td>0.545544</td>\n",
       "      <td>0.615389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.109900</td>\n",
       "      <td>0.691133</td>\n",
       "      <td>0.556236</td>\n",
       "      <td>0.527506</td>\n",
       "      <td>0.541490</td>\n",
       "      <td>0.621762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.735292</td>\n",
       "      <td>0.555834</td>\n",
       "      <td>0.532437</td>\n",
       "      <td>0.543884</td>\n",
       "      <td>0.623667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.083900</td>\n",
       "      <td>0.801477</td>\n",
       "      <td>0.554385</td>\n",
       "      <td>0.536227</td>\n",
       "      <td>0.545155</td>\n",
       "      <td>0.622040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.765613</td>\n",
       "      <td>0.553210</td>\n",
       "      <td>0.535223</td>\n",
       "      <td>0.544068</td>\n",
       "      <td>0.618527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.768133</td>\n",
       "      <td>0.560609</td>\n",
       "      <td>0.539747</td>\n",
       "      <td>0.549980</td>\n",
       "      <td>0.627186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.777458</td>\n",
       "      <td>0.556313</td>\n",
       "      <td>0.538460</td>\n",
       "      <td>0.547241</td>\n",
       "      <td>0.622582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.782193</td>\n",
       "      <td>0.558358</td>\n",
       "      <td>0.539733</td>\n",
       "      <td>0.548887</td>\n",
       "      <td>0.623318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.770577</td>\n",
       "      <td>0.554971</td>\n",
       "      <td>0.533237</td>\n",
       "      <td>0.543887</td>\n",
       "      <td>0.621749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.825626</td>\n",
       "      <td>0.552496</td>\n",
       "      <td>0.534532</td>\n",
       "      <td>0.543365</td>\n",
       "      <td>0.617604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.822646</td>\n",
       "      <td>0.552917</td>\n",
       "      <td>0.536532</td>\n",
       "      <td>0.544601</td>\n",
       "      <td>0.618966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.830570</td>\n",
       "      <td>0.549485</td>\n",
       "      <td>0.532263</td>\n",
       "      <td>0.540737</td>\n",
       "      <td>0.613304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.839386</td>\n",
       "      <td>0.543736</td>\n",
       "      <td>0.524574</td>\n",
       "      <td>0.533983</td>\n",
       "      <td>0.609739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.854015</td>\n",
       "      <td>0.554419</td>\n",
       "      <td>0.539187</td>\n",
       "      <td>0.546697</td>\n",
       "      <td>0.620845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.841108</td>\n",
       "      <td>0.553479</td>\n",
       "      <td>0.533012</td>\n",
       "      <td>0.543053</td>\n",
       "      <td>0.619463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.912762</td>\n",
       "      <td>0.551222</td>\n",
       "      <td>0.534190</td>\n",
       "      <td>0.542573</td>\n",
       "      <td>0.616687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.906289</td>\n",
       "      <td>0.553669</td>\n",
       "      <td>0.537274</td>\n",
       "      <td>0.545348</td>\n",
       "      <td>0.619554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.936407</td>\n",
       "      <td>0.553312</td>\n",
       "      <td>0.533957</td>\n",
       "      <td>0.543462</td>\n",
       "      <td>0.619315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.912646</td>\n",
       "      <td>0.550688</td>\n",
       "      <td>0.533055</td>\n",
       "      <td>0.541728</td>\n",
       "      <td>0.616577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.902376</td>\n",
       "      <td>0.555505</td>\n",
       "      <td>0.537973</td>\n",
       "      <td>0.546598</td>\n",
       "      <td>0.621265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.921640</td>\n",
       "      <td>0.556493</td>\n",
       "      <td>0.536161</td>\n",
       "      <td>0.546138</td>\n",
       "      <td>0.623241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.027400</td>\n",
       "      <td>0.968231</td>\n",
       "      <td>0.556809</td>\n",
       "      <td>0.537762</td>\n",
       "      <td>0.547120</td>\n",
       "      <td>0.622743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.971756</td>\n",
       "      <td>0.555525</td>\n",
       "      <td>0.537216</td>\n",
       "      <td>0.546217</td>\n",
       "      <td>0.621413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.973845</td>\n",
       "      <td>0.555819</td>\n",
       "      <td>0.536409</td>\n",
       "      <td>0.545941</td>\n",
       "      <td>0.621659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.978758</td>\n",
       "      <td>0.556606</td>\n",
       "      <td>0.537311</td>\n",
       "      <td>0.546788</td>\n",
       "      <td>0.622588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.976564</td>\n",
       "      <td>0.555581</td>\n",
       "      <td>0.536685</td>\n",
       "      <td>0.545970</td>\n",
       "      <td>0.621542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.975764</td>\n",
       "      <td>0.555702</td>\n",
       "      <td>0.536911</td>\n",
       "      <td>0.546145</td>\n",
       "      <td>0.621620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6305108603011441\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56151e5ed1f49e285f6507ffa7fe606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/100961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e47d25af513457180d17bf6aa7a8f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563d5303d4e84d969b7a2125830396f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490462ca32824b618aaa2035f25a4016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([18]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 1024]) in the checkpoint and torch.Size([18, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15780' max='15780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15780/15780 55:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.329300</td>\n",
       "      <td>0.572784</td>\n",
       "      <td>0.582843</td>\n",
       "      <td>0.567146</td>\n",
       "      <td>0.574887</td>\n",
       "      <td>0.640164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.158200</td>\n",
       "      <td>0.619201</td>\n",
       "      <td>0.566522</td>\n",
       "      <td>0.552918</td>\n",
       "      <td>0.559638</td>\n",
       "      <td>0.622504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.659734</td>\n",
       "      <td>0.557878</td>\n",
       "      <td>0.539986</td>\n",
       "      <td>0.548786</td>\n",
       "      <td>0.618132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.124300</td>\n",
       "      <td>0.585766</td>\n",
       "      <td>0.575057</td>\n",
       "      <td>0.571233</td>\n",
       "      <td>0.573139</td>\n",
       "      <td>0.634600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>0.613824</td>\n",
       "      <td>0.560777</td>\n",
       "      <td>0.539034</td>\n",
       "      <td>0.549690</td>\n",
       "      <td>0.617210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.590900</td>\n",
       "      <td>0.578770</td>\n",
       "      <td>0.575053</td>\n",
       "      <td>0.576905</td>\n",
       "      <td>0.642165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>0.630312</td>\n",
       "      <td>0.572149</td>\n",
       "      <td>0.570573</td>\n",
       "      <td>0.571360</td>\n",
       "      <td>0.632318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.611541</td>\n",
       "      <td>0.577801</td>\n",
       "      <td>0.577706</td>\n",
       "      <td>0.577753</td>\n",
       "      <td>0.641412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.080600</td>\n",
       "      <td>0.646122</td>\n",
       "      <td>0.565672</td>\n",
       "      <td>0.557868</td>\n",
       "      <td>0.561743</td>\n",
       "      <td>0.627810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>0.661121</td>\n",
       "      <td>0.557466</td>\n",
       "      <td>0.561371</td>\n",
       "      <td>0.559412</td>\n",
       "      <td>0.615479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.079800</td>\n",
       "      <td>0.625863</td>\n",
       "      <td>0.579547</td>\n",
       "      <td>0.571588</td>\n",
       "      <td>0.575540</td>\n",
       "      <td>0.638197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.077700</td>\n",
       "      <td>0.683437</td>\n",
       "      <td>0.556400</td>\n",
       "      <td>0.555419</td>\n",
       "      <td>0.555909</td>\n",
       "      <td>0.618874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>0.682282</td>\n",
       "      <td>0.554053</td>\n",
       "      <td>0.556739</td>\n",
       "      <td>0.555392</td>\n",
       "      <td>0.614433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.693606</td>\n",
       "      <td>0.555746</td>\n",
       "      <td>0.555076</td>\n",
       "      <td>0.555411</td>\n",
       "      <td>0.616884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.696766</td>\n",
       "      <td>0.566793</td>\n",
       "      <td>0.567527</td>\n",
       "      <td>0.567160</td>\n",
       "      <td>0.626866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.716303</td>\n",
       "      <td>0.566339</td>\n",
       "      <td>0.569761</td>\n",
       "      <td>0.568045</td>\n",
       "      <td>0.626978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.731160</td>\n",
       "      <td>0.554221</td>\n",
       "      <td>0.554594</td>\n",
       "      <td>0.554407</td>\n",
       "      <td>0.616389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.692084</td>\n",
       "      <td>0.559922</td>\n",
       "      <td>0.558998</td>\n",
       "      <td>0.559459</td>\n",
       "      <td>0.618570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.709682</td>\n",
       "      <td>0.570250</td>\n",
       "      <td>0.569989</td>\n",
       "      <td>0.570119</td>\n",
       "      <td>0.632599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.734275</td>\n",
       "      <td>0.566840</td>\n",
       "      <td>0.567171</td>\n",
       "      <td>0.567006</td>\n",
       "      <td>0.627170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.715684</td>\n",
       "      <td>0.571727</td>\n",
       "      <td>0.566118</td>\n",
       "      <td>0.568909</td>\n",
       "      <td>0.631340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.740563</td>\n",
       "      <td>0.567266</td>\n",
       "      <td>0.567006</td>\n",
       "      <td>0.567136</td>\n",
       "      <td>0.629541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.736204</td>\n",
       "      <td>0.567433</td>\n",
       "      <td>0.567793</td>\n",
       "      <td>0.567613</td>\n",
       "      <td>0.628901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.740935</td>\n",
       "      <td>0.568397</td>\n",
       "      <td>0.568390</td>\n",
       "      <td>0.568393</td>\n",
       "      <td>0.630587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.747740</td>\n",
       "      <td>0.567804</td>\n",
       "      <td>0.567451</td>\n",
       "      <td>0.567627</td>\n",
       "      <td>0.629272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.775881</td>\n",
       "      <td>0.567455</td>\n",
       "      <td>0.566562</td>\n",
       "      <td>0.567008</td>\n",
       "      <td>0.628642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.786577</td>\n",
       "      <td>0.566879</td>\n",
       "      <td>0.566778</td>\n",
       "      <td>0.566828</td>\n",
       "      <td>0.627990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.794547</td>\n",
       "      <td>0.567759</td>\n",
       "      <td>0.567514</td>\n",
       "      <td>0.567637</td>\n",
       "      <td>0.628901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.785730</td>\n",
       "      <td>0.567667</td>\n",
       "      <td>0.567552</td>\n",
       "      <td>0.567610</td>\n",
       "      <td>0.628743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.787363</td>\n",
       "      <td>0.567820</td>\n",
       "      <td>0.567704</td>\n",
       "      <td>0.567762</td>\n",
       "      <td>0.628979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.788629</td>\n",
       "      <td>0.567572</td>\n",
       "      <td>0.567565</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.628687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6421650179856115\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2baf71c571954322b3a162544712a87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/96524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d210237cfdcf42759137d8568f55b35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/96524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ef9cb40c364602ac34fd39bd0d1e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8f26d9abe44dc5a6d6c560cbf966f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([18]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 1024]) in the checkpoint and torch.Size([18, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15085' max='15085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15085/15085 58:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.307100</td>\n",
       "      <td>0.607125</td>\n",
       "      <td>0.610775</td>\n",
       "      <td>0.602287</td>\n",
       "      <td>0.606501</td>\n",
       "      <td>0.671751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.632038</td>\n",
       "      <td>0.601547</td>\n",
       "      <td>0.600306</td>\n",
       "      <td>0.600926</td>\n",
       "      <td>0.664296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.692767</td>\n",
       "      <td>0.603571</td>\n",
       "      <td>0.595032</td>\n",
       "      <td>0.599271</td>\n",
       "      <td>0.665710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.704617</td>\n",
       "      <td>0.608442</td>\n",
       "      <td>0.595995</td>\n",
       "      <td>0.602154</td>\n",
       "      <td>0.665275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.674937</td>\n",
       "      <td>0.602553</td>\n",
       "      <td>0.592177</td>\n",
       "      <td>0.597320</td>\n",
       "      <td>0.660664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.101100</td>\n",
       "      <td>0.723118</td>\n",
       "      <td>0.600464</td>\n",
       "      <td>0.596888</td>\n",
       "      <td>0.598671</td>\n",
       "      <td>0.667564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.739483</td>\n",
       "      <td>0.601847</td>\n",
       "      <td>0.594164</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>0.664024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.738468</td>\n",
       "      <td>0.600664</td>\n",
       "      <td>0.596832</td>\n",
       "      <td>0.598742</td>\n",
       "      <td>0.663812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.698591</td>\n",
       "      <td>0.604619</td>\n",
       "      <td>0.590221</td>\n",
       "      <td>0.597333</td>\n",
       "      <td>0.663328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.077400</td>\n",
       "      <td>0.710062</td>\n",
       "      <td>0.603024</td>\n",
       "      <td>0.599113</td>\n",
       "      <td>0.601062</td>\n",
       "      <td>0.665987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.718273</td>\n",
       "      <td>0.605803</td>\n",
       "      <td>0.599500</td>\n",
       "      <td>0.602635</td>\n",
       "      <td>0.666933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.732684</td>\n",
       "      <td>0.612594</td>\n",
       "      <td>0.601018</td>\n",
       "      <td>0.606751</td>\n",
       "      <td>0.671343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.052700</td>\n",
       "      <td>0.776532</td>\n",
       "      <td>0.602217</td>\n",
       "      <td>0.602612</td>\n",
       "      <td>0.602414</td>\n",
       "      <td>0.664905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.782475</td>\n",
       "      <td>0.607123</td>\n",
       "      <td>0.595882</td>\n",
       "      <td>0.601450</td>\n",
       "      <td>0.667085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.766551</td>\n",
       "      <td>0.604425</td>\n",
       "      <td>0.595176</td>\n",
       "      <td>0.599765</td>\n",
       "      <td>0.664774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.802096</td>\n",
       "      <td>0.604390</td>\n",
       "      <td>0.603049</td>\n",
       "      <td>0.603719</td>\n",
       "      <td>0.668075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.775413</td>\n",
       "      <td>0.609848</td>\n",
       "      <td>0.605005</td>\n",
       "      <td>0.607417</td>\n",
       "      <td>0.670506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.810378</td>\n",
       "      <td>0.609826</td>\n",
       "      <td>0.602018</td>\n",
       "      <td>0.605897</td>\n",
       "      <td>0.669429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.848546</td>\n",
       "      <td>0.611319</td>\n",
       "      <td>0.607442</td>\n",
       "      <td>0.609374</td>\n",
       "      <td>0.672371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.861066</td>\n",
       "      <td>0.611435</td>\n",
       "      <td>0.599844</td>\n",
       "      <td>0.605584</td>\n",
       "      <td>0.670315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.889758</td>\n",
       "      <td>0.607839</td>\n",
       "      <td>0.602274</td>\n",
       "      <td>0.605044</td>\n",
       "      <td>0.669402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.881452</td>\n",
       "      <td>0.608376</td>\n",
       "      <td>0.602993</td>\n",
       "      <td>0.605672</td>\n",
       "      <td>0.668722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.881044</td>\n",
       "      <td>0.606257</td>\n",
       "      <td>0.601556</td>\n",
       "      <td>0.603897</td>\n",
       "      <td>0.668162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.866639</td>\n",
       "      <td>0.608507</td>\n",
       "      <td>0.602849</td>\n",
       "      <td>0.605665</td>\n",
       "      <td>0.669462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.903650</td>\n",
       "      <td>0.608020</td>\n",
       "      <td>0.602199</td>\n",
       "      <td>0.605096</td>\n",
       "      <td>0.669456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.918947</td>\n",
       "      <td>0.600659</td>\n",
       "      <td>0.595532</td>\n",
       "      <td>0.598085</td>\n",
       "      <td>0.663007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.922624</td>\n",
       "      <td>0.601678</td>\n",
       "      <td>0.595170</td>\n",
       "      <td>0.598406</td>\n",
       "      <td>0.663660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.922799</td>\n",
       "      <td>0.600906</td>\n",
       "      <td>0.595045</td>\n",
       "      <td>0.597961</td>\n",
       "      <td>0.663094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.926160</td>\n",
       "      <td>0.601375</td>\n",
       "      <td>0.595407</td>\n",
       "      <td>0.598376</td>\n",
       "      <td>0.663551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.926647</td>\n",
       "      <td>0.601484</td>\n",
       "      <td>0.595501</td>\n",
       "      <td>0.598478</td>\n",
       "      <td>0.663605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6723706676236774\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "all_preds = []\n",
    "\n",
    "for fold in [1, 2, 3, 4]:\n",
    "    score, fold_pred = train_fold(fold)\n",
    "    \n",
    "    scores.append(score)\n",
    "    all_preds.append(fold_pred)\n",
    "    \n",
    "    print(score)\n",
    "    print()\n",
    "    \n",
    "avg_score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b509e41f-8169-484e-af92-3853030b1f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5897871313025633,\n",
       " 0.6305108603011441,\n",
       " 0.6421650179856115,\n",
       " 0.6723706676236774]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f41ddd1d-b6b7-40fb-b1f2-7f5b58e361ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6337084193032492"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5423319-fc1d-4e9c-ab53-0eb0122803dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 190, 18)\n"
     ]
    }
   ],
   "source": [
    "test_pos_ids = np.mean([p.predictions for p in all_preds], axis=0)\n",
    "print(test_pos_ids.shape)\n",
    "test_pos_ids = test_pos_ids.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "45ce2daa-b304-4065-8ab6-40fe16f91f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pos = []\n",
    "data_pos = []\n",
    "\n",
    "for pos_ids, sentence in zip(test_pos_ids, sentences):\n",
    "    length = len(sentence.split(' '))\n",
    "    \n",
    "    clean_pos_ids = [x for x in pos_ids if x not in [-100, 17]]\n",
    "    \n",
    "    sentence_pos = list(map(id_to_label.get, clean_pos_ids))[1:length+1]\n",
    "    # sentence_pos = list(map(id_to_label.get, clean_pos_ids))[:length]\n",
    "    \n",
    "    final_pos.extend(sentence_pos)\n",
    "    data_pos.append(' '.join(sentence_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab63a58c-f2ec-43d8-8f79-fbc90939c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Pos'] = final_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "55dd1deb-7091-4db3-8cf9-8cd0147a6346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOUN     6689\n",
       "VERB     4930\n",
       "ADP      4412\n",
       "PUNCT    3061\n",
       "PROPN    2955\n",
       "AUX      2801\n",
       "PRON     1657\n",
       "SCONJ    1255\n",
       "DET       826\n",
       "ADV       823\n",
       "PART      724\n",
       "CCONJ     675\n",
       "ADJ       617\n",
       "NUM       553\n",
       "INTJ       28\n",
       "X          26\n",
       "SYM        13\n",
       "Name: Pos, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Pos.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5ff57974-cd0b-47c0-9356-e08ceb7af5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "      <th>Language</th>\n",
       "      <th>Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Id00qog2f11n_0</td>\n",
       "      <td>Ne</td>\n",
       "      <td>luo</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Id00qog2f11n_1</td>\n",
       "      <td>otim</td>\n",
       "      <td>luo</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Id00qog2f11n_2</td>\n",
       "      <td>penj</td>\n",
       "      <td>luo</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Id00qog2f11n_3</td>\n",
       "      <td>e</td>\n",
       "      <td>luo</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Id00qog2f11n_4</td>\n",
       "      <td>kind</td>\n",
       "      <td>luo</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  Word Language   Pos\n",
       "0  Id00qog2f11n_0    Ne      luo   AUX\n",
       "1  Id00qog2f11n_1  otim      luo  VERB\n",
       "2  Id00qog2f11n_2  penj      luo  NOUN\n",
       "3  Id00qog2f11n_3     e      luo   ADP\n",
       "4  Id00qog2f11n_4  kind      luo  NOUN"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f4af1392-9850-4c05-97bb-510eae24ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({'Word': sentences, 'Pos': data_pos})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd03bca7-647e-41ce-84c7-6ffbf1096b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['Id', 'Pos']].to_csv(f'submissions/ps-round1-pos-ner-{model_name.split(\"/\")[-1]}-{avg_score:.3f}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c85bbd-29a8-40f8-933b-5e45111e84be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test[test.Pos == 'SYM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b6f72-85c4-4881-8319-c59d5dd08de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
