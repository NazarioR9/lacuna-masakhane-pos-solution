{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a239eea-e414-412d-9242-d7316e9cc18d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a7fcd7-eb6b-4ee8-8f57-ff5a443c78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch==1.13.1 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6eb2fe-f515-471e-8ae3-1aae649d697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c866a9c-6192-4cf1-b8e7-1b96c98953f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1961ba0b-d202-4477-8004-63b1c5e5ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 17 12:42:12 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   46C    P8    20W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a20d7-05ea-4951-a079-08c8b8a71940",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16825bfc-8853-445c-9e6c-53395bea41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "import subprocess\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedGroupKFold, GroupKFold\n",
    "from sklearn.metrics import f1_score, mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6e65d9-657c-4b4d-a20c-ccd0dc3f5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b9b22b-0a41-44b0-beec-e050afbeaa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR\n",
    "from torch import LongTensor\n",
    "from torch import nn, optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4c3033-b975-4aac-bc6b-24346ea9c6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "# from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm, trange\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict, Dataset as HFDataset\n",
    "from datasets import concatenate_datasets, interleave_datasets\n",
    "from datasets import ClassLabel, load_metric\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a120b4-51ef-49dc-9748-873fbd66e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext watermark\n",
    "# %watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563effdb-c378-45ba-8f7e-8d7cc8bf8b08",
   "metadata": {},
   "source": [
    "# Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db742f8-2be3-47d5-bd7b-82629ce77866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_warnings(strict=False):\n",
    "\twarnings.simplefilter('ignore')\n",
    "\tif strict:\n",
    "\t\tlogging.disable(logging.WARNING)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "\trandom.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c01b0cb9-83b7-46c0-8383-c9384772e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "disable_warnings()\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7fe6b-98d6-4d75-9912-bf1bf44b6643",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe357fad-adaf-4b32-8c8a-3bfba4d57170",
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = '../masakhane-pos/data/'\n",
    "add_path = '../masakhane-pos/transfer_corpus/'\n",
    "# pseudo_path = './pseudos/pos-ner-afro-xlmr-large-75L-best.csv'\n",
    "pseudo_path = './pseudos/pos-ner-multi-3.csv'\n",
    "\n",
    "langs = sorted(os.listdir(path))\n",
    "add_langs = os.listdir(add_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5bd3e9-2a48-4227-af3e-24fde850a994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang2family = {\n",
    " 'bam': 'Mande',\n",
    " 'bbj': 'Grassfields',\n",
    " 'ewe': 'Kwa',\n",
    " 'fon': 'Volta-Niger',\n",
    " 'hau': 'Chadic',\n",
    " 'ibo': 'Volta-Niger',\n",
    " 'kin': 'Bantu',\n",
    " 'lug': 'Bantu',\n",
    " 'mos': 'Gur',\n",
    " 'nya': 'Bantu',\n",
    " 'pcm': 'English-Creole',\n",
    " 'sna': 'Bantu',\n",
    " 'swa': 'Bantu',\n",
    " 'twi': 'Kwa',\n",
    " 'wol': 'Senegambia',\n",
    " 'xho': 'Bantu',\n",
    " 'yor': 'Volta-Niger',\n",
    " 'zul': 'Bantu'\n",
    "}\n",
    "\n",
    "lang2region = {\n",
    "    'bam': 'West',\n",
    "    'bbj': 'Central',\n",
    "    'ewe': 'West',\n",
    "    'fon': 'West',\n",
    "    'hau': 'West',\n",
    "    'ibo': 'West',\n",
    "    'kin': 'East',\n",
    "    'lug': 'East',\n",
    "    'mos': 'West',\n",
    "    'nya': 'East',\n",
    "    'pcm': 'West',\n",
    "    'sna': 'South',\n",
    "    'swa': 'East',\n",
    "    'twi': 'South',\n",
    "    'wol': 'West',\n",
    "    'xho': 'South',\n",
    "    'yor': 'West',\n",
    "    'zul': 'South'\n",
    "}\n",
    "\n",
    "mapper = {'NOUN': 0,\n",
    " 'ADJ': 1,\n",
    " 'PUNCT': 2,\n",
    " 'CCONJ': 3,\n",
    " 'PRON': 4,\n",
    " 'ADV': 5,\n",
    " 'AUX': 6,\n",
    " 'VERB': 7,\n",
    " 'ADP': 8,\n",
    " 'PART': 9,\n",
    " 'SCONJ': 10,\n",
    " 'PROPN': 11,\n",
    " 'X': 12,\n",
    " 'NUM': 13,\n",
    " 'INTJ': 14,\n",
    " 'SYM': 15,\n",
    " 'DET': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d2ead2-2302-480b-82b6-37346e9e07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2fix = {\n",
    "    'yo': 'yor',\n",
    "    'bm': 'bam'\n",
    "}\n",
    "\n",
    "def load_lang_data(data_path, lang, split='train'):\n",
    "    data = []\n",
    "    language = lang.split('_')[0]\n",
    "    language = lang2fix.get(language, language)\n",
    "    \n",
    "    try:\n",
    "        with open(f'{data_path}{lang}/{split}.txt', 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                splits = line.split(' ')\n",
    "                if len(splits) != 2:\n",
    "                    continue\n",
    "                data.append(splits)\n",
    "        data = np.array(data).squeeze()\n",
    "        data = pd.DataFrame({\n",
    "            'Word': data[:, 0],\n",
    "            'Pos': data[:, 1],\n",
    "            'Language': language.split('_')[0]\n",
    "        })\n",
    "        data['family'] = lang2family.get(language, language)\n",
    "        data['region'] = lang2region.get(language, language)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d23f39a4-6053-418d-b865-aaf8f4af5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lang_data_ner(data_path, lang, split='train'):\n",
    "    data = []\n",
    "    language = lang.split('_')[0]\n",
    "    language = lang2fix.get(language, language)\n",
    "    \n",
    "    try:\n",
    "        with open(f'{data_path}{lang}/{split}.txt', 'r') as f:\n",
    "            sentences, labels = [], []\n",
    "            \n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                splits = line.split(' ')\n",
    "                \n",
    "                if len(splits) == 2:\n",
    "                    sentences.append(splits[0])\n",
    "                    labels.append(splits[1])\n",
    "                elif len(splits) == 1:\n",
    "                    data.append(\n",
    "                        [\n",
    "                            ' '.join(sentences),\n",
    "                            ' '.join(labels),\n",
    "                        ]\n",
    "                    )\n",
    "                    \n",
    "                    sentences, labels = [], []\n",
    "\n",
    "        data = np.array(data).squeeze()\n",
    "        data = pd.DataFrame({\n",
    "            'Word': data[:, 0],\n",
    "            'Pos': data[:, 1],\n",
    "            'Language': language.split('_')[0]\n",
    "        })\n",
    "        data['family'] = lang2family.get(language, language)\n",
    "        data['region'] = lang2region.get(language, language)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "378e3114-de98-4f91-a64f-e7ebf677b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):  \n",
    "    # pos = df.Pos.unique()\n",
    "    use_pos = ['PUNCT', 'SYM']\n",
    "    \n",
    "    df_others = df[~df.Pos.isin(use_pos)]\n",
    "    df_toclean = df[df.Pos.isin(use_pos)]\n",
    "    \n",
    "    dfs = [df_others]\n",
    "    \n",
    "    for p in use_pos:\n",
    "        df_p = df_toclean[df_toclean.Pos == p]\n",
    "        words = df_p.Word.value_counts()[df_p.Word.value_counts() < 2].index.values\n",
    "        df_p = df_p[~df_p.Word.isin(words)]\n",
    "        \n",
    "        dfs.append(df_p)\n",
    "        \n",
    "    return pd.concat(dfs).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f54e1e-1c04-40a8-b297-d7bf05d901d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_data(with_pseudo=False):\n",
    "    train = pd.concat([load_lang_data_ner(path, lang, split) for lang in langs for split in ['train', 'dev', 'test']], axis=0).dropna().reset_index(drop=True)\n",
    "    add_data = (\n",
    "        pd.concat([load_lang_data_ner(add_path, lang, split) for lang in add_langs for split in ['train', 'dev', 'test']], axis=0)\n",
    "        .dropna()\n",
    "        .drop_duplicates(subset=['Word', 'Pos'], keep='first')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    train = pd.concat([train, add_data])\n",
    "    if with_pseudo:\n",
    "        train = pd.concat([train, pd.read_csv(pseudo_path)])\n",
    "    train = train.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return train\n",
    "\n",
    "def load_pos_data():\n",
    "    train = pd.concat([load_lang_data(path, lang, split) for lang in langs for split in ['train', 'dev', 'test']], axis=0).dropna().reset_index(drop=True)\n",
    "    add_data = (\n",
    "        pd.concat([load_lang_data(add_path, lang, split) for lang in add_langs for split in ['train', 'dev', 'test']], axis=0)\n",
    "        .dropna()\n",
    "        .drop_duplicates(subset=['Word', 'Pos'], keep='first')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    train = pd.concat([train, add_data])\n",
    "    train = train[train.Pos.isin(list(mapper.keys()))]\n",
    "\n",
    "    train = train.drop_duplicates().drop_duplicates(subset=['Word', 'Language'], keep='first').reset_index(drop=True)\n",
    "    # train = train.drop_duplicates().reset_index(drop=True)\n",
    "    # train = clean_data(train)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd40436e-a4f4-4823-99b3-8ff6c05636de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muso</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ŋana</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afiriki</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tilebinyanfan</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word    Pos Language family region\n",
       "0           Muso   NOUN      bam  Mande   West\n",
       "1           ŋana    ADJ      bam  Mande   West\n",
       "2              ,  PUNCT      bam  Mande   West\n",
       "3        Afiriki   NOUN      bam  Mande   West\n",
       "4  tilebinyanfan   NOUN      bam  Mande   West"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_pseudo = True\n",
    "\n",
    "train_ner = load_ner_data(with_pseudo)\n",
    "train = load_pos_data()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fc60dcc-029f-4508-85ac-070e6a79b23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muso ŋana , Afiriki tilebinyanfan n' a cɛmancɛ...</td>\n",
       "      <td>NOUN ADJ PUNCT NOUN NOUN CCONJ PRON NOUN ADV C...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ni mɔgɔ ka dɔgɔn kojugu , i bɛ mɔgɔw juguya mi...</td>\n",
       "      <td>CCONJ NOUN PART ADJ NOUN PUNCT PRON AUX NOUN N...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A kɔrɔtalen a ka taɲɛ fɛ kow ɲɛmɔgɔyabaaraw la...</td>\n",
       "      <td>PRON VERB PRON PART NOUN PART NOUN NOUN PART C...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ale y' a ( basikɛti ) to a ka se ka bɔ a ka so...</td>\n",
       "      <td>PRON PART PRON PUNCT NOUN PUNCT VERB PRON PART...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sannayɛlɛn galabukɛnɛya a sera ka min fiyɛ Afi...</td>\n",
       "      <td>NOUN NOUN PRON VERB PART PRON VERB PROPN NOUN ...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Muso ŋana , Afiriki tilebinyanfan n' a cɛmancɛ...   \n",
       "1  Ni mɔgɔ ka dɔgɔn kojugu , i bɛ mɔgɔw juguya mi...   \n",
       "2  A kɔrɔtalen a ka taɲɛ fɛ kow ɲɛmɔgɔyabaaraw la...   \n",
       "3  Ale y' a ( basikɛti ) to a ka se ka bɔ a ka so...   \n",
       "4  Sannayɛlɛn galabukɛnɛya a sera ka min fiyɛ Afi...   \n",
       "\n",
       "                                                 Pos Language family region  \n",
       "0  NOUN ADJ PUNCT NOUN NOUN CCONJ PRON NOUN ADV C...      bam  Mande   West  \n",
       "1  CCONJ NOUN PART ADJ NOUN PUNCT PRON AUX NOUN N...      bam  Mande   West  \n",
       "2  PRON VERB PRON PART NOUN PART NOUN NOUN PART C...      bam  Mande   West  \n",
       "3  PRON PART PRON PUNCT NOUN PUNCT VERB PRON PART...      bam  Mande   West  \n",
       "4  NOUN NOUN PRON VERB PART PRON VERB PROPN NOUN ...      bam  Mande   West  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1e73767-4a99-4f9f-b4dc-7c7d6eeb8ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "      <th>Language</th>\n",
       "      <th>Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Id00qog2f11n_0</td>\n",
       "      <td>Ne</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Id00qog2f11n_1</td>\n",
       "      <td>otim</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Id00qog2f11n_2</td>\n",
       "      <td>penj</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Id00qog2f11n_3</td>\n",
       "      <td>e</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Id00qog2f11n_4</td>\n",
       "      <td>kind</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  Word Language  Pos\n",
       "0  Id00qog2f11n_0    Ne      luo  NaN\n",
       "1  Id00qog2f11n_1  otim      luo  NaN\n",
       "2  Id00qog2f11n_2  penj      luo  NaN\n",
       "3  Id00qog2f11n_3     e      luo  NaN\n",
       "4  Id00qog2f11n_4  kind      luo  NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../Test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07d27197-1217-4279-bffa-8052260878dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((255140, 5), (104039, 5), (32045, 4))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, train_ner.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2114cb1d-b750-4c81-b4e9-cdf1b4d7bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = train_ner[train_ner.Language.isin(langs)].reset_index(drop=True)\n",
    "add_train = train_ner[~train_ner.Language.isin(langs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "989646fd-faa8-4d30-92a5-db49b4e2518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['fold'] = -1\n",
    "train_ner['fold'] = -1\n",
    "\n",
    "\n",
    "# val_langs = ['fon', 'pcm', 'twi', 'xho']\n",
    "# train.loc[train.Language.isin(val_langs), 'fold'] = 0\n",
    "\n",
    "base_train['fold'] = base_train['Language'].map(dict(zip(langs, range(len(langs)))))\n",
    "base_train.loc[base_train['fold'].isna(), 'fold'] = -1\n",
    "base_train['fold'] = base_train['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12da99ae-93ad-4f4a-9f4d-87e539c55ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    10338\n",
       "1     6908\n",
       "2     6541\n",
       "3     6133\n",
       "4     7515\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_fold = 5\n",
    "# Fold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "# Fold = StratifiedGroupKFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "Fold = GroupKFold(n_splits=n_fold)\n",
    "\n",
    "base_train['fold'] = -1\n",
    "# for n, (train_index, val_index) in enumerate(Fold.split(train, train['region'], groups=train['Language'])):\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(base_train, base_train['Pos'], groups=base_train['Language'])):\n",
    "    base_train.loc[val_index, 'fold'] = int(n)\n",
    "    \n",
    "base_train['fold'] = base_train['fold'].astype(int)\n",
    "\n",
    "display(base_train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b0cabda-60df-4c10-bb88-93fcd7cddafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bam', 'bbj', 'ewe', 'fon', 'hau', 'ibo', 'kin', 'lug', 'mos',\n",
       "       'nya', 'pcm', 'sna', 'swa', 'twi', 'wol', 'xho', 'yor', 'zul',\n",
       "       'en', 'fr', 'eng-ron-wol-sna', 'ar', 'af', 'luo', 'tsn'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.Language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6f2731d-47d6-4b01-814b-bd5a8d97bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_train['fold'] = -1\n",
    "base_train.loc[base_train.Language.isin(['sna', 'wol']), 'fold'] = -1\n",
    "# base_train.loc[base_train.Language.isin(['sna', 'bam', 'pcm', 'yor', 'wol']), 'fold'] = -1\n",
    "\n",
    "train_ner = pd.concat([base_train, add_train]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef6675b1-ada8-436b-829d-7d24d62d9382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Make anibodi wey dey call say make dem impeach...</td>\n",
       "      <td>VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...</td>\n",
       "      <td>pcm</td>\n",
       "      <td>English-Creole</td>\n",
       "      <td>West</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وزير كورى : حل القضية النووية مع كوريا الشمالي...</td>\n",
       "      <td>NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mano nyalo konyi ahinya e yudo tich .</td>\n",
       "      <td>PRON VERB VERB ADV ADP VERB NOUN PUNCT</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Son nom d' espèce , composé de loyalti et de l...</td>\n",
       "      <td>DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les lemmings sont des êtres expressifs , capab...</td>\n",
       "      <td>DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Make anibodi wey dey call say make dem impeach...   \n",
       "1  وزير كورى : حل القضية النووية مع كوريا الشمالي...   \n",
       "2              Mano nyalo konyi ahinya e yudo tich .   \n",
       "3  Son nom d' espèce , composé de loyalti et de l...   \n",
       "4  Les lemmings sont des êtres expressifs , capab...   \n",
       "\n",
       "                                                 Pos Language          family  \\\n",
       "0  VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...      pcm  English-Creole   \n",
       "1  NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...       ar              ar   \n",
       "2             PRON VERB VERB ADV ADP VERB NOUN PUNCT      luo             luo   \n",
       "3  DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...       fr              fr   \n",
       "4  DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...       fr              fr   \n",
       "\n",
       "  region  fold  \n",
       "0   West     0  \n",
       "1     ar    -1  \n",
       "2    luo    -1  \n",
       "3     fr    -1  \n",
       "4     fr    -1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7a768de-2e21-4900-b470-ea68408a58ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th>Language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">-1</th>\n",
       "      <th>af</th>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng-ron-wol-sna</th>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luo</th>\n",
       "      <td>7244</td>\n",
       "      <td>7244</td>\n",
       "      <td>7244</td>\n",
       "      <td>7244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sna</th>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsn</th>\n",
       "      <td>4936</td>\n",
       "      <td>4936</td>\n",
       "      <td>4936</td>\n",
       "      <td>4936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wol</th>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>pcm</th>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>bam</th>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbj</th>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nya</th>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zul</th>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
       "      <th>ewe</th>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hau</th>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mos</th>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yor</th>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>fon</th>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kin</th>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">4</th>\n",
       "      <th>ibo</th>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lug</th>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swa</th>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twi</th>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xho</th>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Word    Pos  family  region\n",
       "fold Language                                     \n",
       "-1   af                1899   1899    1899    1899\n",
       "     ar                7534   7534    7534    7534\n",
       "     en               15532  15532   15532   15532\n",
       "     eng-ron-wol-sna  13120  13120   13120   13120\n",
       "     fr               16339  16339   16339   16339\n",
       "     luo               7244   7244    7244    7244\n",
       "     sna               1492   1492    1492    1492\n",
       "     tsn               4936   4936    4936    4936\n",
       "     wol               1563   1563    1563    1563\n",
       " 0   pcm              10338  10338   10338   10338\n",
       " 1   bam               2481   2481    2481    2481\n",
       "     bbj               1484   1484    1484    1484\n",
       "     nya               1440   1440    1440    1440\n",
       "     zul               1503   1503    1503    1503\n",
       " 2   ewe               1453   1453    1453    1453\n",
       "     hau               1484   1484    1484    1484\n",
       "     mos               1508   1508    1508    1508\n",
       "     yor               2096   2096    2096    2096\n",
       " 3   fon               1617   1617    1617    1617\n",
       "     kin               1461   1461    1461    1461\n",
       " 4   ibo               1603   1603    1603    1603\n",
       "     lug               1461   1461    1461    1461\n",
       "     swa               1383   1383    1383    1383\n",
       "     twi               1567   1567    1567    1567\n",
       "     xho               1501   1501    1501    1501"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.groupby(['fold', 'Language']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c4eea-ebba-4afe-8bde-2f37646bfb13",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b46cb191-2de4-4698-ace9-95cabd8475a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def paralellize(fct, data, verbose=0, with_tqdm=True):\n",
    "    fn = map(delayed(fct), data)\n",
    "    if with_tqdm:\n",
    "        fn = tqdm(fn, total=len(data))\n",
    "    return Parallel(n_jobs=-1, verbose=verbose, backend=\"multiprocessing\")(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccd222f9-e3e1-4398-994b-33637be80614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self._time = 0\n",
    "\n",
    "    def start(self):\n",
    "        self._time = time()\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        return (time() - self._time) / 60\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de588c-aabf-4134-b3a3-c208e25b642b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8677ddf8-cd35-411b-bb67-397c2f501348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Make anibodi wey dey call say make dem impeach...</td>\n",
       "      <td>VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...</td>\n",
       "      <td>pcm</td>\n",
       "      <td>English-Creole</td>\n",
       "      <td>West</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وزير كورى : حل القضية النووية مع كوريا الشمالي...</td>\n",
       "      <td>NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mano nyalo konyi ahinya e yudo tich .</td>\n",
       "      <td>PRON VERB VERB ADV ADP VERB NOUN PUNCT</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Son nom d' espèce , composé de loyalti et de l...</td>\n",
       "      <td>DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les lemmings sont des êtres expressifs , capab...</td>\n",
       "      <td>DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Make anibodi wey dey call say make dem impeach...   \n",
       "1  وزير كورى : حل القضية النووية مع كوريا الشمالي...   \n",
       "2              Mano nyalo konyi ahinya e yudo tich .   \n",
       "3  Son nom d' espèce , composé de loyalti et de l...   \n",
       "4  Les lemmings sont des êtres expressifs , capab...   \n",
       "\n",
       "                                                 Pos Language          family  \\\n",
       "0  VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...      pcm  English-Creole   \n",
       "1  NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...       ar              ar   \n",
       "2             PRON VERB VERB ADV ADP VERB NOUN PUNCT      luo             luo   \n",
       "3  DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...       fr              fr   \n",
       "4  DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...       fr              fr   \n",
       "\n",
       "  region  fold  \n",
       "0   West     0  \n",
       "1     ar    -1  \n",
       "2    luo    -1  \n",
       "3     fr    -1  \n",
       "4     fr    -1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4e4278d-e53b-4675-a97f-1643fac183bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': 0,\n",
       " 'ADP': 1,\n",
       " 'ADV': 2,\n",
       " 'AUX': 3,\n",
       " 'CCONJ': 4,\n",
       " 'DET': 5,\n",
       " 'INTJ': 6,\n",
       " 'NOUN': 7,\n",
       " 'NUM': 8,\n",
       " 'PART': 9,\n",
       " 'PRON': 10,\n",
       " 'PROPN': 11,\n",
       " 'PUNCT': 12,\n",
       " 'SCONJ': 13,\n",
       " 'SYM': 14,\n",
       " 'VERB': 15,\n",
       " 'X': 16,\n",
       " 'NAW': 17}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_column_name = 'Word'\n",
    "label_column_name = 'Pos'\n",
    "label_list = sorted(train[label_column_name].unique()) + ['NAW']\n",
    "\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9478b6bc-ce17-412b-9bac-ef7b581460f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ADJ',\n",
       " 1: 'ADP',\n",
       " 2: 'ADV',\n",
       " 3: 'AUX',\n",
       " 4: 'CCONJ',\n",
       " 5: 'DET',\n",
       " 6: 'INTJ',\n",
       " 7: 'NOUN',\n",
       " 8: 'NUM',\n",
       " 9: 'PART',\n",
       " 10: 'PRON',\n",
       " 11: 'PROPN',\n",
       " 12: 'PUNCT',\n",
       " 13: 'SCONJ',\n",
       " 14: 'SYM',\n",
       " 15: 'VERB',\n",
       " 16: 'X',\n",
       " 17: 'NAW'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label = {v:k for k,v in label_to_id.items()}\n",
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a78a58e7-22ef-49cc-857e-9811cf748e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_path,\n",
    "        # config=config,\n",
    "        num_labels=num_labels, id2label=id_to_label, label2id=label_to_id,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        classifier_dropout=0.5, hidden_dropout_prob=0.2, attention_probs_dropout_prob=0.2\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04218f8f-11fc-429e-b70e-c59931217409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'Davlan/afro-xlmr-base'\n",
    "# model_name = 'Davlan/afro-xlmr-large-61L'\n",
    "model_name = 'Davlan/afro-xlmr-large-75L'\n",
    "# model_name = 'masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0'\n",
    "# model_name = 'google/rembert'\n",
    "# model_name = 'bonadossou/afrolm_active_learning'\n",
    "# model_name = 'castorini/afriberta_large'\n",
    "\n",
    "max_seq_length = 256\n",
    "padding = False\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "tokenizer_name_or_path = model_name\n",
    "if config.model_type in {\"gpt2\", \"roberta\"}:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name_or_path,\n",
    "        use_fast=True,\n",
    "        add_prefix_space=True,\n",
    "    )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name_or_path,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    \n",
    "# data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, max_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54b5558a-0961-4dc5-ba9e-8f4103accfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(examples):\n",
    "    is_test = examples.get(label_column_name) is None\n",
    "    \n",
    "    for idx in range(len(examples[text_column_name])):\n",
    "        if not is_test:\n",
    "            examples[label_column_name][idx] = examples[label_column_name][idx].split()\n",
    "        examples[text_column_name][idx] = examples[text_column_name][idx].split()\n",
    "        \n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length if not is_test else None,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    \n",
    "    if not is_test:\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[label_column_name]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label_to_id[label[word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(label_to_id['NAW'])\n",
    "                previous_word_idx = word_idx\n",
    "            labels.append(label_ids)\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8f08892-823c-474a-a3f1-43480ec1213a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f65215c68734b4d900dbff9900b39be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cee4234b-51b0-4f63-9050-7c226b1596d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l not in [-100, 17]]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    lengths = list(map(len, true_labels))\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l not in [-100, 17]]\n",
    "        for prediction, label, length in zip(predictions, labels, lengths)\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f668b84f-ac49-4378-8cea-b0c9c3ecad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32045/32045 [00:00<00:00, 478537.94it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "words = []\n",
    "\n",
    "last_lang = test.Language.values[0]\n",
    "for i, row in enumerate(tqdm(test.itertuples(), total=len(test))):\n",
    "    words.append(row.Word)\n",
    "    \n",
    "    if row.Word == '.' or i == len(test)-1 or row.Language != last_lang:\n",
    "        sentences.append(' '.join(words))\n",
    "        words = []\n",
    "        \n",
    "    last_lang = row.Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00797992-b49d-47d1-bc47-6cbffc86ccf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58a392efa5c4c299e087a74313d4e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1974 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1974\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_dataset = HFDataset.from_pandas(test).remove_columns(column_names=['Id', 'Language', 'Pos'])\n",
    "# test_dataset = HFDataset.from_pandas(test_ner).remove_columns(column_names=['Language', 'family', 'region'])\n",
    "test_dataset = HFDataset.from_pandas(pd.DataFrame({'Word': sentences}))\n",
    "test_dataset = test_dataset.map(\n",
    "    process_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=['Word'],\n",
    ")\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94e34428-b18d-4949-83cc-e625b0ecf5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(self, model, optimizer, *, adv_param='weight',\n",
    "                 adv_lr=0.001, adv_eps=0.001):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.backup = {}\n",
    "\n",
    "    def perturb(self):\n",
    "        \"\"\"\n",
    "        Perturb model parameters for AWP gradient\n",
    "        Call before loss and loss.backward()\n",
    "        \"\"\"\n",
    "        self._save()  # save model parameters\n",
    "        self._attack_step()  # perturb weights\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                grad = self.optimizer.state[param]['exp_avg']\n",
    "                norm_grad = torch.norm(grad)\n",
    "                norm_data = torch.norm(param.detach())\n",
    "\n",
    "                if norm_grad != 0 and not torch.isnan(norm_grad):\n",
    "                    # Set lower and upper limit in change\n",
    "                    limit_eps = self.adv_eps * param.detach().abs()\n",
    "                    param_min = param.data - limit_eps\n",
    "                    param_max = param.data + limit_eps\n",
    "\n",
    "                    # Perturb along gradient\n",
    "                    # w += (adv_lr * |w| / |grad|) * grad\n",
    "                    param.data.add_(grad, alpha=(self.adv_lr * (norm_data + e) / (norm_grad + e)))\n",
    "\n",
    "                    # Apply the limit to the change\n",
    "                    param.data.clamp_(param_min, param_max)\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.clone().detach()\n",
    "                else:\n",
    "                    self.backup[name].copy_(param.data)\n",
    "\n",
    "    def restore(self):\n",
    "        \"\"\"\n",
    "        Restore model parameter to correct position; AWP do not perturbe weights, it perturb gradients\n",
    "        Call after loss.backward(), before optimizer.step()\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data.copy_(self.backup[name])\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.awp = AWP(self.model, self.optimizer, adv_lr=0.001, adv_eps=0.001)\n",
    "    \n",
    "    def training_step(self, model: nn.Module, inputs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        # if is_sagemaker_mp_enabled():\n",
    "        #     loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n",
    "        #     return loss_mb.reduce_mean().detach().to(self.args.device)\n",
    "        \n",
    "        self.awp.perturb() # \n",
    "        \n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        if self.do_grad_scaling:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            self.accelerator.backward(loss)\n",
    "            \n",
    "        self.awp.restore()\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            if is_peft_available() and isinstance(model, PeftModel):\n",
    "                model_name = unwrap_model(model.base_model)._get_name()\n",
    "            else:\n",
    "                model_name = unwrap_model(model)._get_name()\n",
    "            if model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n",
    "                loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
    "            else:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
    "                raise ValueError(\n",
    "                    \"The model did not return a loss from the inputs, only the following keys: \"\n",
    "                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
    "                )\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c3ab5b5-1122-4b5a-bf90-b3c07e34c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(fold):\n",
    "    raw_dataset = DatasetDict({\n",
    "        'train': HFDataset.from_pandas(train_ner[train_ner.fold != fold]),\n",
    "        'validation': HFDataset.from_pandas(train_ner[train_ner.fold == fold])\n",
    "    }).remove_columns(column_names=['fold', '__index_level_0__', 'Language', 'family', 'region'])\n",
    "\n",
    "    train_dataset = raw_dataset[\"train\"]\n",
    "    column_names = train_dataset.column_names\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        process_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "    )\n",
    "    if max_seq_length is not None:\n",
    "        train_dataset = train_dataset.filter(\n",
    "            lambda example: len(example['input_ids']) <= max_seq_length\n",
    "        )\n",
    "\n",
    "    eval_dataset = raw_dataset['validation']\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        process_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "    )\n",
    "    if max_seq_length is not None:\n",
    "        eval_dataset = eval_dataset.filter(\n",
    "            lambda example: len(example['input_ids']) <= max_seq_length\n",
    "        )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'pos-tagging-ner/{model_name.replace(\"/\", \"-\")}/{fold}',\n",
    "        learning_rate=2e-5,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_grad_norm=10,\n",
    "        group_by_length=True,\n",
    "        overwrite_output_dir=True,\n",
    "        warmup_steps=0.15,\n",
    "        num_train_epochs=5,\n",
    "        lr_scheduler_type ='cosine',\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,\n",
    "        fp16=True,\n",
    "        report_to='none'\n",
    "    )\n",
    "    \n",
    "    # model_path = glob(f'./pos-tagging-ner/{model_name.replace(\"/\", \"-\")}/{fold}/checkpoint-*')[0]\n",
    "    # model = load_model(model_path)\n",
    "    \n",
    "    model = load_model(model_name)\n",
    "\n",
    "    # trainer = Trainer(\n",
    "    trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    results = trainer.evaluate(eval_dataset)\n",
    "    \n",
    "    test_pos_ids = trainer.predict(test_dataset)\n",
    "    \n",
    "    return results['eval_accuracy'], test_pos_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65bf22a7-cfe7-454b-a739-20bfc29d1430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6372ad63574e92a65f51765fd5838d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/97131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12677c134e13433fb5c8233589576c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/97131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252c4292963d43d39aa55840174a8231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c388dbc59d43414ab17fd8d36762a208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15180' max='15180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15180/15180 56:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.636600</td>\n",
       "      <td>0.587893</td>\n",
       "      <td>0.601599</td>\n",
       "      <td>0.590742</td>\n",
       "      <td>0.596121</td>\n",
       "      <td>0.649221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.253300</td>\n",
       "      <td>0.574321</td>\n",
       "      <td>0.610973</td>\n",
       "      <td>0.589819</td>\n",
       "      <td>0.600210</td>\n",
       "      <td>0.657664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.203000</td>\n",
       "      <td>0.600903</td>\n",
       "      <td>0.605519</td>\n",
       "      <td>0.593350</td>\n",
       "      <td>0.599373</td>\n",
       "      <td>0.652360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.610776</td>\n",
       "      <td>0.617928</td>\n",
       "      <td>0.600769</td>\n",
       "      <td>0.609227</td>\n",
       "      <td>0.661993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.153900</td>\n",
       "      <td>0.626613</td>\n",
       "      <td>0.617518</td>\n",
       "      <td>0.602722</td>\n",
       "      <td>0.610030</td>\n",
       "      <td>0.664062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.139800</td>\n",
       "      <td>0.607242</td>\n",
       "      <td>0.629113</td>\n",
       "      <td>0.603493</td>\n",
       "      <td>0.616037</td>\n",
       "      <td>0.671192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.609423</td>\n",
       "      <td>0.617966</td>\n",
       "      <td>0.598851</td>\n",
       "      <td>0.608258</td>\n",
       "      <td>0.664078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.641358</td>\n",
       "      <td>0.612570</td>\n",
       "      <td>0.596988</td>\n",
       "      <td>0.604679</td>\n",
       "      <td>0.659426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>0.653946</td>\n",
       "      <td>0.615222</td>\n",
       "      <td>0.598547</td>\n",
       "      <td>0.606770</td>\n",
       "      <td>0.661672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.627178</td>\n",
       "      <td>0.613971</td>\n",
       "      <td>0.599541</td>\n",
       "      <td>0.606671</td>\n",
       "      <td>0.660674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.638450</td>\n",
       "      <td>0.607345</td>\n",
       "      <td>0.597310</td>\n",
       "      <td>0.602286</td>\n",
       "      <td>0.651483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.632359</td>\n",
       "      <td>0.610444</td>\n",
       "      <td>0.598099</td>\n",
       "      <td>0.604208</td>\n",
       "      <td>0.658026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.095300</td>\n",
       "      <td>0.647423</td>\n",
       "      <td>0.611518</td>\n",
       "      <td>0.600285</td>\n",
       "      <td>0.605849</td>\n",
       "      <td>0.659845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.094600</td>\n",
       "      <td>0.628438</td>\n",
       "      <td>0.611495</td>\n",
       "      <td>0.598296</td>\n",
       "      <td>0.604823</td>\n",
       "      <td>0.657833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.630569</td>\n",
       "      <td>0.613769</td>\n",
       "      <td>0.599783</td>\n",
       "      <td>0.606695</td>\n",
       "      <td>0.660247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.639459</td>\n",
       "      <td>0.614797</td>\n",
       "      <td>0.601477</td>\n",
       "      <td>0.608064</td>\n",
       "      <td>0.661470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>0.629396</td>\n",
       "      <td>0.612800</td>\n",
       "      <td>0.598672</td>\n",
       "      <td>0.605654</td>\n",
       "      <td>0.659909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.629165</td>\n",
       "      <td>0.615753</td>\n",
       "      <td>0.598412</td>\n",
       "      <td>0.606959</td>\n",
       "      <td>0.661438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.080700</td>\n",
       "      <td>0.631083</td>\n",
       "      <td>0.619349</td>\n",
       "      <td>0.603188</td>\n",
       "      <td>0.611162</td>\n",
       "      <td>0.664239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.627358</td>\n",
       "      <td>0.615081</td>\n",
       "      <td>0.600796</td>\n",
       "      <td>0.607855</td>\n",
       "      <td>0.660223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.642647</td>\n",
       "      <td>0.616383</td>\n",
       "      <td>0.602283</td>\n",
       "      <td>0.609252</td>\n",
       "      <td>0.661913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.636728</td>\n",
       "      <td>0.616747</td>\n",
       "      <td>0.601378</td>\n",
       "      <td>0.608965</td>\n",
       "      <td>0.662171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.077700</td>\n",
       "      <td>0.640517</td>\n",
       "      <td>0.613650</td>\n",
       "      <td>0.600536</td>\n",
       "      <td>0.607022</td>\n",
       "      <td>0.659491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.639718</td>\n",
       "      <td>0.613106</td>\n",
       "      <td>0.599658</td>\n",
       "      <td>0.606307</td>\n",
       "      <td>0.659136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>0.648332</td>\n",
       "      <td>0.614208</td>\n",
       "      <td>0.600724</td>\n",
       "      <td>0.607391</td>\n",
       "      <td>0.659901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.648511</td>\n",
       "      <td>0.614053</td>\n",
       "      <td>0.600616</td>\n",
       "      <td>0.607260</td>\n",
       "      <td>0.659153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.651762</td>\n",
       "      <td>0.613762</td>\n",
       "      <td>0.600151</td>\n",
       "      <td>0.606880</td>\n",
       "      <td>0.659853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.648791</td>\n",
       "      <td>0.613587</td>\n",
       "      <td>0.599792</td>\n",
       "      <td>0.606611</td>\n",
       "      <td>0.659370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.649222</td>\n",
       "      <td>0.613519</td>\n",
       "      <td>0.599792</td>\n",
       "      <td>0.606578</td>\n",
       "      <td>0.659193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.648727</td>\n",
       "      <td>0.613444</td>\n",
       "      <td>0.599801</td>\n",
       "      <td>0.606546</td>\n",
       "      <td>0.659209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.671192306144622\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef533a251054847b9b32cd1fc467bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/97498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ab51dd5e0b49d3a963ddd8830f4961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/97498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac9c5e0611441cf8c4c3bef994f15ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc48ee86d924ee2b5974503ac510362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15235' max='15235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15235/15235 59:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.677500</td>\n",
       "      <td>0.584889</td>\n",
       "      <td>0.621388</td>\n",
       "      <td>0.614034</td>\n",
       "      <td>0.617689</td>\n",
       "      <td>0.686517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.261800</td>\n",
       "      <td>0.588786</td>\n",
       "      <td>0.655221</td>\n",
       "      <td>0.644184</td>\n",
       "      <td>0.649656</td>\n",
       "      <td>0.717090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.209500</td>\n",
       "      <td>0.565092</td>\n",
       "      <td>0.656330</td>\n",
       "      <td>0.648453</td>\n",
       "      <td>0.652368</td>\n",
       "      <td>0.716070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.177000</td>\n",
       "      <td>0.589295</td>\n",
       "      <td>0.664458</td>\n",
       "      <td>0.652679</td>\n",
       "      <td>0.658516</td>\n",
       "      <td>0.722610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.162100</td>\n",
       "      <td>0.569345</td>\n",
       "      <td>0.671698</td>\n",
       "      <td>0.656753</td>\n",
       "      <td>0.664141</td>\n",
       "      <td>0.728635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.595786</td>\n",
       "      <td>0.663685</td>\n",
       "      <td>0.647020</td>\n",
       "      <td>0.655247</td>\n",
       "      <td>0.721868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.601691</td>\n",
       "      <td>0.672462</td>\n",
       "      <td>0.657735</td>\n",
       "      <td>0.665017</td>\n",
       "      <td>0.729048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.129100</td>\n",
       "      <td>0.557123</td>\n",
       "      <td>0.667727</td>\n",
       "      <td>0.654789</td>\n",
       "      <td>0.661195</td>\n",
       "      <td>0.724483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.589709</td>\n",
       "      <td>0.673289</td>\n",
       "      <td>0.660586</td>\n",
       "      <td>0.666877</td>\n",
       "      <td>0.729513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>0.594187</td>\n",
       "      <td>0.668351</td>\n",
       "      <td>0.656956</td>\n",
       "      <td>0.662605</td>\n",
       "      <td>0.725342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.588470</td>\n",
       "      <td>0.669382</td>\n",
       "      <td>0.658105</td>\n",
       "      <td>0.663696</td>\n",
       "      <td>0.726342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.614190</td>\n",
       "      <td>0.674122</td>\n",
       "      <td>0.663423</td>\n",
       "      <td>0.668729</td>\n",
       "      <td>0.730165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.599392</td>\n",
       "      <td>0.670131</td>\n",
       "      <td>0.660295</td>\n",
       "      <td>0.665177</td>\n",
       "      <td>0.727033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.591472</td>\n",
       "      <td>0.672924</td>\n",
       "      <td>0.658920</td>\n",
       "      <td>0.665848</td>\n",
       "      <td>0.729790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>0.601922</td>\n",
       "      <td>0.670919</td>\n",
       "      <td>0.658396</td>\n",
       "      <td>0.664599</td>\n",
       "      <td>0.728008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.605053</td>\n",
       "      <td>0.669486</td>\n",
       "      <td>0.658062</td>\n",
       "      <td>0.663725</td>\n",
       "      <td>0.726446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.607823</td>\n",
       "      <td>0.675214</td>\n",
       "      <td>0.667460</td>\n",
       "      <td>0.671315</td>\n",
       "      <td>0.731463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.093700</td>\n",
       "      <td>0.602584</td>\n",
       "      <td>0.675648</td>\n",
       "      <td>0.665568</td>\n",
       "      <td>0.670570</td>\n",
       "      <td>0.731456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.593544</td>\n",
       "      <td>0.672329</td>\n",
       "      <td>0.661844</td>\n",
       "      <td>0.667045</td>\n",
       "      <td>0.728802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>0.603087</td>\n",
       "      <td>0.673124</td>\n",
       "      <td>0.662652</td>\n",
       "      <td>0.667847</td>\n",
       "      <td>0.729868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.601103</td>\n",
       "      <td>0.675845</td>\n",
       "      <td>0.666288</td>\n",
       "      <td>0.671033</td>\n",
       "      <td>0.732334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.607113</td>\n",
       "      <td>0.677040</td>\n",
       "      <td>0.666550</td>\n",
       "      <td>0.671754</td>\n",
       "      <td>0.733529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.599176</td>\n",
       "      <td>0.674351</td>\n",
       "      <td>0.663241</td>\n",
       "      <td>0.668750</td>\n",
       "      <td>0.731191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.604479</td>\n",
       "      <td>0.676865</td>\n",
       "      <td>0.664295</td>\n",
       "      <td>0.670521</td>\n",
       "      <td>0.732715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>0.608041</td>\n",
       "      <td>0.676400</td>\n",
       "      <td>0.665197</td>\n",
       "      <td>0.670752</td>\n",
       "      <td>0.732463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.077900</td>\n",
       "      <td>0.607321</td>\n",
       "      <td>0.674851</td>\n",
       "      <td>0.662957</td>\n",
       "      <td>0.668851</td>\n",
       "      <td>0.731333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.607967</td>\n",
       "      <td>0.674908</td>\n",
       "      <td>0.662797</td>\n",
       "      <td>0.668798</td>\n",
       "      <td>0.731327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.078700</td>\n",
       "      <td>0.609393</td>\n",
       "      <td>0.675483</td>\n",
       "      <td>0.663721</td>\n",
       "      <td>0.669550</td>\n",
       "      <td>0.731811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.608876</td>\n",
       "      <td>0.675713</td>\n",
       "      <td>0.664114</td>\n",
       "      <td>0.669863</td>\n",
       "      <td>0.731979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.608956</td>\n",
       "      <td>0.675513</td>\n",
       "      <td>0.663888</td>\n",
       "      <td>0.669650</td>\n",
       "      <td>0.731844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7335287584906635\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924b0df8936f4f1fbb764de26178439d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/100961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8eedd6dcd8f42b5b633a5a0c2a8ae80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d394f412774d52a976270b0c081626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1ac51f045c4728bf2a67061219f29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15780' max='15780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15780/15780 55:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.670600</td>\n",
       "      <td>0.573646</td>\n",
       "      <td>0.597864</td>\n",
       "      <td>0.596832</td>\n",
       "      <td>0.597348</td>\n",
       "      <td>0.658925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>0.597667</td>\n",
       "      <td>0.617998</td>\n",
       "      <td>0.626861</td>\n",
       "      <td>0.622398</td>\n",
       "      <td>0.679575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.216700</td>\n",
       "      <td>0.551349</td>\n",
       "      <td>0.647626</td>\n",
       "      <td>0.656864</td>\n",
       "      <td>0.652212</td>\n",
       "      <td>0.707442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.523812</td>\n",
       "      <td>0.656073</td>\n",
       "      <td>0.665990</td>\n",
       "      <td>0.660994</td>\n",
       "      <td>0.715603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.168700</td>\n",
       "      <td>0.503733</td>\n",
       "      <td>0.665016</td>\n",
       "      <td>0.671625</td>\n",
       "      <td>0.668304</td>\n",
       "      <td>0.721886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.158400</td>\n",
       "      <td>0.520752</td>\n",
       "      <td>0.662358</td>\n",
       "      <td>0.667932</td>\n",
       "      <td>0.665133</td>\n",
       "      <td>0.720425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>0.484952</td>\n",
       "      <td>0.672685</td>\n",
       "      <td>0.682350</td>\n",
       "      <td>0.677483</td>\n",
       "      <td>0.729362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>0.520577</td>\n",
       "      <td>0.666337</td>\n",
       "      <td>0.674988</td>\n",
       "      <td>0.670635</td>\n",
       "      <td>0.725371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.127400</td>\n",
       "      <td>0.529319</td>\n",
       "      <td>0.667568</td>\n",
       "      <td>0.676879</td>\n",
       "      <td>0.672191</td>\n",
       "      <td>0.725056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>0.533949</td>\n",
       "      <td>0.669783</td>\n",
       "      <td>0.679329</td>\n",
       "      <td>0.674522</td>\n",
       "      <td>0.727585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.120700</td>\n",
       "      <td>0.527533</td>\n",
       "      <td>0.666272</td>\n",
       "      <td>0.679050</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.724685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.115300</td>\n",
       "      <td>0.554944</td>\n",
       "      <td>0.667385</td>\n",
       "      <td>0.675737</td>\n",
       "      <td>0.671535</td>\n",
       "      <td>0.725742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.109500</td>\n",
       "      <td>0.544796</td>\n",
       "      <td>0.662394</td>\n",
       "      <td>0.669683</td>\n",
       "      <td>0.666019</td>\n",
       "      <td>0.721437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.546881</td>\n",
       "      <td>0.667892</td>\n",
       "      <td>0.680116</td>\n",
       "      <td>0.673949</td>\n",
       "      <td>0.725169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.546381</td>\n",
       "      <td>0.662117</td>\n",
       "      <td>0.669848</td>\n",
       "      <td>0.665960</td>\n",
       "      <td>0.720728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.565168</td>\n",
       "      <td>0.656074</td>\n",
       "      <td>0.664226</td>\n",
       "      <td>0.660124</td>\n",
       "      <td>0.717266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.100800</td>\n",
       "      <td>0.548033</td>\n",
       "      <td>0.659576</td>\n",
       "      <td>0.666599</td>\n",
       "      <td>0.663069</td>\n",
       "      <td>0.719964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.548485</td>\n",
       "      <td>0.663977</td>\n",
       "      <td>0.671511</td>\n",
       "      <td>0.667723</td>\n",
       "      <td>0.724067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.532262</td>\n",
       "      <td>0.669664</td>\n",
       "      <td>0.677704</td>\n",
       "      <td>0.673660</td>\n",
       "      <td>0.728867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>0.566921</td>\n",
       "      <td>0.660939</td>\n",
       "      <td>0.668833</td>\n",
       "      <td>0.664863</td>\n",
       "      <td>0.721246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.087800</td>\n",
       "      <td>0.545543</td>\n",
       "      <td>0.661894</td>\n",
       "      <td>0.670051</td>\n",
       "      <td>0.665948</td>\n",
       "      <td>0.722437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.085900</td>\n",
       "      <td>0.554460</td>\n",
       "      <td>0.661855</td>\n",
       "      <td>0.669785</td>\n",
       "      <td>0.665796</td>\n",
       "      <td>0.722212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.086600</td>\n",
       "      <td>0.553258</td>\n",
       "      <td>0.664561</td>\n",
       "      <td>0.672970</td>\n",
       "      <td>0.668739</td>\n",
       "      <td>0.724640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.533274</td>\n",
       "      <td>0.668891</td>\n",
       "      <td>0.675547</td>\n",
       "      <td>0.672202</td>\n",
       "      <td>0.728395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.543601</td>\n",
       "      <td>0.666353</td>\n",
       "      <td>0.674760</td>\n",
       "      <td>0.670530</td>\n",
       "      <td>0.726169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>0.542321</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.677679</td>\n",
       "      <td>0.673159</td>\n",
       "      <td>0.728249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>0.545336</td>\n",
       "      <td>0.666128</td>\n",
       "      <td>0.674912</td>\n",
       "      <td>0.670491</td>\n",
       "      <td>0.726214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.545588</td>\n",
       "      <td>0.666834</td>\n",
       "      <td>0.675382</td>\n",
       "      <td>0.671081</td>\n",
       "      <td>0.726551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.547305</td>\n",
       "      <td>0.666575</td>\n",
       "      <td>0.674722</td>\n",
       "      <td>0.670623</td>\n",
       "      <td>0.726473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>0.547556</td>\n",
       "      <td>0.667005</td>\n",
       "      <td>0.675166</td>\n",
       "      <td>0.671061</td>\n",
       "      <td>0.726765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.548756</td>\n",
       "      <td>0.666274</td>\n",
       "      <td>0.674569</td>\n",
       "      <td>0.670396</td>\n",
       "      <td>0.726281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7293615107913669\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32970b02f924332a906b83d91757be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/96524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e9683a26b54bd18480ea87bc6c8177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/96524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee5876eb1784980b85d33f32e826f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9e1d4e89294fabb9c8e9a4dced04af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15085' max='15085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15085/15085 58:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.673600</td>\n",
       "      <td>0.540364</td>\n",
       "      <td>0.637809</td>\n",
       "      <td>0.658098</td>\n",
       "      <td>0.647795</td>\n",
       "      <td>0.706981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.571079</td>\n",
       "      <td>0.659790</td>\n",
       "      <td>0.677493</td>\n",
       "      <td>0.668525</td>\n",
       "      <td>0.723739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.195900</td>\n",
       "      <td>0.581279</td>\n",
       "      <td>0.669521</td>\n",
       "      <td>0.684029</td>\n",
       "      <td>0.676697</td>\n",
       "      <td>0.731754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.630932</td>\n",
       "      <td>0.654093</td>\n",
       "      <td>0.669039</td>\n",
       "      <td>0.661481</td>\n",
       "      <td>0.715849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.601002</td>\n",
       "      <td>0.655047</td>\n",
       "      <td>0.671288</td>\n",
       "      <td>0.663068</td>\n",
       "      <td>0.719030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>0.621388</td>\n",
       "      <td>0.677106</td>\n",
       "      <td>0.689671</td>\n",
       "      <td>0.683331</td>\n",
       "      <td>0.737214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.629616</td>\n",
       "      <td>0.670292</td>\n",
       "      <td>0.685416</td>\n",
       "      <td>0.677770</td>\n",
       "      <td>0.732499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>0.613932</td>\n",
       "      <td>0.668659</td>\n",
       "      <td>0.681598</td>\n",
       "      <td>0.675067</td>\n",
       "      <td>0.728894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.623257</td>\n",
       "      <td>0.671306</td>\n",
       "      <td>0.684385</td>\n",
       "      <td>0.677783</td>\n",
       "      <td>0.732983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>0.618645</td>\n",
       "      <td>0.673615</td>\n",
       "      <td>0.688978</td>\n",
       "      <td>0.681210</td>\n",
       "      <td>0.735098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.592337</td>\n",
       "      <td>0.668999</td>\n",
       "      <td>0.683567</td>\n",
       "      <td>0.676204</td>\n",
       "      <td>0.730944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.618065</td>\n",
       "      <td>0.672078</td>\n",
       "      <td>0.686760</td>\n",
       "      <td>0.679340</td>\n",
       "      <td>0.733723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.639444</td>\n",
       "      <td>0.672712</td>\n",
       "      <td>0.687184</td>\n",
       "      <td>0.679871</td>\n",
       "      <td>0.733587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.094100</td>\n",
       "      <td>0.615327</td>\n",
       "      <td>0.671234</td>\n",
       "      <td>0.685297</td>\n",
       "      <td>0.678193</td>\n",
       "      <td>0.732809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.093800</td>\n",
       "      <td>0.607546</td>\n",
       "      <td>0.671927</td>\n",
       "      <td>0.684766</td>\n",
       "      <td>0.678286</td>\n",
       "      <td>0.732521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.627218</td>\n",
       "      <td>0.671795</td>\n",
       "      <td>0.684229</td>\n",
       "      <td>0.677955</td>\n",
       "      <td>0.733342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.634045</td>\n",
       "      <td>0.673610</td>\n",
       "      <td>0.687428</td>\n",
       "      <td>0.680449</td>\n",
       "      <td>0.734614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.609923</td>\n",
       "      <td>0.679165</td>\n",
       "      <td>0.690627</td>\n",
       "      <td>0.684848</td>\n",
       "      <td>0.739122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.619573</td>\n",
       "      <td>0.682021</td>\n",
       "      <td>0.694589</td>\n",
       "      <td>0.688248</td>\n",
       "      <td>0.742075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.634785</td>\n",
       "      <td>0.673954</td>\n",
       "      <td>0.686553</td>\n",
       "      <td>0.680195</td>\n",
       "      <td>0.734968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.078200</td>\n",
       "      <td>0.625815</td>\n",
       "      <td>0.676993</td>\n",
       "      <td>0.690534</td>\n",
       "      <td>0.683696</td>\n",
       "      <td>0.738078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.641961</td>\n",
       "      <td>0.676942</td>\n",
       "      <td>0.690452</td>\n",
       "      <td>0.683631</td>\n",
       "      <td>0.737795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>0.632931</td>\n",
       "      <td>0.678732</td>\n",
       "      <td>0.692214</td>\n",
       "      <td>0.685407</td>\n",
       "      <td>0.739432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.636028</td>\n",
       "      <td>0.674927</td>\n",
       "      <td>0.688747</td>\n",
       "      <td>0.681767</td>\n",
       "      <td>0.736126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.073100</td>\n",
       "      <td>0.643697</td>\n",
       "      <td>0.675453</td>\n",
       "      <td>0.688153</td>\n",
       "      <td>0.681744</td>\n",
       "      <td>0.736229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.639610</td>\n",
       "      <td>0.674553</td>\n",
       "      <td>0.687872</td>\n",
       "      <td>0.681147</td>\n",
       "      <td>0.735740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.638393</td>\n",
       "      <td>0.676046</td>\n",
       "      <td>0.688909</td>\n",
       "      <td>0.682417</td>\n",
       "      <td>0.736882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.072300</td>\n",
       "      <td>0.641730</td>\n",
       "      <td>0.676142</td>\n",
       "      <td>0.689209</td>\n",
       "      <td>0.682613</td>\n",
       "      <td>0.737061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.642022</td>\n",
       "      <td>0.675624</td>\n",
       "      <td>0.688909</td>\n",
       "      <td>0.682202</td>\n",
       "      <td>0.736746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.074200</td>\n",
       "      <td>0.642131</td>\n",
       "      <td>0.675656</td>\n",
       "      <td>0.688959</td>\n",
       "      <td>0.682243</td>\n",
       "      <td>0.736768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7420747555816558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "all_preds = []\n",
    "\n",
    "for fold in [1, 2, 3, 4]:\n",
    "    score, fold_pred = train_fold(fold)\n",
    "    \n",
    "    scores.append(score)\n",
    "    all_preds.append(fold_pred)\n",
    "    \n",
    "    print(score)\n",
    "    print()\n",
    "    \n",
    "avg_score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b509e41f-8169-484e-af92-3853030b1f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.671192306144622, 0.7335287584906635, 0.7293615107913669, 0.7420747555816558]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f41ddd1d-b6b7-40fb-b1f2-7f5b58e361ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.719039332752077"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5423319-fc1d-4e9c-ab53-0eb0122803dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 190, 18)\n"
     ]
    }
   ],
   "source": [
    "test_pos_ids = np.mean([p.predictions for p in all_preds], axis=0)\n",
    "print(test_pos_ids.shape)\n",
    "test_pos_ids = test_pos_ids.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45ce2daa-b304-4065-8ab6-40fe16f91f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pos = []\n",
    "data_pos = []\n",
    "\n",
    "for pos_ids, sentence in zip(test_pos_ids, sentences):\n",
    "    length = len(sentence.split(' '))\n",
    "    \n",
    "    clean_pos_ids = [x for x in pos_ids if x not in [-100, 17]]\n",
    "    \n",
    "    if 'xlmr' in model_name:\n",
    "        sentence_pos = list(map(id_to_label.get, clean_pos_ids))[1:length+1]\n",
    "    else:\n",
    "        sentence_pos = list(map(id_to_label.get, clean_pos_ids))[:length]\n",
    "    \n",
    "    final_pos.extend(sentence_pos)\n",
    "    data_pos.append(' '.join(sentence_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab63a58c-f2ec-43d8-8f79-fbc90939c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Pos'] = final_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55dd1deb-7091-4db3-8cf9-8cd0147a6346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOUN     6674\n",
       "VERB     4875\n",
       "ADP      4350\n",
       "AUX      2988\n",
       "PROPN    2988\n",
       "PUNCT    2964\n",
       "PRON     1486\n",
       "SCONJ    1257\n",
       "DET       912\n",
       "ADV       875\n",
       "PART      720\n",
       "CCONJ     673\n",
       "ADJ       654\n",
       "NUM       566\n",
       "X          27\n",
       "INTJ       22\n",
       "SYM        14\n",
       "Name: Pos, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Pos.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ff57974-cd0b-47c0-9356-e08ceb7af5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "      <th>Language</th>\n",
       "      <th>Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Id00qog2f11n_0</td>\n",
       "      <td>Ne</td>\n",
       "      <td>luo</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Id00qog2f11n_1</td>\n",
       "      <td>otim</td>\n",
       "      <td>luo</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Id00qog2f11n_2</td>\n",
       "      <td>penj</td>\n",
       "      <td>luo</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Id00qog2f11n_3</td>\n",
       "      <td>e</td>\n",
       "      <td>luo</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Id00qog2f11n_4</td>\n",
       "      <td>kind</td>\n",
       "      <td>luo</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  Word Language   Pos\n",
       "0  Id00qog2f11n_0    Ne      luo   AUX\n",
       "1  Id00qog2f11n_1  otim      luo  VERB\n",
       "2  Id00qog2f11n_2  penj      luo  NOUN\n",
       "3  Id00qog2f11n_3     e      luo   ADP\n",
       "4  Id00qog2f11n_4  kind      luo  NOUN"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd03bca7-647e-41ce-84c7-6ffbf1096b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['Id', 'Pos']].to_csv(f'submissions/ps-round2-pos-ner-{model_name.split(\"/\")[-1]}-{avg_score:.3f}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af1392-9850-4c05-97bb-510eae24ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({'Word': sentences, 'Pos': data_pos})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c85bbd-29a8-40f8-933b-5e45111e84be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test[test.Pos == 'SYM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b6f72-85c4-4881-8319-c59d5dd08de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
