{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a239eea-e414-412d-9242-d7316e9cc18d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a7fcd7-eb6b-4ee8-8f57-ff5a443c78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch==1.13.1 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6eb2fe-f515-471e-8ae3-1aae649d697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c866a9c-6192-4cf1-b8e7-1b96c98953f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1961ba0b-d202-4477-8004-63b1c5e5ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 15 14:45:40 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   32C    P8    22W / 300W |  22947MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a20d7-05ea-4951-a079-08c8b8a71940",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16825bfc-8853-445c-9e6c-53395bea41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "import subprocess\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedGroupKFold, GroupKFold\n",
    "from sklearn.metrics import f1_score, mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6e65d9-657c-4b4d-a20c-ccd0dc3f5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b9b22b-0a41-44b0-beec-e050afbeaa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR\n",
    "from torch import LongTensor\n",
    "from torch import nn, optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4c3033-b975-4aac-bc6b-24346ea9c6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "# from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm, trange\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict, Dataset as HFDataset\n",
    "from datasets import concatenate_datasets, interleave_datasets\n",
    "from datasets import ClassLabel, load_metric\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a120b4-51ef-49dc-9748-873fbd66e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext watermark\n",
    "# %watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563effdb-c378-45ba-8f7e-8d7cc8bf8b08",
   "metadata": {},
   "source": [
    "# Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db742f8-2be3-47d5-bd7b-82629ce77866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_warnings(strict=False):\n",
    "\twarnings.simplefilter('ignore')\n",
    "\tif strict:\n",
    "\t\tlogging.disable(logging.WARNING)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "\trandom.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c01b0cb9-83b7-46c0-8383-c9384772e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "disable_warnings()\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7fe6b-98d6-4d75-9912-bf1bf44b6643",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe357fad-adaf-4b32-8c8a-3bfba4d57170",
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = '../masakhane-pos/data/'\n",
    "add_path = '../masakhane-pos/transfer_corpus/'\n",
    "pseudo_path = './pseudos/pos-ner-afro-xlmr-large-75L-best.csv'\n",
    "\n",
    "langs = sorted(os.listdir(path))\n",
    "add_langs = os.listdir(add_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5bd3e9-2a48-4227-af3e-24fde850a994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang2family = {\n",
    " 'bam': 'Mande',\n",
    " 'bbj': 'Grassfields',\n",
    " 'ewe': 'Kwa',\n",
    " 'fon': 'Volta-Niger',\n",
    " 'hau': 'Chadic',\n",
    " 'ibo': 'Volta-Niger',\n",
    " 'kin': 'Bantu',\n",
    " 'lug': 'Bantu',\n",
    " 'mos': 'Gur',\n",
    " 'nya': 'Bantu',\n",
    " 'pcm': 'English-Creole',\n",
    " 'sna': 'Bantu',\n",
    " 'swa': 'Bantu',\n",
    " 'twi': 'Kwa',\n",
    " 'wol': 'Senegambia',\n",
    " 'xho': 'Bantu',\n",
    " 'yor': 'Volta-Niger',\n",
    " 'zul': 'Bantu'\n",
    "}\n",
    "\n",
    "lang2region = {\n",
    "    'bam': 'West',\n",
    "    'bbj': 'Central',\n",
    "    'ewe': 'West',\n",
    "    'fon': 'West',\n",
    "    'hau': 'West',\n",
    "    'ibo': 'West',\n",
    "    'kin': 'East',\n",
    "    'lug': 'East',\n",
    "    'mos': 'West',\n",
    "    'nya': 'East',\n",
    "    'pcm': 'West',\n",
    "    'sna': 'South',\n",
    "    'swa': 'East',\n",
    "    'twi': 'South',\n",
    "    'wol': 'West',\n",
    "    'xho': 'South',\n",
    "    'yor': 'West',\n",
    "    'zul': 'South'\n",
    "}\n",
    "\n",
    "mapper = {'NOUN': 0,\n",
    " 'ADJ': 1,\n",
    " 'PUNCT': 2,\n",
    " 'CCONJ': 3,\n",
    " 'PRON': 4,\n",
    " 'ADV': 5,\n",
    " 'AUX': 6,\n",
    " 'VERB': 7,\n",
    " 'ADP': 8,\n",
    " 'PART': 9,\n",
    " 'SCONJ': 10,\n",
    " 'PROPN': 11,\n",
    " 'X': 12,\n",
    " 'NUM': 13,\n",
    " 'INTJ': 14,\n",
    " 'SYM': 15,\n",
    " 'DET': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d2ead2-2302-480b-82b6-37346e9e07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2fix = {\n",
    "    'yo': 'yor',\n",
    "    'bm': 'bam'\n",
    "}\n",
    "\n",
    "def load_lang_data(data_path, lang, split='train'):\n",
    "    data = []\n",
    "    language = lang.split('_')[0]\n",
    "    language = lang2fix.get(language, language)\n",
    "    \n",
    "    try:\n",
    "        with open(f'{data_path}{lang}/{split}.txt', 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                splits = line.split(' ')\n",
    "                if len(splits) != 2:\n",
    "                    continue\n",
    "                data.append(splits)\n",
    "        data = np.array(data).squeeze()\n",
    "        data = pd.DataFrame({\n",
    "            'Word': data[:, 0],\n",
    "            'Pos': data[:, 1],\n",
    "            'Language': language.split('_')[0]\n",
    "        })\n",
    "        data['family'] = lang2family.get(language, language)\n",
    "        data['region'] = lang2region.get(language, language)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d23f39a4-6053-418d-b865-aaf8f4af5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lang_data_ner(data_path, lang, split='train'):\n",
    "    data = []\n",
    "    language = lang.split('_')[0]\n",
    "    language = lang2fix.get(language, language)\n",
    "    \n",
    "    try:\n",
    "        with open(f'{data_path}{lang}/{split}.txt', 'r') as f:\n",
    "            sentences, labels = [], []\n",
    "            \n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                splits = line.split(' ')\n",
    "                \n",
    "                if len(splits) == 2:\n",
    "                    sentences.append(splits[0])\n",
    "                    labels.append(splits[1])\n",
    "                elif len(splits) == 1:\n",
    "                    data.append(\n",
    "                        [\n",
    "                            ' '.join(sentences),\n",
    "                            ' '.join(labels),\n",
    "                        ]\n",
    "                    )\n",
    "                    \n",
    "                    sentences, labels = [], []\n",
    "\n",
    "        data = np.array(data).squeeze()\n",
    "        data = pd.DataFrame({\n",
    "            'Word': data[:, 0],\n",
    "            'Pos': data[:, 1],\n",
    "            'Language': language.split('_')[0]\n",
    "        })\n",
    "        data['family'] = lang2family.get(language, language)\n",
    "        data['region'] = lang2region.get(language, language)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "378e3114-de98-4f91-a64f-e7ebf677b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):  \n",
    "    # pos = df.Pos.unique()\n",
    "    use_pos = ['PUNCT', 'SYM']\n",
    "    \n",
    "    df_others = df[~df.Pos.isin(use_pos)]\n",
    "    df_toclean = df[df.Pos.isin(use_pos)]\n",
    "    \n",
    "    dfs = [df_others]\n",
    "    \n",
    "    for p in use_pos:\n",
    "        df_p = df_toclean[df_toclean.Pos == p]\n",
    "        words = df_p.Word.value_counts()[df_p.Word.value_counts() < 2].index.values\n",
    "        df_p = df_p[~df_p.Word.isin(words)]\n",
    "        \n",
    "        dfs.append(df_p)\n",
    "        \n",
    "    return pd.concat(dfs).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f54e1e-1c04-40a8-b297-d7bf05d901d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_data(with_pseudo=False):\n",
    "    train = pd.concat([load_lang_data_ner(path, lang, split) for lang in langs for split in ['train', 'dev', 'test']], axis=0).dropna().reset_index(drop=True)\n",
    "    add_data = (\n",
    "        pd.concat([load_lang_data_ner(add_path, lang, split) for lang in add_langs for split in ['train', 'dev', 'test']], axis=0)\n",
    "        .dropna()\n",
    "        .drop_duplicates(subset=['Word', 'Pos'], keep='first')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    train = pd.concat([train, add_data])\n",
    "    if with_pseudo:\n",
    "        train = pd.concat([train, pd.read_csv(pseudo_path)])\n",
    "    train = train.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return train\n",
    "\n",
    "def load_pos_data():\n",
    "    train = pd.concat([load_lang_data(path, lang, split) for lang in langs for split in ['train', 'dev', 'test']], axis=0).dropna().reset_index(drop=True)\n",
    "    add_data = (\n",
    "        pd.concat([load_lang_data(add_path, lang, split) for lang in add_langs for split in ['train', 'dev', 'test']], axis=0)\n",
    "        .dropna()\n",
    "        .drop_duplicates(subset=['Word', 'Pos'], keep='first')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    train = pd.concat([train, add_data])\n",
    "    train = train[train.Pos.isin(list(mapper.keys()))]\n",
    "\n",
    "    train = train.drop_duplicates().drop_duplicates(subset=['Word', 'Language'], keep='first').reset_index(drop=True)\n",
    "    # train = train.drop_duplicates().reset_index(drop=True)\n",
    "    # train = clean_data(train)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd40436e-a4f4-4823-99b3-8ff6c05636de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muso</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ŋana</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afiriki</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tilebinyanfan</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word    Pos Language family region\n",
       "0           Muso   NOUN      bam  Mande   West\n",
       "1           ŋana    ADJ      bam  Mande   West\n",
       "2              ,  PUNCT      bam  Mande   West\n",
       "3        Afiriki   NOUN      bam  Mande   West\n",
       "4  tilebinyanfan   NOUN      bam  Mande   West"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_pseudo = True\n",
    "\n",
    "train_ner = load_ner_data(with_pseudo)\n",
    "train = load_pos_data()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fc60dcc-029f-4508-85ac-070e6a79b23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muso ŋana , Afiriki tilebinyanfan n' a cɛmancɛ...</td>\n",
       "      <td>NOUN ADJ PUNCT NOUN NOUN CCONJ PRON NOUN ADV C...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ni mɔgɔ ka dɔgɔn kojugu , i bɛ mɔgɔw juguya mi...</td>\n",
       "      <td>CCONJ NOUN PART ADJ NOUN PUNCT PRON AUX NOUN N...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A kɔrɔtalen a ka taɲɛ fɛ kow ɲɛmɔgɔyabaaraw la...</td>\n",
       "      <td>PRON VERB PRON PART NOUN PART NOUN NOUN PART C...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ale y' a ( basikɛti ) to a ka se ka bɔ a ka so...</td>\n",
       "      <td>PRON PART PRON PUNCT NOUN PUNCT VERB PRON PART...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sannayɛlɛn galabukɛnɛya a sera ka min fiyɛ Afi...</td>\n",
       "      <td>NOUN NOUN PRON VERB PART PRON VERB PROPN NOUN ...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Muso ŋana , Afiriki tilebinyanfan n' a cɛmancɛ...   \n",
       "1  Ni mɔgɔ ka dɔgɔn kojugu , i bɛ mɔgɔw juguya mi...   \n",
       "2  A kɔrɔtalen a ka taɲɛ fɛ kow ɲɛmɔgɔyabaaraw la...   \n",
       "3  Ale y' a ( basikɛti ) to a ka se ka bɔ a ka so...   \n",
       "4  Sannayɛlɛn galabukɛnɛya a sera ka min fiyɛ Afi...   \n",
       "\n",
       "                                                 Pos Language family region  \n",
       "0  NOUN ADJ PUNCT NOUN NOUN CCONJ PRON NOUN ADV C...      bam  Mande   West  \n",
       "1  CCONJ NOUN PART ADJ NOUN PUNCT PRON AUX NOUN N...      bam  Mande   West  \n",
       "2  PRON VERB PRON PART NOUN PART NOUN NOUN PART C...      bam  Mande   West  \n",
       "3  PRON PART PRON PUNCT NOUN PUNCT VERB PRON PART...      bam  Mande   West  \n",
       "4  NOUN NOUN PRON VERB PART PRON VERB PROPN NOUN ...      bam  Mande   West  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1e73767-4a99-4f9f-b4dc-7c7d6eeb8ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "      <th>Language</th>\n",
       "      <th>Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Id00qog2f11n_0</td>\n",
       "      <td>Ne</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Id00qog2f11n_1</td>\n",
       "      <td>otim</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Id00qog2f11n_2</td>\n",
       "      <td>penj</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Id00qog2f11n_3</td>\n",
       "      <td>e</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Id00qog2f11n_4</td>\n",
       "      <td>kind</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  Word Language  Pos\n",
       "0  Id00qog2f11n_0    Ne      luo  NaN\n",
       "1  Id00qog2f11n_1  otim      luo  NaN\n",
       "2  Id00qog2f11n_2  penj      luo  NaN\n",
       "3  Id00qog2f11n_3     e      luo  NaN\n",
       "4  Id00qog2f11n_4  kind      luo  NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../Test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07d27197-1217-4279-bffa-8052260878dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((255140, 5), (104039, 5), (32045, 4))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, train_ner.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2114cb1d-b750-4c81-b4e9-cdf1b4d7bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = train_ner[train_ner.Language.isin(langs)].reset_index(drop=True)\n",
    "add_train = train_ner[~train_ner.Language.isin(langs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "989646fd-faa8-4d30-92a5-db49b4e2518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['fold'] = -1\n",
    "train_ner['fold'] = -1\n",
    "\n",
    "\n",
    "# val_langs = ['fon', 'pcm', 'twi', 'xho']\n",
    "# train.loc[train.Language.isin(val_langs), 'fold'] = 0\n",
    "\n",
    "base_train['fold'] = base_train['Language'].map(dict(zip(langs, range(len(langs)))))\n",
    "base_train.loc[base_train['fold'].isna(), 'fold'] = -1\n",
    "base_train['fold'] = base_train['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12da99ae-93ad-4f4a-9f4d-87e539c55ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    10338\n",
       "1     6908\n",
       "2     6541\n",
       "3     6133\n",
       "4     7515\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_fold = 5\n",
    "# Fold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "# Fold = StratifiedGroupKFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "Fold = GroupKFold(n_splits=n_fold)\n",
    "\n",
    "base_train['fold'] = -1\n",
    "# for n, (train_index, val_index) in enumerate(Fold.split(train, train['region'], groups=train['Language'])):\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(base_train, base_train['Pos'], groups=base_train['Language'])):\n",
    "    base_train.loc[val_index, 'fold'] = int(n)\n",
    "    \n",
    "base_train['fold'] = base_train['fold'].astype(int)\n",
    "\n",
    "display(base_train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b0cabda-60df-4c10-bb88-93fcd7cddafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bam', 'bbj', 'ewe', 'fon', 'hau', 'ibo', 'kin', 'lug', 'mos',\n",
       "       'nya', 'pcm', 'sna', 'swa', 'twi', 'wol', 'xho', 'yor', 'zul',\n",
       "       'en', 'fr', 'eng-ron-wol-sna', 'ar', 'af', 'luo', 'tsn'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.Language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6f2731d-47d6-4b01-814b-bd5a8d97bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_train['fold'] = -1\n",
    "base_train.loc[base_train.Language.isin(['sna', 'wol']), 'fold'] = -1\n",
    "# base_train.loc[base_train.Language.isin(['sna', 'bam', 'pcm', 'yor', 'wol']), 'fold'] = -1\n",
    "\n",
    "train_ner = pd.concat([base_train, add_train]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef6675b1-ada8-436b-829d-7d24d62d9382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Make anibodi wey dey call say make dem impeach...</td>\n",
       "      <td>VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...</td>\n",
       "      <td>pcm</td>\n",
       "      <td>English-Creole</td>\n",
       "      <td>West</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وزير كورى : حل القضية النووية مع كوريا الشمالي...</td>\n",
       "      <td>NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mano nyalo konyi ahinya e yudo tich .</td>\n",
       "      <td>PRON VERB VERB ADV ADP VERB NOUN PUNCT</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Son nom d' espèce , composé de loyalti et de l...</td>\n",
       "      <td>DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les lemmings sont des êtres expressifs , capab...</td>\n",
       "      <td>DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Make anibodi wey dey call say make dem impeach...   \n",
       "1  وزير كورى : حل القضية النووية مع كوريا الشمالي...   \n",
       "2              Mano nyalo konyi ahinya e yudo tich .   \n",
       "3  Son nom d' espèce , composé de loyalti et de l...   \n",
       "4  Les lemmings sont des êtres expressifs , capab...   \n",
       "\n",
       "                                                 Pos Language          family  \\\n",
       "0  VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...      pcm  English-Creole   \n",
       "1  NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...       ar              ar   \n",
       "2             PRON VERB VERB ADV ADP VERB NOUN PUNCT      luo             luo   \n",
       "3  DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...       fr              fr   \n",
       "4  DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...       fr              fr   \n",
       "\n",
       "  region  fold  \n",
       "0   West     0  \n",
       "1     ar    -1  \n",
       "2    luo    -1  \n",
       "3     fr    -1  \n",
       "4     fr    -1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7a768de-2e21-4900-b470-ea68408a58ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th>Language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">-1</th>\n",
       "      <th>af</th>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng-ron-wol-sna</th>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luo</th>\n",
       "      <td>7244</td>\n",
       "      <td>7244</td>\n",
       "      <td>7244</td>\n",
       "      <td>7244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sna</th>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsn</th>\n",
       "      <td>4936</td>\n",
       "      <td>4936</td>\n",
       "      <td>4936</td>\n",
       "      <td>4936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wol</th>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>pcm</th>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>bam</th>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbj</th>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nya</th>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zul</th>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
       "      <th>ewe</th>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hau</th>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mos</th>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yor</th>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>fon</th>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kin</th>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">4</th>\n",
       "      <th>ibo</th>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lug</th>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swa</th>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twi</th>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xho</th>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Word    Pos  family  region\n",
       "fold Language                                     \n",
       "-1   af                1899   1899    1899    1899\n",
       "     ar                7534   7534    7534    7534\n",
       "     en               15532  15532   15532   15532\n",
       "     eng-ron-wol-sna  13120  13120   13120   13120\n",
       "     fr               16339  16339   16339   16339\n",
       "     luo               7244   7244    7244    7244\n",
       "     sna               1492   1492    1492    1492\n",
       "     tsn               4936   4936    4936    4936\n",
       "     wol               1563   1563    1563    1563\n",
       " 0   pcm              10338  10338   10338   10338\n",
       " 1   bam               2481   2481    2481    2481\n",
       "     bbj               1484   1484    1484    1484\n",
       "     nya               1440   1440    1440    1440\n",
       "     zul               1503   1503    1503    1503\n",
       " 2   ewe               1453   1453    1453    1453\n",
       "     hau               1484   1484    1484    1484\n",
       "     mos               1508   1508    1508    1508\n",
       "     yor               2096   2096    2096    2096\n",
       " 3   fon               1617   1617    1617    1617\n",
       "     kin               1461   1461    1461    1461\n",
       " 4   ibo               1603   1603    1603    1603\n",
       "     lug               1461   1461    1461    1461\n",
       "     swa               1383   1383    1383    1383\n",
       "     twi               1567   1567    1567    1567\n",
       "     xho               1501   1501    1501    1501"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.groupby(['fold', 'Language']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c4eea-ebba-4afe-8bde-2f37646bfb13",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b46cb191-2de4-4698-ace9-95cabd8475a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def paralellize(fct, data, verbose=0, with_tqdm=True):\n",
    "    fn = map(delayed(fct), data)\n",
    "    if with_tqdm:\n",
    "        fn = tqdm(fn, total=len(data))\n",
    "    return Parallel(n_jobs=-1, verbose=verbose, backend=\"multiprocessing\")(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccd222f9-e3e1-4398-994b-33637be80614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self._time = 0\n",
    "\n",
    "    def start(self):\n",
    "        self._time = time()\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        return (time() - self._time) / 60\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de588c-aabf-4134-b3a3-c208e25b642b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8677ddf8-cd35-411b-bb67-397c2f501348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Make anibodi wey dey call say make dem impeach...</td>\n",
       "      <td>VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...</td>\n",
       "      <td>pcm</td>\n",
       "      <td>English-Creole</td>\n",
       "      <td>West</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وزير كورى : حل القضية النووية مع كوريا الشمالي...</td>\n",
       "      <td>NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mano nyalo konyi ahinya e yudo tich .</td>\n",
       "      <td>PRON VERB VERB ADV ADP VERB NOUN PUNCT</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Son nom d' espèce , composé de loyalti et de l...</td>\n",
       "      <td>DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les lemmings sont des êtres expressifs , capab...</td>\n",
       "      <td>DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Make anibodi wey dey call say make dem impeach...   \n",
       "1  وزير كورى : حل القضية النووية مع كوريا الشمالي...   \n",
       "2              Mano nyalo konyi ahinya e yudo tich .   \n",
       "3  Son nom d' espèce , composé de loyalti et de l...   \n",
       "4  Les lemmings sont des êtres expressifs , capab...   \n",
       "\n",
       "                                                 Pos Language          family  \\\n",
       "0  VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...      pcm  English-Creole   \n",
       "1  NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...       ar              ar   \n",
       "2             PRON VERB VERB ADV ADP VERB NOUN PUNCT      luo             luo   \n",
       "3  DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...       fr              fr   \n",
       "4  DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...       fr              fr   \n",
       "\n",
       "  region  fold  \n",
       "0   West     0  \n",
       "1     ar    -1  \n",
       "2    luo    -1  \n",
       "3     fr    -1  \n",
       "4     fr    -1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4e4278d-e53b-4675-a97f-1643fac183bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': 0,\n",
       " 'ADP': 1,\n",
       " 'ADV': 2,\n",
       " 'AUX': 3,\n",
       " 'CCONJ': 4,\n",
       " 'DET': 5,\n",
       " 'INTJ': 6,\n",
       " 'NOUN': 7,\n",
       " 'NUM': 8,\n",
       " 'PART': 9,\n",
       " 'PRON': 10,\n",
       " 'PROPN': 11,\n",
       " 'PUNCT': 12,\n",
       " 'SCONJ': 13,\n",
       " 'SYM': 14,\n",
       " 'VERB': 15,\n",
       " 'X': 16,\n",
       " 'NAW': 17}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_column_name = 'Word'\n",
    "label_column_name = 'Pos'\n",
    "label_list = sorted(train[label_column_name].unique()) + ['NAW']\n",
    "\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9478b6bc-ce17-412b-9bac-ef7b581460f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ADJ',\n",
       " 1: 'ADP',\n",
       " 2: 'ADV',\n",
       " 3: 'AUX',\n",
       " 4: 'CCONJ',\n",
       " 5: 'DET',\n",
       " 6: 'INTJ',\n",
       " 7: 'NOUN',\n",
       " 8: 'NUM',\n",
       " 9: 'PART',\n",
       " 10: 'PRON',\n",
       " 11: 'PROPN',\n",
       " 12: 'PUNCT',\n",
       " 13: 'SCONJ',\n",
       " 14: 'SYM',\n",
       " 15: 'VERB',\n",
       " 16: 'X',\n",
       " 17: 'NAW'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label = {v:k for k,v in label_to_id.items()}\n",
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a78a58e7-22ef-49cc-857e-9811cf748e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        # config=config,\n",
    "        num_labels=num_labels, id2label=id_to_label, label2id=label_to_id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04218f8f-11fc-429e-b70e-c59931217409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'Davlan/afro-xlmr-base'\n",
    "# model_name = 'Davlan/afro-xlmr-large-61L'\n",
    "model_name = 'Davlan/afro-xlmr-large-75L'\n",
    "# model_name = 'masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0'\n",
    "# model_name = 'bonadossou/afrolm_active_learning'\n",
    "\n",
    "max_seq_length = 256\n",
    "padding = False\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "tokenizer_name_or_path = model_name\n",
    "if config.model_type in {\"gpt2\", \"roberta\"}:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name_or_path,\n",
    "        use_fast=True,\n",
    "        add_prefix_space=True,\n",
    "    )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name_or_path,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    \n",
    "# data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, max_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54b5558a-0961-4dc5-ba9e-8f4103accfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(examples):\n",
    "    is_test = examples.get(label_column_name) is None\n",
    "    \n",
    "    for idx in range(len(examples[text_column_name])):\n",
    "        if not is_test:\n",
    "            examples[label_column_name][idx] = examples[label_column_name][idx].split()\n",
    "        examples[text_column_name][idx] = examples[text_column_name][idx].split()\n",
    "        \n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length if not is_test else None,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    \n",
    "    if not is_test:\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[label_column_name]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label_to_id[label[word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(label_to_id['NAW'])\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            labels.append(label_ids)\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8f08892-823c-474a-a3f1-43480ec1213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cee4234b-51b0-4f63-9050-7c226b1596d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l not in [-100, 17]]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    lengths = list(map(len, true_labels))\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l not in [-100, 17]]\n",
    "        for prediction, label, length in zip(predictions, labels, lengths)\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f668b84f-ac49-4378-8cea-b0c9c3ecad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32045/32045 [00:00<00:00, 566496.83it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "words = []\n",
    "\n",
    "last_lang = test.Language.values[0]\n",
    "for i, row in enumerate(tqdm(test.itertuples(), total=len(test))):\n",
    "    words.append(row.Word)\n",
    "    \n",
    "    if row.Word == '.' or i == len(test)-1 or row.Language != last_lang:\n",
    "        sentences.append(' '.join(words))\n",
    "        words = []\n",
    "        \n",
    "    last_lang = row.Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00797992-b49d-47d1-bc47-6cbffc86ccf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538f7b8a4aee4166a4e8d410141c4e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1974 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1974\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_dataset = HFDataset.from_pandas(test).remove_columns(column_names=['Id', 'Language', 'Pos'])\n",
    "# test_dataset = HFDataset.from_pandas(test_ner).remove_columns(column_names=['Language', 'family', 'region'])\n",
    "test_dataset = HFDataset.from_pandas(pd.DataFrame({'Word': sentences}))\n",
    "test_dataset = test_dataset.map(\n",
    "    process_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=['Word'],\n",
    ")\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94e34428-b18d-4949-83cc-e625b0ecf5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(self, model, optimizer, *, adv_param='weight',\n",
    "                 adv_lr=0.001, adv_eps=0.001):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.backup = {}\n",
    "\n",
    "    def perturb(self):\n",
    "        \"\"\"\n",
    "        Perturb model parameters for AWP gradient\n",
    "        Call before loss and loss.backward()\n",
    "        \"\"\"\n",
    "        self._save()  # save model parameters\n",
    "        self._attack_step()  # perturb weights\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                grad = self.optimizer.state[param]['exp_avg']\n",
    "                norm_grad = torch.norm(grad)\n",
    "                norm_data = torch.norm(param.detach())\n",
    "\n",
    "                if norm_grad != 0 and not torch.isnan(norm_grad):\n",
    "                    # Set lower and upper limit in change\n",
    "                    limit_eps = self.adv_eps * param.detach().abs()\n",
    "                    param_min = param.data - limit_eps\n",
    "                    param_max = param.data + limit_eps\n",
    "\n",
    "                    # Perturb along gradient\n",
    "                    # w += (adv_lr * |w| / |grad|) * grad\n",
    "                    param.data.add_(grad, alpha=(self.adv_lr * (norm_data + e) / (norm_grad + e)))\n",
    "\n",
    "                    # Apply the limit to the change\n",
    "                    param.data.clamp_(param_min, param_max)\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.clone().detach()\n",
    "                else:\n",
    "                    self.backup[name].copy_(param.data)\n",
    "\n",
    "    def restore(self):\n",
    "        \"\"\"\n",
    "        Restore model parameter to correct position; AWP do not perturbe weights, it perturb gradients\n",
    "        Call after loss.backward(), before optimizer.step()\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data.copy_(self.backup[name])\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.awp = AWP(self.model, self.optimizer, adv_lr=0.001, adv_eps=0.001)\n",
    "    \n",
    "    def training_step(self, model: nn.Module, inputs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        # if is_sagemaker_mp_enabled():\n",
    "        #     loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n",
    "        #     return loss_mb.reduce_mean().detach().to(self.args.device)\n",
    "        \n",
    "        self.awp.perturb() # \n",
    "        \n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        if self.do_grad_scaling:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            self.accelerator.backward(loss)\n",
    "            \n",
    "        self.awp.restore()\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            if is_peft_available() and isinstance(model, PeftModel):\n",
    "                model_name = unwrap_model(model.base_model)._get_name()\n",
    "            else:\n",
    "                model_name = unwrap_model(model)._get_name()\n",
    "            if model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n",
    "                loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
    "            else:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
    "                raise ValueError(\n",
    "                    \"The model did not return a loss from the inputs, only the following keys: \"\n",
    "                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
    "                )\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c3ab5b5-1122-4b5a-bf90-b3c07e34c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(fold):\n",
    "    raw_dataset = DatasetDict({\n",
    "        'train': HFDataset.from_pandas(train_ner[train_ner.fold != fold]),\n",
    "        'validation': HFDataset.from_pandas(train_ner[train_ner.fold == fold])\n",
    "    }).remove_columns(column_names=['fold', '__index_level_0__', 'Language', 'family', 'region'])\n",
    "\n",
    "    train_dataset = raw_dataset[\"train\"]\n",
    "    column_names = train_dataset.column_names\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        process_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "    )\n",
    "    if max_seq_length is not None:\n",
    "        train_dataset = train_dataset.filter(\n",
    "            lambda example: len(example['input_ids']) <= max_seq_length\n",
    "        )\n",
    "\n",
    "    eval_dataset = raw_dataset['validation']\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        process_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "    )\n",
    "    if max_seq_length is not None:\n",
    "        eval_dataset = eval_dataset.filter(\n",
    "            lambda example: len(example['input_ids']) <= max_seq_length\n",
    "        )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'pos-tagging-ner/{model_name.replace(\"/\", \"-\")}/{fold}',\n",
    "        learning_rate=5e-5,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        group_by_length=True,\n",
    "        overwrite_output_dir=True,\n",
    "        warmup_steps=0.15,\n",
    "        num_train_epochs=5,\n",
    "        lr_scheduler_type ='cosine',\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,\n",
    "        fp16=True,\n",
    "        report_to='none'\n",
    "    )\n",
    "\n",
    "    model = load_model()\n",
    "\n",
    "    trainer = Trainer(\n",
    "    # trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    results = trainer.evaluate(eval_dataset)\n",
    "    \n",
    "    test_pos_ids = trainer.predict(test_dataset)\n",
    "    \n",
    "    return results['eval_accuracy'], test_pos_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65bf22a7-cfe7-454b-a739-20bfc29d1430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d68272b42e4151ac0d72676d91629e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/97131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef9cc67adf04ceba7571ad0ff4e9811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/97131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f36723e7ea74b7f911a843b356ddfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ee7eb469854449b0199c6b1b819e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee1a583c9dc48d1bbb2c2a44d54e0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15180' max='15180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15180/15180 56:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.561145</td>\n",
       "      <td>0.599117</td>\n",
       "      <td>0.587436</td>\n",
       "      <td>0.593219</td>\n",
       "      <td>0.645061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.574595</td>\n",
       "      <td>0.585332</td>\n",
       "      <td>0.563870</td>\n",
       "      <td>0.574400</td>\n",
       "      <td>0.631138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>0.608752</td>\n",
       "      <td>0.589598</td>\n",
       "      <td>0.569327</td>\n",
       "      <td>0.579285</td>\n",
       "      <td>0.636353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.108200</td>\n",
       "      <td>0.574484</td>\n",
       "      <td>0.597281</td>\n",
       "      <td>0.577517</td>\n",
       "      <td>0.587233</td>\n",
       "      <td>0.644747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.602212</td>\n",
       "      <td>0.597562</td>\n",
       "      <td>0.567122</td>\n",
       "      <td>0.581945</td>\n",
       "      <td>0.639258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>0.577872</td>\n",
       "      <td>0.613927</td>\n",
       "      <td>0.582131</td>\n",
       "      <td>0.597607</td>\n",
       "      <td>0.654750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.077300</td>\n",
       "      <td>0.586064</td>\n",
       "      <td>0.616932</td>\n",
       "      <td>0.599237</td>\n",
       "      <td>0.607955</td>\n",
       "      <td>0.661937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.629258</td>\n",
       "      <td>0.602403</td>\n",
       "      <td>0.583484</td>\n",
       "      <td>0.592793</td>\n",
       "      <td>0.648030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.627032</td>\n",
       "      <td>0.598152</td>\n",
       "      <td>0.578439</td>\n",
       "      <td>0.588131</td>\n",
       "      <td>0.644232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.073900</td>\n",
       "      <td>0.599571</td>\n",
       "      <td>0.604814</td>\n",
       "      <td>0.583359</td>\n",
       "      <td>0.593893</td>\n",
       "      <td>0.650694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.069900</td>\n",
       "      <td>0.590122</td>\n",
       "      <td>0.601574</td>\n",
       "      <td>0.582687</td>\n",
       "      <td>0.591980</td>\n",
       "      <td>0.643169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.595447</td>\n",
       "      <td>0.598164</td>\n",
       "      <td>0.581002</td>\n",
       "      <td>0.589458</td>\n",
       "      <td>0.642276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.653146</td>\n",
       "      <td>0.598981</td>\n",
       "      <td>0.574990</td>\n",
       "      <td>0.586740</td>\n",
       "      <td>0.643467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.639717</td>\n",
       "      <td>0.604010</td>\n",
       "      <td>0.585500</td>\n",
       "      <td>0.594611</td>\n",
       "      <td>0.649905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.642499</td>\n",
       "      <td>0.605280</td>\n",
       "      <td>0.583645</td>\n",
       "      <td>0.594266</td>\n",
       "      <td>0.651435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.650369</td>\n",
       "      <td>0.608440</td>\n",
       "      <td>0.586011</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.630847</td>\n",
       "      <td>0.606613</td>\n",
       "      <td>0.588654</td>\n",
       "      <td>0.597499</td>\n",
       "      <td>0.649954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.627175</td>\n",
       "      <td>0.608032</td>\n",
       "      <td>0.586343</td>\n",
       "      <td>0.596990</td>\n",
       "      <td>0.651475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.698958</td>\n",
       "      <td>0.604141</td>\n",
       "      <td>0.585671</td>\n",
       "      <td>0.594762</td>\n",
       "      <td>0.649801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.663832</td>\n",
       "      <td>0.602171</td>\n",
       "      <td>0.582104</td>\n",
       "      <td>0.591968</td>\n",
       "      <td>0.648883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.681627</td>\n",
       "      <td>0.607183</td>\n",
       "      <td>0.589147</td>\n",
       "      <td>0.598029</td>\n",
       "      <td>0.652545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.703580</td>\n",
       "      <td>0.603834</td>\n",
       "      <td>0.587104</td>\n",
       "      <td>0.595351</td>\n",
       "      <td>0.648690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.711565</td>\n",
       "      <td>0.601686</td>\n",
       "      <td>0.586369</td>\n",
       "      <td>0.593929</td>\n",
       "      <td>0.647749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.709196</td>\n",
       "      <td>0.601877</td>\n",
       "      <td>0.584559</td>\n",
       "      <td>0.593092</td>\n",
       "      <td>0.647290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.736274</td>\n",
       "      <td>0.604433</td>\n",
       "      <td>0.586208</td>\n",
       "      <td>0.595181</td>\n",
       "      <td>0.649954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.745119</td>\n",
       "      <td>0.603072</td>\n",
       "      <td>0.586817</td>\n",
       "      <td>0.594834</td>\n",
       "      <td>0.649012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.022500</td>\n",
       "      <td>0.743389</td>\n",
       "      <td>0.605251</td>\n",
       "      <td>0.588699</td>\n",
       "      <td>0.596860</td>\n",
       "      <td>0.651322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.744061</td>\n",
       "      <td>0.604504</td>\n",
       "      <td>0.587642</td>\n",
       "      <td>0.595953</td>\n",
       "      <td>0.650332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.743592</td>\n",
       "      <td>0.604584</td>\n",
       "      <td>0.587606</td>\n",
       "      <td>0.595974</td>\n",
       "      <td>0.650445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.743871</td>\n",
       "      <td>0.604444</td>\n",
       "      <td>0.587454</td>\n",
       "      <td>0.595828</td>\n",
       "      <td>0.650251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6619371453865036\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1659c8958c7e49fd911d142faa809c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/97498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c926d15a16049fabdff3291b4d0d3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/97498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d888fa25844d0b8f7a1bfe9828548c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161e1603a7f54df5b2966ebfeff36ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15235' max='15235' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15235/15235 57:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.520378</td>\n",
       "      <td>0.637461</td>\n",
       "      <td>0.630807</td>\n",
       "      <td>0.634117</td>\n",
       "      <td>0.699024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.139700</td>\n",
       "      <td>0.534924</td>\n",
       "      <td>0.652873</td>\n",
       "      <td>0.644933</td>\n",
       "      <td>0.648878</td>\n",
       "      <td>0.712790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.519453</td>\n",
       "      <td>0.662997</td>\n",
       "      <td>0.651726</td>\n",
       "      <td>0.657313</td>\n",
       "      <td>0.721158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.541514</td>\n",
       "      <td>0.668990</td>\n",
       "      <td>0.660426</td>\n",
       "      <td>0.664680</td>\n",
       "      <td>0.725787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.105000</td>\n",
       "      <td>0.482127</td>\n",
       "      <td>0.663652</td>\n",
       "      <td>0.650184</td>\n",
       "      <td>0.656849</td>\n",
       "      <td>0.722288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.522780</td>\n",
       "      <td>0.659039</td>\n",
       "      <td>0.639543</td>\n",
       "      <td>0.649145</td>\n",
       "      <td>0.719376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.078400</td>\n",
       "      <td>0.528970</td>\n",
       "      <td>0.671342</td>\n",
       "      <td>0.658767</td>\n",
       "      <td>0.664995</td>\n",
       "      <td>0.728848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>0.522135</td>\n",
       "      <td>0.662751</td>\n",
       "      <td>0.650454</td>\n",
       "      <td>0.656545</td>\n",
       "      <td>0.722430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.075800</td>\n",
       "      <td>0.546045</td>\n",
       "      <td>0.664738</td>\n",
       "      <td>0.649450</td>\n",
       "      <td>0.657005</td>\n",
       "      <td>0.722488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.532743</td>\n",
       "      <td>0.663522</td>\n",
       "      <td>0.650708</td>\n",
       "      <td>0.657053</td>\n",
       "      <td>0.722798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.546045</td>\n",
       "      <td>0.669652</td>\n",
       "      <td>0.658142</td>\n",
       "      <td>0.663847</td>\n",
       "      <td>0.728467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.548538</td>\n",
       "      <td>0.664535</td>\n",
       "      <td>0.652403</td>\n",
       "      <td>0.658413</td>\n",
       "      <td>0.724005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.075300</td>\n",
       "      <td>0.625664</td>\n",
       "      <td>0.635873</td>\n",
       "      <td>0.624966</td>\n",
       "      <td>0.630372</td>\n",
       "      <td>0.698120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.604975</td>\n",
       "      <td>0.648842</td>\n",
       "      <td>0.634910</td>\n",
       "      <td>0.641800</td>\n",
       "      <td>0.709322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.594144</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.649602</td>\n",
       "      <td>0.653806</td>\n",
       "      <td>0.716877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.573036</td>\n",
       "      <td>0.663303</td>\n",
       "      <td>0.655560</td>\n",
       "      <td>0.659409</td>\n",
       "      <td>0.720570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.571184</td>\n",
       "      <td>0.669835</td>\n",
       "      <td>0.660797</td>\n",
       "      <td>0.665285</td>\n",
       "      <td>0.727427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.565341</td>\n",
       "      <td>0.668254</td>\n",
       "      <td>0.656156</td>\n",
       "      <td>0.662150</td>\n",
       "      <td>0.725141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.609257</td>\n",
       "      <td>0.666116</td>\n",
       "      <td>0.654556</td>\n",
       "      <td>0.660286</td>\n",
       "      <td>0.723495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.037300</td>\n",
       "      <td>0.610560</td>\n",
       "      <td>0.670156</td>\n",
       "      <td>0.658062</td>\n",
       "      <td>0.664054</td>\n",
       "      <td>0.727233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>0.630577</td>\n",
       "      <td>0.669596</td>\n",
       "      <td>0.659037</td>\n",
       "      <td>0.664274</td>\n",
       "      <td>0.726562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.637117</td>\n",
       "      <td>0.666546</td>\n",
       "      <td>0.657494</td>\n",
       "      <td>0.661989</td>\n",
       "      <td>0.724567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.626864</td>\n",
       "      <td>0.666790</td>\n",
       "      <td>0.655465</td>\n",
       "      <td>0.661079</td>\n",
       "      <td>0.724612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.627658</td>\n",
       "      <td>0.671319</td>\n",
       "      <td>0.657691</td>\n",
       "      <td>0.664435</td>\n",
       "      <td>0.729009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.665395</td>\n",
       "      <td>0.671425</td>\n",
       "      <td>0.659240</td>\n",
       "      <td>0.665277</td>\n",
       "      <td>0.728434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.664585</td>\n",
       "      <td>0.667340</td>\n",
       "      <td>0.656098</td>\n",
       "      <td>0.661671</td>\n",
       "      <td>0.724580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.671601</td>\n",
       "      <td>0.668333</td>\n",
       "      <td>0.656273</td>\n",
       "      <td>0.662248</td>\n",
       "      <td>0.725632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.674081</td>\n",
       "      <td>0.668749</td>\n",
       "      <td>0.657182</td>\n",
       "      <td>0.662915</td>\n",
       "      <td>0.726071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.674113</td>\n",
       "      <td>0.668497</td>\n",
       "      <td>0.656905</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.725929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.673114</td>\n",
       "      <td>0.668744</td>\n",
       "      <td>0.657313</td>\n",
       "      <td>0.662979</td>\n",
       "      <td>0.726084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7290090136625429\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f8312cc01740b1b2b7098b5f4eac7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/100961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423691e6be264a0bbe6b25c67ffa77b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede65c79ff59487ea132f7bd78aeb8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbf3e5c4aaa4f64a6492b3e8c845fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15780' max='15780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15780/15780 55:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.318800</td>\n",
       "      <td>0.473396</td>\n",
       "      <td>0.653578</td>\n",
       "      <td>0.651090</td>\n",
       "      <td>0.652331</td>\n",
       "      <td>0.715085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.472707</td>\n",
       "      <td>0.644326</td>\n",
       "      <td>0.633613</td>\n",
       "      <td>0.638924</td>\n",
       "      <td>0.704822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.127800</td>\n",
       "      <td>0.566394</td>\n",
       "      <td>0.643515</td>\n",
       "      <td>0.641571</td>\n",
       "      <td>0.642541</td>\n",
       "      <td>0.703361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.587935</td>\n",
       "      <td>0.618942</td>\n",
       "      <td>0.620985</td>\n",
       "      <td>0.619962</td>\n",
       "      <td>0.683734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.515652</td>\n",
       "      <td>0.621464</td>\n",
       "      <td>0.619627</td>\n",
       "      <td>0.620544</td>\n",
       "      <td>0.685803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.471395</td>\n",
       "      <td>0.650927</td>\n",
       "      <td>0.647523</td>\n",
       "      <td>0.649221</td>\n",
       "      <td>0.712107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.540723</td>\n",
       "      <td>0.639901</td>\n",
       "      <td>0.644084</td>\n",
       "      <td>0.641986</td>\n",
       "      <td>0.703339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.555198</td>\n",
       "      <td>0.633048</td>\n",
       "      <td>0.640022</td>\n",
       "      <td>0.636516</td>\n",
       "      <td>0.697370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.547858</td>\n",
       "      <td>0.622699</td>\n",
       "      <td>0.624145</td>\n",
       "      <td>0.623421</td>\n",
       "      <td>0.688366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.077300</td>\n",
       "      <td>0.562222</td>\n",
       "      <td>0.614350</td>\n",
       "      <td>0.622241</td>\n",
       "      <td>0.618271</td>\n",
       "      <td>0.680564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.565002</td>\n",
       "      <td>0.627164</td>\n",
       "      <td>0.635872</td>\n",
       "      <td>0.631488</td>\n",
       "      <td>0.692513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.505891</td>\n",
       "      <td>0.653269</td>\n",
       "      <td>0.657854</td>\n",
       "      <td>0.655554</td>\n",
       "      <td>0.714973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.532609</td>\n",
       "      <td>0.655018</td>\n",
       "      <td>0.660862</td>\n",
       "      <td>0.657927</td>\n",
       "      <td>0.716839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.528005</td>\n",
       "      <td>0.643116</td>\n",
       "      <td>0.647549</td>\n",
       "      <td>0.645325</td>\n",
       "      <td>0.705463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.533929</td>\n",
       "      <td>0.652785</td>\n",
       "      <td>0.658882</td>\n",
       "      <td>0.655819</td>\n",
       "      <td>0.714759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.565642</td>\n",
       "      <td>0.634767</td>\n",
       "      <td>0.639464</td>\n",
       "      <td>0.637107</td>\n",
       "      <td>0.699775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.567563</td>\n",
       "      <td>0.640948</td>\n",
       "      <td>0.645366</td>\n",
       "      <td>0.643149</td>\n",
       "      <td>0.704148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.534735</td>\n",
       "      <td>0.642463</td>\n",
       "      <td>0.647282</td>\n",
       "      <td>0.644864</td>\n",
       "      <td>0.704406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.562119</td>\n",
       "      <td>0.657238</td>\n",
       "      <td>0.662626</td>\n",
       "      <td>0.659921</td>\n",
       "      <td>0.718570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.594831</td>\n",
       "      <td>0.647209</td>\n",
       "      <td>0.651800</td>\n",
       "      <td>0.649496</td>\n",
       "      <td>0.709521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.593252</td>\n",
       "      <td>0.639274</td>\n",
       "      <td>0.644515</td>\n",
       "      <td>0.641884</td>\n",
       "      <td>0.702675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.620467</td>\n",
       "      <td>0.639426</td>\n",
       "      <td>0.646381</td>\n",
       "      <td>0.642885</td>\n",
       "      <td>0.703226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.590923</td>\n",
       "      <td>0.647228</td>\n",
       "      <td>0.654580</td>\n",
       "      <td>0.650883</td>\n",
       "      <td>0.711387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.606249</td>\n",
       "      <td>0.645909</td>\n",
       "      <td>0.652016</td>\n",
       "      <td>0.648948</td>\n",
       "      <td>0.709982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.033300</td>\n",
       "      <td>0.597513</td>\n",
       "      <td>0.648403</td>\n",
       "      <td>0.655316</td>\n",
       "      <td>0.651841</td>\n",
       "      <td>0.711578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.623823</td>\n",
       "      <td>0.647616</td>\n",
       "      <td>0.655811</td>\n",
       "      <td>0.651688</td>\n",
       "      <td>0.710229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.632725</td>\n",
       "      <td>0.647103</td>\n",
       "      <td>0.655316</td>\n",
       "      <td>0.651184</td>\n",
       "      <td>0.710117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.637495</td>\n",
       "      <td>0.647444</td>\n",
       "      <td>0.655900</td>\n",
       "      <td>0.651645</td>\n",
       "      <td>0.710263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.636214</td>\n",
       "      <td>0.645297</td>\n",
       "      <td>0.652536</td>\n",
       "      <td>0.648896</td>\n",
       "      <td>0.708330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.637027</td>\n",
       "      <td>0.646209</td>\n",
       "      <td>0.653755</td>\n",
       "      <td>0.649960</td>\n",
       "      <td>0.709195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.637018</td>\n",
       "      <td>0.645966</td>\n",
       "      <td>0.653615</td>\n",
       "      <td>0.649768</td>\n",
       "      <td>0.709105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.718570143884892\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a579b03ea9f451c8c45ef2857d23460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/96524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e4b4538a15411aacc1fa3163eba50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/96524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feee37c5a79f4cf0bba1b1cf8c64f644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f84c9d6abfa49e5bdba1dc0c2a5352b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15085' max='15085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15085/15085 58:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.305200</td>\n",
       "      <td>0.520846</td>\n",
       "      <td>0.670027</td>\n",
       "      <td>0.683085</td>\n",
       "      <td>0.676493</td>\n",
       "      <td>0.733511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.141500</td>\n",
       "      <td>0.507354</td>\n",
       "      <td>0.678628</td>\n",
       "      <td>0.692952</td>\n",
       "      <td>0.685715</td>\n",
       "      <td>0.739084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>0.547353</td>\n",
       "      <td>0.648838</td>\n",
       "      <td>0.666365</td>\n",
       "      <td>0.657485</td>\n",
       "      <td>0.715605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.105700</td>\n",
       "      <td>0.488527</td>\n",
       "      <td>0.667181</td>\n",
       "      <td>0.678687</td>\n",
       "      <td>0.672884</td>\n",
       "      <td>0.728285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.102300</td>\n",
       "      <td>0.549690</td>\n",
       "      <td>0.648256</td>\n",
       "      <td>0.658998</td>\n",
       "      <td>0.653583</td>\n",
       "      <td>0.712038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.543642</td>\n",
       "      <td>0.675010</td>\n",
       "      <td>0.688997</td>\n",
       "      <td>0.681932</td>\n",
       "      <td>0.737453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.550831</td>\n",
       "      <td>0.668745</td>\n",
       "      <td>0.679730</td>\n",
       "      <td>0.674193</td>\n",
       "      <td>0.729514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.564202</td>\n",
       "      <td>0.669361</td>\n",
       "      <td>0.683448</td>\n",
       "      <td>0.676331</td>\n",
       "      <td>0.729884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.077200</td>\n",
       "      <td>0.521886</td>\n",
       "      <td>0.664615</td>\n",
       "      <td>0.677256</td>\n",
       "      <td>0.670876</td>\n",
       "      <td>0.728144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.552200</td>\n",
       "      <td>0.669232</td>\n",
       "      <td>0.684010</td>\n",
       "      <td>0.676541</td>\n",
       "      <td>0.731395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.071500</td>\n",
       "      <td>0.515776</td>\n",
       "      <td>0.672891</td>\n",
       "      <td>0.686341</td>\n",
       "      <td>0.679549</td>\n",
       "      <td>0.734783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.552455</td>\n",
       "      <td>0.674012</td>\n",
       "      <td>0.687034</td>\n",
       "      <td>0.680461</td>\n",
       "      <td>0.736197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.622693</td>\n",
       "      <td>0.666334</td>\n",
       "      <td>0.680024</td>\n",
       "      <td>0.673109</td>\n",
       "      <td>0.729188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.549865</td>\n",
       "      <td>0.671240</td>\n",
       "      <td>0.680836</td>\n",
       "      <td>0.676004</td>\n",
       "      <td>0.732575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.591004</td>\n",
       "      <td>0.676702</td>\n",
       "      <td>0.684698</td>\n",
       "      <td>0.680676</td>\n",
       "      <td>0.738084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>0.651781</td>\n",
       "      <td>0.639145</td>\n",
       "      <td>0.637716</td>\n",
       "      <td>0.638430</td>\n",
       "      <td>0.709825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.586821</td>\n",
       "      <td>0.669497</td>\n",
       "      <td>0.679918</td>\n",
       "      <td>0.674667</td>\n",
       "      <td>0.731352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.575626</td>\n",
       "      <td>0.677959</td>\n",
       "      <td>0.688109</td>\n",
       "      <td>0.682997</td>\n",
       "      <td>0.738584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.650886</td>\n",
       "      <td>0.674206</td>\n",
       "      <td>0.685029</td>\n",
       "      <td>0.679574</td>\n",
       "      <td>0.735664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.643019</td>\n",
       "      <td>0.675680</td>\n",
       "      <td>0.686691</td>\n",
       "      <td>0.681141</td>\n",
       "      <td>0.736431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.649766</td>\n",
       "      <td>0.676392</td>\n",
       "      <td>0.687816</td>\n",
       "      <td>0.682056</td>\n",
       "      <td>0.737480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.662724</td>\n",
       "      <td>0.671631</td>\n",
       "      <td>0.683054</td>\n",
       "      <td>0.677294</td>\n",
       "      <td>0.732999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.654843</td>\n",
       "      <td>0.675572</td>\n",
       "      <td>0.686653</td>\n",
       "      <td>0.681068</td>\n",
       "      <td>0.736626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.654038</td>\n",
       "      <td>0.676039</td>\n",
       "      <td>0.687359</td>\n",
       "      <td>0.681652</td>\n",
       "      <td>0.737007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.691136</td>\n",
       "      <td>0.675820</td>\n",
       "      <td>0.686503</td>\n",
       "      <td>0.681120</td>\n",
       "      <td>0.736806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.697004</td>\n",
       "      <td>0.675880</td>\n",
       "      <td>0.687747</td>\n",
       "      <td>0.681762</td>\n",
       "      <td>0.737176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.695474</td>\n",
       "      <td>0.676603</td>\n",
       "      <td>0.687316</td>\n",
       "      <td>0.681917</td>\n",
       "      <td>0.737502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>0.699716</td>\n",
       "      <td>0.675634</td>\n",
       "      <td>0.687641</td>\n",
       "      <td>0.681585</td>\n",
       "      <td>0.736980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>0.699311</td>\n",
       "      <td>0.675826</td>\n",
       "      <td>0.687228</td>\n",
       "      <td>0.681480</td>\n",
       "      <td>0.736958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.699296</td>\n",
       "      <td>0.675847</td>\n",
       "      <td>0.687278</td>\n",
       "      <td>0.681514</td>\n",
       "      <td>0.736996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7390840973105826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "all_preds = []\n",
    "\n",
    "for fold in [1, 2, 3, 4]:\n",
    "    score, fold_pred = train_fold(fold)\n",
    "    \n",
    "    scores.append(score)\n",
    "    all_preds.append(fold_pred)\n",
    "    \n",
    "    print(score)\n",
    "    print()\n",
    "    \n",
    "avg_score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b509e41f-8169-484e-af92-3853030b1f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6619371453865036, 0.7290090136625429, 0.718570143884892, 0.7390840973105826]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f41ddd1d-b6b7-40fb-b1f2-7f5b58e361ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7121501000611303"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5423319-fc1d-4e9c-ab53-0eb0122803dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 190, 18)\n"
     ]
    }
   ],
   "source": [
    "test_pos_ids = np.mean([p.predictions for p in all_preds], axis=0)\n",
    "print(test_pos_ids.shape)\n",
    "test_pos_ids = test_pos_ids.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45ce2daa-b304-4065-8ab6-40fe16f91f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pos = []\n",
    "for pos_ids, sentence in zip(test_pos_ids, sentences):\n",
    "    length = len(sentence.split(' '))\n",
    "    final_pos.extend(\n",
    "        list(map(id_to_label.get, [x for x in pos_ids if x not in [-100, 17]]))[1:length+1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab63a58c-f2ec-43d8-8f79-fbc90939c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Pos'] = final_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55dd1deb-7091-4db3-8cf9-8cd0147a6346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOUN     6735\n",
       "VERB     4960\n",
       "ADP      4333\n",
       "PROPN    3003\n",
       "PUNCT    2980\n",
       "AUX      2638\n",
       "PRON     1674\n",
       "SCONJ    1241\n",
       "DET       948\n",
       "ADV       851\n",
       "CCONJ     732\n",
       "PART      730\n",
       "ADJ       601\n",
       "NUM       551\n",
       "INTJ       27\n",
       "X          27\n",
       "SYM        14\n",
       "Name: Pos, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Pos.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd03bca7-647e-41ce-84c7-6ffbf1096b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['Id', 'Pos']].to_csv(f'submissions/ps-round1-pos-ner-{model_name.split(\"/\")[-1]}-{avg_score:.3f}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff57974-cd0b-47c0-9356-e08ceb7af5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c85bbd-29a8-40f8-933b-5e45111e84be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test[test.Pos == 'SYM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b6f72-85c4-4881-8319-c59d5dd08de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
