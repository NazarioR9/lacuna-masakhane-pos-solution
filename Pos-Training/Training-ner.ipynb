{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c866a9c-6192-4cf1-b8e7-1b96c98953f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1961ba0b-d202-4477-8004-63b1c5e5ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 14 20:37:43 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   51C    P8    20W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a20d7-05ea-4951-a079-08c8b8a71940",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16825bfc-8853-445c-9e6c-53395bea41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "import subprocess\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedGroupKFold, GroupKFold\n",
    "from sklearn.metrics import f1_score, mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6e65d9-657c-4b4d-a20c-ccd0dc3f5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b9b22b-0a41-44b0-beec-e050afbeaa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR\n",
    "from torch import LongTensor\n",
    "from torch import nn, optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4c3033-b975-4aac-bc6b-24346ea9c6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "# from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm, trange\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict, Dataset as HFDataset\n",
    "from datasets import concatenate_datasets, interleave_datasets\n",
    "from datasets import ClassLabel, load_metric\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a120b4-51ef-49dc-9748-873fbd66e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext watermark\n",
    "# %watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563effdb-c378-45ba-8f7e-8d7cc8bf8b08",
   "metadata": {},
   "source": [
    "# Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db742f8-2be3-47d5-bd7b-82629ce77866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_warnings(strict=False):\n",
    "\twarnings.simplefilter('ignore')\n",
    "\tif strict:\n",
    "\t\tlogging.disable(logging.WARNING)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "\trandom.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c01b0cb9-83b7-46c0-8383-c9384772e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "disable_warnings()\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7fe6b-98d6-4d75-9912-bf1bf44b6643",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe357fad-adaf-4b32-8c8a-3bfba4d57170",
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = '../masakhane-pos/data/'\n",
    "add_path = '../masakhane-pos/transfer_corpus/'\n",
    "\n",
    "langs = sorted(os.listdir(path))\n",
    "add_langs = os.listdir(add_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5bd3e9-2a48-4227-af3e-24fde850a994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang2family = {\n",
    " 'bam': 'Mande',\n",
    " 'bbj': 'Grassfields',\n",
    " 'ewe': 'Kwa',\n",
    " 'fon': 'Volta-Niger',\n",
    " 'hau': 'Chadic',\n",
    " 'ibo': 'Volta-Niger',\n",
    " 'kin': 'Bantu',\n",
    " 'lug': 'Bantu',\n",
    " 'mos': 'Gur',\n",
    " 'nya': 'Bantu',\n",
    " 'pcm': 'English-Creole',\n",
    " 'sna': 'Bantu',\n",
    " 'swa': 'Bantu',\n",
    " 'twi': 'Kwa',\n",
    " 'wol': 'Senegambia',\n",
    " 'xho': 'Bantu',\n",
    " 'yor': 'Volta-Niger',\n",
    " 'zul': 'Bantu'\n",
    "}\n",
    "\n",
    "lang2region = {\n",
    "    'bam': 'West',\n",
    "    'bbj': 'Central',\n",
    "    'ewe': 'West',\n",
    "    'fon': 'West',\n",
    "    'hau': 'West',\n",
    "    'ibo': 'West',\n",
    "    'kin': 'East',\n",
    "    'lug': 'East',\n",
    "    'mos': 'West',\n",
    "    'nya': 'East',\n",
    "    'pcm': 'West',\n",
    "    'sna': 'South',\n",
    "    'swa': 'East',\n",
    "    'twi': 'South',\n",
    "    'wol': 'West',\n",
    "    'xho': 'South',\n",
    "    'yor': 'West',\n",
    "    'zul': 'South'\n",
    "}\n",
    "\n",
    "mapper = {'NOUN': 0,\n",
    " 'ADJ': 1,\n",
    " 'PUNCT': 2,\n",
    " 'CCONJ': 3,\n",
    " 'PRON': 4,\n",
    " 'ADV': 5,\n",
    " 'AUX': 6,\n",
    " 'VERB': 7,\n",
    " 'ADP': 8,\n",
    " 'PART': 9,\n",
    " 'SCONJ': 10,\n",
    " 'PROPN': 11,\n",
    " 'X': 12,\n",
    " 'NUM': 13,\n",
    " 'INTJ': 14,\n",
    " 'SYM': 15,\n",
    " 'DET': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d2ead2-2302-480b-82b6-37346e9e07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2fix = {\n",
    "    'yo': 'yor',\n",
    "    'bm': 'bam'\n",
    "}\n",
    "\n",
    "def load_lang_data(data_path, lang, split='train'):\n",
    "    data = []\n",
    "    language = lang.split('_')[0]\n",
    "    language = lang2fix.get(language, language)\n",
    "    \n",
    "    try:\n",
    "        with open(f'{data_path}{lang}/{split}.txt', 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                splits = line.split(' ')\n",
    "                if len(splits) != 2:\n",
    "                    continue\n",
    "                data.append(splits)\n",
    "        data = np.array(data).squeeze()\n",
    "        data = pd.DataFrame({\n",
    "            'Word': data[:, 0],\n",
    "            'Pos': data[:, 1],\n",
    "            'Language': language.split('_')[0]\n",
    "        })\n",
    "        data['family'] = lang2family.get(language, language)\n",
    "        data['region'] = lang2region.get(language, language)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d23f39a4-6053-418d-b865-aaf8f4af5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lang_data_ner(data_path, lang, split='train'):\n",
    "    data = []\n",
    "    language = lang.split('_')[0]\n",
    "    language = lang2fix.get(language, language)\n",
    "    \n",
    "    try:\n",
    "        with open(f'{data_path}{lang}/{split}.txt', 'r') as f:\n",
    "            sentences, labels = [], []\n",
    "            \n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                splits = line.split(' ')\n",
    "                \n",
    "                if len(splits) == 2:\n",
    "                    sentences.append(splits[0])\n",
    "                    labels.append(splits[1])\n",
    "                elif len(splits) == 1:\n",
    "                    data.append(\n",
    "                        [\n",
    "                            ' '.join(sentences),\n",
    "                            ' '.join(labels),\n",
    "                        ]\n",
    "                    )\n",
    "                    \n",
    "                    sentences, labels = [], []\n",
    "\n",
    "        data = np.array(data).squeeze()\n",
    "        data = pd.DataFrame({\n",
    "            'Word': data[:, 0],\n",
    "            'Pos': data[:, 1],\n",
    "            'Language': language.split('_')[0]\n",
    "        })\n",
    "        data['family'] = lang2family.get(language, language)\n",
    "        data['region'] = lang2region.get(language, language)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "378e3114-de98-4f91-a64f-e7ebf677b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):  \n",
    "    # pos = df.Pos.unique()\n",
    "    use_pos = ['PUNCT', 'SYM']\n",
    "    \n",
    "    df_others = df[~df.Pos.isin(use_pos)]\n",
    "    df_toclean = df[df.Pos.isin(use_pos)]\n",
    "    \n",
    "    dfs = [df_others]\n",
    "    \n",
    "    for p in use_pos:\n",
    "        df_p = df_toclean[df_toclean.Pos == p]\n",
    "        words = df_p.Word.value_counts()[df_p.Word.value_counts() < 2].index.values\n",
    "        df_p = df_p[~df_p.Word.isin(words)]\n",
    "        \n",
    "        dfs.append(df_p)\n",
    "        \n",
    "    return pd.concat(dfs).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f54e1e-1c04-40a8-b297-d7bf05d901d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_data():\n",
    "    train = pd.concat([load_lang_data_ner(path, lang, split) for lang in langs for split in ['train', 'dev', 'test']], axis=0).dropna().reset_index(drop=True)\n",
    "    add_data = (\n",
    "        pd.concat([load_lang_data_ner(add_path, lang, split) for lang in add_langs for split in ['train', 'dev', 'test']], axis=0)\n",
    "        .dropna()\n",
    "        .drop_duplicates(subset=['Word', 'Pos'], keep='first')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    train = pd.concat([train, add_data])\n",
    "    train = train.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return train\n",
    "\n",
    "def load_pos_data():\n",
    "    train = pd.concat([load_lang_data(path, lang, split) for lang in langs for split in ['train', 'dev', 'test']], axis=0).dropna().reset_index(drop=True)\n",
    "    add_data = (\n",
    "        pd.concat([load_lang_data(add_path, lang, split) for lang in add_langs for split in ['train', 'dev', 'test']], axis=0)\n",
    "        .dropna()\n",
    "        .drop_duplicates(subset=['Word', 'Pos'], keep='first')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    train = pd.concat([train, add_data])\n",
    "    train = train[train.Pos.isin(list(mapper.keys()))]\n",
    "\n",
    "    train = train.drop_duplicates().drop_duplicates(subset=['Word', 'Language'], keep='first').reset_index(drop=True)\n",
    "    # train = train.drop_duplicates().reset_index(drop=True)\n",
    "    # train = clean_data(train)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd40436e-a4f4-4823-99b3-8ff6c05636de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muso</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ŋana</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afiriki</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tilebinyanfan</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word    Pos Language family region\n",
       "0           Muso   NOUN      bam  Mande   West\n",
       "1           ŋana    ADJ      bam  Mande   West\n",
       "2              ,  PUNCT      bam  Mande   West\n",
       "3        Afiriki   NOUN      bam  Mande   West\n",
       "4  tilebinyanfan   NOUN      bam  Mande   West"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner = load_ner_data()\n",
    "train = load_pos_data()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fc60dcc-029f-4508-85ac-070e6a79b23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muso ŋana , Afiriki tilebinyanfan n' a cɛmancɛ...</td>\n",
       "      <td>NOUN ADJ PUNCT NOUN NOUN CCONJ PRON NOUN ADV C...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ni mɔgɔ ka dɔgɔn kojugu , i bɛ mɔgɔw juguya mi...</td>\n",
       "      <td>CCONJ NOUN PART ADJ NOUN PUNCT PRON AUX NOUN N...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A kɔrɔtalen a ka taɲɛ fɛ kow ɲɛmɔgɔyabaaraw la...</td>\n",
       "      <td>PRON VERB PRON PART NOUN PART NOUN NOUN PART C...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ale y' a ( basikɛti ) to a ka se ka bɔ a ka so...</td>\n",
       "      <td>PRON PART PRON PUNCT NOUN PUNCT VERB PRON PART...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sannayɛlɛn galabukɛnɛya a sera ka min fiyɛ Afi...</td>\n",
       "      <td>NOUN NOUN PRON VERB PART PRON VERB PROPN NOUN ...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Muso ŋana , Afiriki tilebinyanfan n' a cɛmancɛ...   \n",
       "1  Ni mɔgɔ ka dɔgɔn kojugu , i bɛ mɔgɔw juguya mi...   \n",
       "2  A kɔrɔtalen a ka taɲɛ fɛ kow ɲɛmɔgɔyabaaraw la...   \n",
       "3  Ale y' a ( basikɛti ) to a ka se ka bɔ a ka so...   \n",
       "4  Sannayɛlɛn galabukɛnɛya a sera ka min fiyɛ Afi...   \n",
       "\n",
       "                                                 Pos Language family region  \n",
       "0  NOUN ADJ PUNCT NOUN NOUN CCONJ PRON NOUN ADV C...      bam  Mande   West  \n",
       "1  CCONJ NOUN PART ADJ NOUN PUNCT PRON AUX NOUN N...      bam  Mande   West  \n",
       "2  PRON VERB PRON PART NOUN PART NOUN NOUN PART C...      bam  Mande   West  \n",
       "3  PRON PART PRON PUNCT NOUN PUNCT VERB PRON PART...      bam  Mande   West  \n",
       "4  NOUN NOUN PRON VERB PART PRON VERB PROPN NOUN ...      bam  Mande   West  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1e73767-4a99-4f9f-b4dc-7c7d6eeb8ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "      <th>Language</th>\n",
       "      <th>Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Id00qog2f11n_0</td>\n",
       "      <td>Ne</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Id00qog2f11n_1</td>\n",
       "      <td>otim</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Id00qog2f11n_2</td>\n",
       "      <td>penj</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Id00qog2f11n_3</td>\n",
       "      <td>e</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Id00qog2f11n_4</td>\n",
       "      <td>kind</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  Word Language  Pos\n",
       "0  Id00qog2f11n_0    Ne      luo  NaN\n",
       "1  Id00qog2f11n_1  otim      luo  NaN\n",
       "2  Id00qog2f11n_2  penj      luo  NaN\n",
       "3  Id00qog2f11n_3     e      luo  NaN\n",
       "4  Id00qog2f11n_4  kind      luo  NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../Test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07d27197-1217-4279-bffa-8052260878dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((255140, 5), (91859, 5), (32045, 4))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, train_ner.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2114cb1d-b750-4c81-b4e9-cdf1b4d7bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = train_ner[train_ner.Language.isin(langs)].reset_index(drop=True)\n",
    "add_train = train_ner[~train_ner.Language.isin(langs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "989646fd-faa8-4d30-92a5-db49b4e2518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['fold'] = -1\n",
    "train_ner['fold'] = -1\n",
    "\n",
    "\n",
    "# val_langs = ['fon', 'pcm', 'twi', 'xho']\n",
    "# train.loc[train.Language.isin(val_langs), 'fold'] = 0\n",
    "\n",
    "base_train['fold'] = base_train['Language'].map(dict(zip(langs, range(len(langs)))))\n",
    "base_train.loc[base_train['fold'].isna(), 'fold'] = -1\n",
    "base_train['fold'] = base_train['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12da99ae-93ad-4f4a-9f4d-87e539c55ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    10338\n",
       "1     6908\n",
       "2     6541\n",
       "3     6133\n",
       "4     7515\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_fold = 5\n",
    "# Fold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "# Fold = StratifiedGroupKFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "Fold = GroupKFold(n_splits=n_fold)\n",
    "\n",
    "base_train['fold'] = -1\n",
    "# for n, (train_index, val_index) in enumerate(Fold.split(train, train['region'], groups=train['Language'])):\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(base_train, base_train['Pos'], groups=base_train['Language'])):\n",
    "    base_train.loc[val_index, 'fold'] = int(n)\n",
    "    \n",
    "base_train['fold'] = base_train['fold'].astype(int)\n",
    "\n",
    "display(base_train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b0cabda-60df-4c10-bb88-93fcd7cddafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bam', 'bbj', 'ewe', 'fon', 'hau', 'ibo', 'kin', 'lug', 'mos',\n",
       "       'nya', 'pcm', 'sna', 'swa', 'twi', 'wol', 'xho', 'yor', 'zul',\n",
       "       'en', 'fr', 'eng-ron-wol-sna', 'ar', 'af'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.Language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6f2731d-47d6-4b01-814b-bd5a8d97bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_train['fold'] = -1\n",
    "base_train.loc[base_train.Language.isin(['sna', 'wol']), 'fold'] = -1\n",
    "# base_train.loc[base_train.Language.isin(['sna', 'bam', 'pcm', 'yor', 'wol']), 'fold'] = -1\n",
    "\n",
    "train_ner = pd.concat([base_train, add_train]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef6675b1-ada8-436b-829d-7d24d62d9382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uthi abalindelanga zingxaki eBuffalo City Muni...</td>\n",
       "      <td>VERB VERB NOUN ADP PROPN PROPN CCONJ VERB NOUN...</td>\n",
       "      <td>xho</td>\n",
       "      <td>Bantu</td>\n",
       "      <td>South</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ons onderwys- en vaardigheidsprogramme sal ons...</td>\n",
       "      <td>PRON X CCONJ NOUN AUX PRON NOUN CCONJ NOUN VER...</td>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Òntajà ojú títì kan ló ṣekúpa Ayẹni lọ́...</td>\n",
       "      <td>NOUN NOUN NOUN NUM PRON VERB PROPN NOUN NUM PU...</td>\n",
       "      <td>yor</td>\n",
       "      <td>Volta-Niger</td>\n",
       "      <td>West</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tili mnjira , yemwe amaoneka kuti ndi mkulu wa...</td>\n",
       "      <td>VERB NOUN PUNCT PRON VERB ADP VERB NOUN ADP NO...</td>\n",
       "      <td>nya</td>\n",
       "      <td>Bantu</td>\n",
       "      <td>East</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Àṣejèrè : Àwọn wo làwòkọ́ṣe yín nínú ...</td>\n",
       "      <td>PROPN PUNCT PRON ADJ NOUN DET ADP NOUN NOUN NO...</td>\n",
       "      <td>yor</td>\n",
       "      <td>Volta-Niger</td>\n",
       "      <td>West</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Uthi abalindelanga zingxaki eBuffalo City Muni...   \n",
       "1  Ons onderwys- en vaardigheidsprogramme sal ons...   \n",
       "2  Òntajà ojú títì kan ló ṣekúpa Ayẹni lọ́...   \n",
       "3  Tili mnjira , yemwe amaoneka kuti ndi mkulu wa...   \n",
       "4  Àṣejèrè : Àwọn wo làwòkọ́ṣe yín nínú ...   \n",
       "\n",
       "                                                 Pos Language       family  \\\n",
       "0  VERB VERB NOUN ADP PROPN PROPN CCONJ VERB NOUN...      xho        Bantu   \n",
       "1  PRON X CCONJ NOUN AUX PRON NOUN CCONJ NOUN VER...       af           af   \n",
       "2  NOUN NOUN NOUN NUM PRON VERB PROPN NOUN NUM PU...      yor  Volta-Niger   \n",
       "3  VERB NOUN PUNCT PRON VERB ADP VERB NOUN ADP NO...      nya        Bantu   \n",
       "4  PROPN PUNCT PRON ADJ NOUN DET ADP NOUN NOUN NO...      yor  Volta-Niger   \n",
       "\n",
       "  region  fold  \n",
       "0  South     4  \n",
       "1     af    -1  \n",
       "2   West     2  \n",
       "3   East     1  \n",
       "4   West     2  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7a768de-2e21-4900-b470-ea68408a58ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th>Language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">-1</th>\n",
       "      <th>af</th>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng-ron-wol-sna</th>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sna</th>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wol</th>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>pcm</th>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>bam</th>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbj</th>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nya</th>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zul</th>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
       "      <th>ewe</th>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hau</th>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mos</th>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yor</th>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>fon</th>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kin</th>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">4</th>\n",
       "      <th>ibo</th>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lug</th>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swa</th>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twi</th>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xho</th>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Word    Pos  family  region\n",
       "fold Language                                     \n",
       "-1   af                1899   1899    1899    1899\n",
       "     ar                7534   7534    7534    7534\n",
       "     en               15532  15532   15532   15532\n",
       "     eng-ron-wol-sna  13120  13120   13120   13120\n",
       "     fr               16339  16339   16339   16339\n",
       "     sna               1492   1492    1492    1492\n",
       "     wol               1563   1563    1563    1563\n",
       " 0   pcm              10338  10338   10338   10338\n",
       " 1   bam               2481   2481    2481    2481\n",
       "     bbj               1484   1484    1484    1484\n",
       "     nya               1440   1440    1440    1440\n",
       "     zul               1503   1503    1503    1503\n",
       " 2   ewe               1453   1453    1453    1453\n",
       "     hau               1484   1484    1484    1484\n",
       "     mos               1508   1508    1508    1508\n",
       "     yor               2096   2096    2096    2096\n",
       " 3   fon               1617   1617    1617    1617\n",
       "     kin               1461   1461    1461    1461\n",
       " 4   ibo               1603   1603    1603    1603\n",
       "     lug               1461   1461    1461    1461\n",
       "     swa               1383   1383    1383    1383\n",
       "     twi               1567   1567    1567    1567\n",
       "     xho               1501   1501    1501    1501"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.groupby(['fold', 'Language']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de588c-aabf-4134-b3a3-c208e25b642b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8677ddf8-cd35-411b-bb67-397c2f501348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uthi abalindelanga zingxaki eBuffalo City Muni...</td>\n",
       "      <td>VERB VERB NOUN ADP PROPN PROPN CCONJ VERB NOUN...</td>\n",
       "      <td>xho</td>\n",
       "      <td>Bantu</td>\n",
       "      <td>South</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ons onderwys- en vaardigheidsprogramme sal ons...</td>\n",
       "      <td>PRON X CCONJ NOUN AUX PRON NOUN CCONJ NOUN VER...</td>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Òntajà ojú títì kan ló ṣekúpa Ayẹni lọ́...</td>\n",
       "      <td>NOUN NOUN NOUN NUM PRON VERB PROPN NOUN NUM PU...</td>\n",
       "      <td>yor</td>\n",
       "      <td>Volta-Niger</td>\n",
       "      <td>West</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tili mnjira , yemwe amaoneka kuti ndi mkulu wa...</td>\n",
       "      <td>VERB NOUN PUNCT PRON VERB ADP VERB NOUN ADP NO...</td>\n",
       "      <td>nya</td>\n",
       "      <td>Bantu</td>\n",
       "      <td>East</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Àṣejèrè : Àwọn wo làwòkọ́ṣe yín nínú ...</td>\n",
       "      <td>PROPN PUNCT PRON ADJ NOUN DET ADP NOUN NOUN NO...</td>\n",
       "      <td>yor</td>\n",
       "      <td>Volta-Niger</td>\n",
       "      <td>West</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Uthi abalindelanga zingxaki eBuffalo City Muni...   \n",
       "1  Ons onderwys- en vaardigheidsprogramme sal ons...   \n",
       "2  Òntajà ojú títì kan ló ṣekúpa Ayẹni lọ́...   \n",
       "3  Tili mnjira , yemwe amaoneka kuti ndi mkulu wa...   \n",
       "4  Àṣejèrè : Àwọn wo làwòkọ́ṣe yín nínú ...   \n",
       "\n",
       "                                                 Pos Language       family  \\\n",
       "0  VERB VERB NOUN ADP PROPN PROPN CCONJ VERB NOUN...      xho        Bantu   \n",
       "1  PRON X CCONJ NOUN AUX PRON NOUN CCONJ NOUN VER...       af           af   \n",
       "2  NOUN NOUN NOUN NUM PRON VERB PROPN NOUN NUM PU...      yor  Volta-Niger   \n",
       "3  VERB NOUN PUNCT PRON VERB ADP VERB NOUN ADP NO...      nya        Bantu   \n",
       "4  PROPN PUNCT PRON ADJ NOUN DET ADP NOUN NOUN NO...      yor  Volta-Niger   \n",
       "\n",
       "  region  fold  \n",
       "0  South     4  \n",
       "1     af    -1  \n",
       "2   West     2  \n",
       "3   East     1  \n",
       "4   West     2  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4e4278d-e53b-4675-a97f-1643fac183bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': 0,\n",
       " 'ADP': 1,\n",
       " 'ADV': 2,\n",
       " 'AUX': 3,\n",
       " 'CCONJ': 4,\n",
       " 'DET': 5,\n",
       " 'INTJ': 6,\n",
       " 'NOUN': 7,\n",
       " 'NUM': 8,\n",
       " 'PART': 9,\n",
       " 'PRON': 10,\n",
       " 'PROPN': 11,\n",
       " 'PUNCT': 12,\n",
       " 'SCONJ': 13,\n",
       " 'SYM': 14,\n",
       " 'VERB': 15,\n",
       " 'X': 16,\n",
       " 'NAW': 17}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_column_name = 'Word'\n",
    "label_column_name = 'Pos'\n",
    "label_list = sorted(train[label_column_name].unique()) + ['NAW'] # Not A Word\n",
    "\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9478b6bc-ce17-412b-9bac-ef7b581460f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ADJ',\n",
       " 1: 'ADP',\n",
       " 2: 'ADV',\n",
       " 3: 'AUX',\n",
       " 4: 'CCONJ',\n",
       " 5: 'DET',\n",
       " 6: 'INTJ',\n",
       " 7: 'NOUN',\n",
       " 8: 'NUM',\n",
       " 9: 'PART',\n",
       " 10: 'PRON',\n",
       " 11: 'PROPN',\n",
       " 12: 'PUNCT',\n",
       " 13: 'SCONJ',\n",
       " 14: 'SYM',\n",
       " 15: 'VERB',\n",
       " 16: 'X',\n",
       " 17: 'NAW'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label = {v:k for k,v in label_to_id.items()}\n",
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a78a58e7-22ef-49cc-857e-9811cf748e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        # config=config,\n",
    "        num_labels=num_labels, id2label=id_to_label, label2id=label_to_id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04218f8f-11fc-429e-b70e-c59931217409",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Davlan/afro-xlmr-large-75L'\n",
    "\n",
    "max_seq_length = 256\n",
    "padding = False\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "tokenizer_name_or_path = model_name\n",
    "if config.model_type in {\"gpt2\", \"roberta\"}:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name_or_path,\n",
    "        use_fast=True,\n",
    "        add_prefix_space=True,\n",
    "    )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name_or_path,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    \n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, max_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54b5558a-0961-4dc5-ba9e-8f4103accfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(examples):\n",
    "    is_test = examples.get(label_column_name) is None\n",
    "    \n",
    "    for idx in range(len(examples[text_column_name])):\n",
    "        if not is_test:\n",
    "            examples[label_column_name][idx] = examples[label_column_name][idx].split(' ')\n",
    "        examples[text_column_name][idx] = examples[text_column_name][idx].split(' ')\n",
    "        \n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length if not is_test else None,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    \n",
    "    if not is_test:\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[label_column_name]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label_to_id[label[word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(label_to_id['NAW'])\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            labels.append(label_ids)\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8f08892-823c-474a-a3f1-43480ec1213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cee4234b-51b0-4f63-9050-7c226b1596d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l not in [-100, 17]]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    lengths = list(map(len, true_labels))\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l not in [-100, 17]]\n",
    "        for prediction, label, length in zip(predictions, labels, lengths)\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f668b84f-ac49-4378-8cea-b0c9c3ecad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32045/32045 [00:00<00:00, 533128.42it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "words = []\n",
    "\n",
    "last_lang = test.Language.values[0]\n",
    "for i, row in enumerate(tqdm(test.itertuples(), total=len(test))):\n",
    "    words.append(row.Word)\n",
    "    \n",
    "    if row.Word == '.' or i == len(test)-1 or row.Language != last_lang:\n",
    "        sentences.append(' '.join(words))\n",
    "        words = []\n",
    "        \n",
    "    last_lang = row.Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00797992-b49d-47d1-bc47-6cbffc86ccf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda2b7ced6b54c1b9cd7e4181b6c7cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1974 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1974\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = HFDataset.from_pandas(pd.DataFrame({'Word': sentences}))\n",
    "test_dataset = test_dataset.map(\n",
    "    process_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=['Word'],\n",
    ")\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94e34428-b18d-4949-83cc-e625b0ecf5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(self, model, optimizer, *, adv_param='weight',\n",
    "                 adv_lr=0.001, adv_eps=0.001):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.backup = {}\n",
    "\n",
    "    def perturb(self):\n",
    "        \"\"\"\n",
    "        Perturb model parameters for AWP gradient\n",
    "        Call before loss and loss.backward()\n",
    "        \"\"\"\n",
    "        self._save()  # save model parameters\n",
    "        self._attack_step()  # perturb weights\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                grad = self.optimizer.state[param]['exp_avg']\n",
    "                norm_grad = torch.norm(grad)\n",
    "                norm_data = torch.norm(param.detach())\n",
    "\n",
    "                if norm_grad != 0 and not torch.isnan(norm_grad):\n",
    "                    # Set lower and upper limit in change\n",
    "                    limit_eps = self.adv_eps * param.detach().abs()\n",
    "                    param_min = param.data - limit_eps\n",
    "                    param_max = param.data + limit_eps\n",
    "\n",
    "                    # Perturb along gradient\n",
    "                    # w += (adv_lr * |w| / |grad|) * grad\n",
    "                    param.data.add_(grad, alpha=(self.adv_lr * (norm_data + e) / (norm_grad + e)))\n",
    "\n",
    "                    # Apply the limit to the change\n",
    "                    param.data.clamp_(param_min, param_max)\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.clone().detach()\n",
    "                else:\n",
    "                    self.backup[name].copy_(param.data)\n",
    "\n",
    "    def restore(self):\n",
    "        \"\"\"\n",
    "        Restore model parameter to correct position; AWP do not perturbe weights, it perturb gradients\n",
    "        Call after loss.backward(), before optimizer.step()\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data.copy_(self.backup[name])\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.awp = AWP(self.model, self.optimizer, adv_lr=0.001, adv_eps=0.001)\n",
    "    \n",
    "    def training_step(self, model: nn.Module, inputs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        # if is_sagemaker_mp_enabled():\n",
    "        #     loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n",
    "        #     return loss_mb.reduce_mean().detach().to(self.args.device)\n",
    "        \n",
    "        self.awp.perturb() # \n",
    "        \n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        if self.do_grad_scaling:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            self.accelerator.backward(loss)\n",
    "            \n",
    "        self.awp.restore()\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            if is_peft_available() and isinstance(model, PeftModel):\n",
    "                model_name = unwrap_model(model.base_model)._get_name()\n",
    "            else:\n",
    "                model_name = unwrap_model(model)._get_name()\n",
    "            if model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n",
    "                loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
    "            else:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
    "                raise ValueError(\n",
    "                    \"The model did not return a loss from the inputs, only the following keys: \"\n",
    "                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
    "                )\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c3ab5b5-1122-4b5a-bf90-b3c07e34c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(fold):\n",
    "    raw_dataset = DatasetDict({\n",
    "        'train': HFDataset.from_pandas(train_ner[train_ner.fold != fold]),\n",
    "        'validation': HFDataset.from_pandas(train_ner[train_ner.fold == fold])\n",
    "    }).remove_columns(column_names=['fold', '__index_level_0__', 'Language', 'family', 'region'])\n",
    "\n",
    "    train_dataset = raw_dataset[\"train\"]\n",
    "    column_names = train_dataset.column_names\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        process_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "    )\n",
    "    if max_seq_length is not None:\n",
    "        train_dataset = train_dataset.filter(\n",
    "            lambda example: len(example['input_ids']) <= max_seq_length\n",
    "        )\n",
    "\n",
    "    eval_dataset = raw_dataset['validation']\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        process_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "    )\n",
    "    if max_seq_length is not None:\n",
    "        eval_dataset = eval_dataset.filter(\n",
    "            lambda example: len(example['input_ids']) <= max_seq_length\n",
    "        )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'pos-tagging-ner/{model_name.replace(\"/\", \"-\")}/{fold}',\n",
    "        learning_rate=2e-5,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        group_by_length=True,\n",
    "        overwrite_output_dir=True,\n",
    "        warmup_steps=0.15,\n",
    "        num_train_epochs=5,\n",
    "        lr_scheduler_type ='cosine',\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,\n",
    "        fp16=True,\n",
    "        report_to='none'\n",
    "    )\n",
    "\n",
    "    model = load_model()\n",
    "\n",
    "    trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    results = trainer.evaluate(eval_dataset)\n",
    "    \n",
    "    test_pos_ids = trainer.predict(test_dataset)\n",
    "    \n",
    "    return results['eval_accuracy'], test_pos_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65bf22a7-cfe7-454b-a739-20bfc29d1430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a4775b61944c37bfea5e665b0adb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/84951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e9f918e4dd4ceba8d225c0cba1d3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/84951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd8989d5fa44e50a2c9111cea17bc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d57fb544954f0084ad4d78bbd74b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13275' max='13275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13275/13275 49:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.404000</td>\n",
       "      <td>0.541685</td>\n",
       "      <td>0.607280</td>\n",
       "      <td>0.597077</td>\n",
       "      <td>0.602135</td>\n",
       "      <td>0.655056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.540332</td>\n",
       "      <td>0.622369</td>\n",
       "      <td>0.587113</td>\n",
       "      <td>0.604227</td>\n",
       "      <td>0.659732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.550769</td>\n",
       "      <td>0.609289</td>\n",
       "      <td>0.591387</td>\n",
       "      <td>0.600205</td>\n",
       "      <td>0.657720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.581688</td>\n",
       "      <td>0.605469</td>\n",
       "      <td>0.580877</td>\n",
       "      <td>0.592918</td>\n",
       "      <td>0.653141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.561374</td>\n",
       "      <td>0.603145</td>\n",
       "      <td>0.579452</td>\n",
       "      <td>0.591061</td>\n",
       "      <td>0.645753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.602891</td>\n",
       "      <td>0.599811</td>\n",
       "      <td>0.584595</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>0.643403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.582975</td>\n",
       "      <td>0.603565</td>\n",
       "      <td>0.583977</td>\n",
       "      <td>0.593610</td>\n",
       "      <td>0.649793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.604213</td>\n",
       "      <td>0.599810</td>\n",
       "      <td>0.571459</td>\n",
       "      <td>0.585292</td>\n",
       "      <td>0.644368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.602794</td>\n",
       "      <td>0.599306</td>\n",
       "      <td>0.580016</td>\n",
       "      <td>0.589503</td>\n",
       "      <td>0.642485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.607784</td>\n",
       "      <td>0.608073</td>\n",
       "      <td>0.587319</td>\n",
       "      <td>0.597516</td>\n",
       "      <td>0.651113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>0.637323</td>\n",
       "      <td>0.600179</td>\n",
       "      <td>0.577866</td>\n",
       "      <td>0.588811</td>\n",
       "      <td>0.643861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.623340</td>\n",
       "      <td>0.601972</td>\n",
       "      <td>0.582042</td>\n",
       "      <td>0.591839</td>\n",
       "      <td>0.645680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.624297</td>\n",
       "      <td>0.601344</td>\n",
       "      <td>0.580411</td>\n",
       "      <td>0.590692</td>\n",
       "      <td>0.645656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.632912</td>\n",
       "      <td>0.595486</td>\n",
       "      <td>0.577077</td>\n",
       "      <td>0.586137</td>\n",
       "      <td>0.640063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.620316</td>\n",
       "      <td>0.602223</td>\n",
       "      <td>0.581145</td>\n",
       "      <td>0.591497</td>\n",
       "      <td>0.646429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.646659</td>\n",
       "      <td>0.600521</td>\n",
       "      <td>0.580456</td>\n",
       "      <td>0.590318</td>\n",
       "      <td>0.645334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.645485</td>\n",
       "      <td>0.603058</td>\n",
       "      <td>0.584577</td>\n",
       "      <td>0.593674</td>\n",
       "      <td>0.646614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.652381</td>\n",
       "      <td>0.601423</td>\n",
       "      <td>0.583412</td>\n",
       "      <td>0.592281</td>\n",
       "      <td>0.645881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>0.601082</td>\n",
       "      <td>0.581450</td>\n",
       "      <td>0.591103</td>\n",
       "      <td>0.645938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.654302</td>\n",
       "      <td>0.602088</td>\n",
       "      <td>0.585088</td>\n",
       "      <td>0.593466</td>\n",
       "      <td>0.647121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.042800</td>\n",
       "      <td>0.654535</td>\n",
       "      <td>0.602945</td>\n",
       "      <td>0.585500</td>\n",
       "      <td>0.594095</td>\n",
       "      <td>0.647684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>0.663398</td>\n",
       "      <td>0.602413</td>\n",
       "      <td>0.584730</td>\n",
       "      <td>0.593440</td>\n",
       "      <td>0.646219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.667839</td>\n",
       "      <td>0.601760</td>\n",
       "      <td>0.583977</td>\n",
       "      <td>0.592735</td>\n",
       "      <td>0.646895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.666157</td>\n",
       "      <td>0.602792</td>\n",
       "      <td>0.584649</td>\n",
       "      <td>0.593582</td>\n",
       "      <td>0.647837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.667270</td>\n",
       "      <td>0.602413</td>\n",
       "      <td>0.584309</td>\n",
       "      <td>0.593223</td>\n",
       "      <td>0.647539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.667484</td>\n",
       "      <td>0.602409</td>\n",
       "      <td>0.584353</td>\n",
       "      <td>0.593244</td>\n",
       "      <td>0.647644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6597320027363084\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336310f7f2c940c3997da2d326715574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/85318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e63e1d2732d4c839472d6859e50da38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/85318 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd32429f0efd42f984ee3cc44ff52004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae734a555d734832b8250d24b2f7dad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13335' max='13335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13335/13335 49:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.404600</td>\n",
       "      <td>0.513209</td>\n",
       "      <td>0.641022</td>\n",
       "      <td>0.618049</td>\n",
       "      <td>0.629326</td>\n",
       "      <td>0.703769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.156900</td>\n",
       "      <td>0.484849</td>\n",
       "      <td>0.652666</td>\n",
       "      <td>0.643223</td>\n",
       "      <td>0.647910</td>\n",
       "      <td>0.711259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.508438</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.628661</td>\n",
       "      <td>0.642949</td>\n",
       "      <td>0.717555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.500881</td>\n",
       "      <td>0.659461</td>\n",
       "      <td>0.645056</td>\n",
       "      <td>0.652179</td>\n",
       "      <td>0.717832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.102500</td>\n",
       "      <td>0.485136</td>\n",
       "      <td>0.663873</td>\n",
       "      <td>0.654636</td>\n",
       "      <td>0.659222</td>\n",
       "      <td>0.721635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.517942</td>\n",
       "      <td>0.662477</td>\n",
       "      <td>0.651312</td>\n",
       "      <td>0.656847</td>\n",
       "      <td>0.720873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.521233</td>\n",
       "      <td>0.671377</td>\n",
       "      <td>0.656913</td>\n",
       "      <td>0.664066</td>\n",
       "      <td>0.728602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.522395</td>\n",
       "      <td>0.666143</td>\n",
       "      <td>0.651225</td>\n",
       "      <td>0.658599</td>\n",
       "      <td>0.724263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.524777</td>\n",
       "      <td>0.675096</td>\n",
       "      <td>0.660782</td>\n",
       "      <td>0.667863</td>\n",
       "      <td>0.731385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.530424</td>\n",
       "      <td>0.671217</td>\n",
       "      <td>0.655974</td>\n",
       "      <td>0.663508</td>\n",
       "      <td>0.727621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.543175</td>\n",
       "      <td>0.667061</td>\n",
       "      <td>0.652010</td>\n",
       "      <td>0.659450</td>\n",
       "      <td>0.723979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.056800</td>\n",
       "      <td>0.525316</td>\n",
       "      <td>0.666404</td>\n",
       "      <td>0.653050</td>\n",
       "      <td>0.659660</td>\n",
       "      <td>0.723747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.532127</td>\n",
       "      <td>0.665506</td>\n",
       "      <td>0.654527</td>\n",
       "      <td>0.659971</td>\n",
       "      <td>0.722836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.542815</td>\n",
       "      <td>0.666548</td>\n",
       "      <td>0.652643</td>\n",
       "      <td>0.659522</td>\n",
       "      <td>0.722875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.556225</td>\n",
       "      <td>0.660889</td>\n",
       "      <td>0.649573</td>\n",
       "      <td>0.655183</td>\n",
       "      <td>0.719188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.549512</td>\n",
       "      <td>0.668548</td>\n",
       "      <td>0.654825</td>\n",
       "      <td>0.661615</td>\n",
       "      <td>0.725897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>0.561507</td>\n",
       "      <td>0.664924</td>\n",
       "      <td>0.649544</td>\n",
       "      <td>0.657144</td>\n",
       "      <td>0.722468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.576046</td>\n",
       "      <td>0.665894</td>\n",
       "      <td>0.653785</td>\n",
       "      <td>0.659784</td>\n",
       "      <td>0.723624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.585396</td>\n",
       "      <td>0.663475</td>\n",
       "      <td>0.651792</td>\n",
       "      <td>0.657582</td>\n",
       "      <td>0.721222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.572371</td>\n",
       "      <td>0.665142</td>\n",
       "      <td>0.651406</td>\n",
       "      <td>0.658202</td>\n",
       "      <td>0.722875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.043200</td>\n",
       "      <td>0.581146</td>\n",
       "      <td>0.667835</td>\n",
       "      <td>0.654098</td>\n",
       "      <td>0.660895</td>\n",
       "      <td>0.725212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.587992</td>\n",
       "      <td>0.666327</td>\n",
       "      <td>0.652243</td>\n",
       "      <td>0.659210</td>\n",
       "      <td>0.723811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.584991</td>\n",
       "      <td>0.666217</td>\n",
       "      <td>0.653356</td>\n",
       "      <td>0.659724</td>\n",
       "      <td>0.723740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.037800</td>\n",
       "      <td>0.588124</td>\n",
       "      <td>0.666439</td>\n",
       "      <td>0.653618</td>\n",
       "      <td>0.659966</td>\n",
       "      <td>0.723979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>0.588747</td>\n",
       "      <td>0.666177</td>\n",
       "      <td>0.652832</td>\n",
       "      <td>0.659437</td>\n",
       "      <td>0.723869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.588700</td>\n",
       "      <td>0.666164</td>\n",
       "      <td>0.652825</td>\n",
       "      <td>0.659427</td>\n",
       "      <td>0.723863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7313851080864692\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e40062a214480ba35b94d46a899b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/88781 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64d4759a2504ec198a18e8327c50bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/88781 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7e7182da0a4fb095df09f99c389af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2cac95d44347849ea4031111ddfeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13875' max='13875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13875/13875 48:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.430500</td>\n",
       "      <td>0.448043</td>\n",
       "      <td>0.650956</td>\n",
       "      <td>0.647181</td>\n",
       "      <td>0.649063</td>\n",
       "      <td>0.713377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.434258</td>\n",
       "      <td>0.666747</td>\n",
       "      <td>0.663337</td>\n",
       "      <td>0.665038</td>\n",
       "      <td>0.726574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.133100</td>\n",
       "      <td>0.471885</td>\n",
       "      <td>0.664739</td>\n",
       "      <td>0.671143</td>\n",
       "      <td>0.667926</td>\n",
       "      <td>0.725191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.439095</td>\n",
       "      <td>0.670029</td>\n",
       "      <td>0.668541</td>\n",
       "      <td>0.669284</td>\n",
       "      <td>0.729643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.454383</td>\n",
       "      <td>0.669228</td>\n",
       "      <td>0.670876</td>\n",
       "      <td>0.670051</td>\n",
       "      <td>0.729080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.098200</td>\n",
       "      <td>0.482453</td>\n",
       "      <td>0.667486</td>\n",
       "      <td>0.672272</td>\n",
       "      <td>0.669870</td>\n",
       "      <td>0.727777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.492608</td>\n",
       "      <td>0.665960</td>\n",
       "      <td>0.669873</td>\n",
       "      <td>0.667911</td>\n",
       "      <td>0.726585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>0.462485</td>\n",
       "      <td>0.670640</td>\n",
       "      <td>0.675585</td>\n",
       "      <td>0.673103</td>\n",
       "      <td>0.730643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>0.480502</td>\n",
       "      <td>0.663555</td>\n",
       "      <td>0.669391</td>\n",
       "      <td>0.666460</td>\n",
       "      <td>0.725360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.494198</td>\n",
       "      <td>0.663992</td>\n",
       "      <td>0.668947</td>\n",
       "      <td>0.666460</td>\n",
       "      <td>0.725180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>0.476368</td>\n",
       "      <td>0.663783</td>\n",
       "      <td>0.670978</td>\n",
       "      <td>0.667361</td>\n",
       "      <td>0.725315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.061200</td>\n",
       "      <td>0.521452</td>\n",
       "      <td>0.657015</td>\n",
       "      <td>0.663553</td>\n",
       "      <td>0.660268</td>\n",
       "      <td>0.719009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.062000</td>\n",
       "      <td>0.485708</td>\n",
       "      <td>0.669691</td>\n",
       "      <td>0.671790</td>\n",
       "      <td>0.670739</td>\n",
       "      <td>0.729058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>0.524869</td>\n",
       "      <td>0.662368</td>\n",
       "      <td>0.667564</td>\n",
       "      <td>0.664956</td>\n",
       "      <td>0.723201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.514489</td>\n",
       "      <td>0.660759</td>\n",
       "      <td>0.662906</td>\n",
       "      <td>0.661830</td>\n",
       "      <td>0.721999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.509119</td>\n",
       "      <td>0.664430</td>\n",
       "      <td>0.666167</td>\n",
       "      <td>0.665298</td>\n",
       "      <td>0.724719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.542118</td>\n",
       "      <td>0.659456</td>\n",
       "      <td>0.665457</td>\n",
       "      <td>0.662443</td>\n",
       "      <td>0.722145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.551540</td>\n",
       "      <td>0.664956</td>\n",
       "      <td>0.667614</td>\n",
       "      <td>0.666282</td>\n",
       "      <td>0.725472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.661740</td>\n",
       "      <td>0.665990</td>\n",
       "      <td>0.663858</td>\n",
       "      <td>0.723325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.046900</td>\n",
       "      <td>0.549072</td>\n",
       "      <td>0.661094</td>\n",
       "      <td>0.664492</td>\n",
       "      <td>0.662789</td>\n",
       "      <td>0.722504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.551131</td>\n",
       "      <td>0.661797</td>\n",
       "      <td>0.665165</td>\n",
       "      <td>0.663476</td>\n",
       "      <td>0.723078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.548130</td>\n",
       "      <td>0.663399</td>\n",
       "      <td>0.666472</td>\n",
       "      <td>0.664932</td>\n",
       "      <td>0.724449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.561889</td>\n",
       "      <td>0.664424</td>\n",
       "      <td>0.669188</td>\n",
       "      <td>0.666797</td>\n",
       "      <td>0.725461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.569139</td>\n",
       "      <td>0.661642</td>\n",
       "      <td>0.666789</td>\n",
       "      <td>0.664206</td>\n",
       "      <td>0.722819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.565813</td>\n",
       "      <td>0.661998</td>\n",
       "      <td>0.666536</td>\n",
       "      <td>0.664259</td>\n",
       "      <td>0.723179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.565073</td>\n",
       "      <td>0.662796</td>\n",
       "      <td>0.667221</td>\n",
       "      <td>0.665001</td>\n",
       "      <td>0.723752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.564952</td>\n",
       "      <td>0.662813</td>\n",
       "      <td>0.667297</td>\n",
       "      <td>0.665048</td>\n",
       "      <td>0.723775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7306429856115108\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcc729c7b974a14a759dc7d63de3940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/84344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01231d71eb56492b856210dc7a9e2b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/84344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9a751aa9514f528ec9c721662101eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3cbe2ae0374cd883e6ef7adfec053c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-large-75L and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13180' max='13180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13180/13180 50:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.390400</td>\n",
       "      <td>0.551822</td>\n",
       "      <td>0.654132</td>\n",
       "      <td>0.666377</td>\n",
       "      <td>0.660198</td>\n",
       "      <td>0.715327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>0.597185</td>\n",
       "      <td>0.658726</td>\n",
       "      <td>0.667621</td>\n",
       "      <td>0.663143</td>\n",
       "      <td>0.715790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.576626</td>\n",
       "      <td>0.652571</td>\n",
       "      <td>0.666883</td>\n",
       "      <td>0.659650</td>\n",
       "      <td>0.713413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.577427</td>\n",
       "      <td>0.654597</td>\n",
       "      <td>0.665902</td>\n",
       "      <td>0.660201</td>\n",
       "      <td>0.715643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.585187</td>\n",
       "      <td>0.653020</td>\n",
       "      <td>0.667146</td>\n",
       "      <td>0.660007</td>\n",
       "      <td>0.713109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.599712</td>\n",
       "      <td>0.656096</td>\n",
       "      <td>0.670182</td>\n",
       "      <td>0.663065</td>\n",
       "      <td>0.717557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>0.608719</td>\n",
       "      <td>0.657331</td>\n",
       "      <td>0.670026</td>\n",
       "      <td>0.663618</td>\n",
       "      <td>0.717948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.619771</td>\n",
       "      <td>0.652128</td>\n",
       "      <td>0.665559</td>\n",
       "      <td>0.658775</td>\n",
       "      <td>0.713065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>0.603344</td>\n",
       "      <td>0.660505</td>\n",
       "      <td>0.671645</td>\n",
       "      <td>0.666028</td>\n",
       "      <td>0.721477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.595753</td>\n",
       "      <td>0.661346</td>\n",
       "      <td>0.672788</td>\n",
       "      <td>0.667018</td>\n",
       "      <td>0.721483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.628121</td>\n",
       "      <td>0.659357</td>\n",
       "      <td>0.669245</td>\n",
       "      <td>0.664264</td>\n",
       "      <td>0.718981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.648736</td>\n",
       "      <td>0.658708</td>\n",
       "      <td>0.670257</td>\n",
       "      <td>0.664433</td>\n",
       "      <td>0.717780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.645973</td>\n",
       "      <td>0.658886</td>\n",
       "      <td>0.668858</td>\n",
       "      <td>0.663835</td>\n",
       "      <td>0.719797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.629859</td>\n",
       "      <td>0.657216</td>\n",
       "      <td>0.669195</td>\n",
       "      <td>0.663152</td>\n",
       "      <td>0.717350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.639998</td>\n",
       "      <td>0.655340</td>\n",
       "      <td>0.668214</td>\n",
       "      <td>0.661714</td>\n",
       "      <td>0.715262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.651868</td>\n",
       "      <td>0.659736</td>\n",
       "      <td>0.671357</td>\n",
       "      <td>0.665496</td>\n",
       "      <td>0.720694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>0.665456</td>\n",
       "      <td>0.655893</td>\n",
       "      <td>0.670032</td>\n",
       "      <td>0.662888</td>\n",
       "      <td>0.718595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.041200</td>\n",
       "      <td>0.663274</td>\n",
       "      <td>0.658614</td>\n",
       "      <td>0.671713</td>\n",
       "      <td>0.665099</td>\n",
       "      <td>0.720205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.676005</td>\n",
       "      <td>0.659059</td>\n",
       "      <td>0.671545</td>\n",
       "      <td>0.665243</td>\n",
       "      <td>0.720319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.681322</td>\n",
       "      <td>0.656822</td>\n",
       "      <td>0.670514</td>\n",
       "      <td>0.663597</td>\n",
       "      <td>0.718313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.679748</td>\n",
       "      <td>0.657712</td>\n",
       "      <td>0.670814</td>\n",
       "      <td>0.664198</td>\n",
       "      <td>0.718829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.697386</td>\n",
       "      <td>0.657451</td>\n",
       "      <td>0.670757</td>\n",
       "      <td>0.664038</td>\n",
       "      <td>0.718633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.704211</td>\n",
       "      <td>0.656424</td>\n",
       "      <td>0.670214</td>\n",
       "      <td>0.663247</td>\n",
       "      <td>0.717671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.703800</td>\n",
       "      <td>0.656599</td>\n",
       "      <td>0.670639</td>\n",
       "      <td>0.663545</td>\n",
       "      <td>0.718024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.035800</td>\n",
       "      <td>0.698693</td>\n",
       "      <td>0.657392</td>\n",
       "      <td>0.670976</td>\n",
       "      <td>0.664115</td>\n",
       "      <td>0.718650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>0.700392</td>\n",
       "      <td>0.657232</td>\n",
       "      <td>0.670882</td>\n",
       "      <td>0.663987</td>\n",
       "      <td>0.718568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7214827139951931\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "all_preds = []\n",
    "\n",
    "for fold in [1, 2, 3, 4]:\n",
    "    score, fold_pred = train_fold(fold)\n",
    "    \n",
    "    scores.append(score)\n",
    "    all_preds.append(fold_pred)\n",
    "    \n",
    "    print(score)\n",
    "    print()\n",
    "    \n",
    "avg_score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b509e41f-8169-484e-af92-3853030b1f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6597320027363084,\n",
       " 0.7313851080864692,\n",
       " 0.7306429856115108,\n",
       " 0.7214827139951931]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f41ddd1d-b6b7-40fb-b1f2-7f5b58e361ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7108107026073703"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5423319-fc1d-4e9c-ab53-0eb0122803dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 190, 18)\n"
     ]
    }
   ],
   "source": [
    "test_pos_ids = np.mean([p.predictions for p in all_preds], axis=0)\n",
    "print(test_pos_ids.shape)\n",
    "test_pos_ids = test_pos_ids.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45ce2daa-b304-4065-8ab6-40fe16f91f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pos = []\n",
    "for pos_ids, sentence in zip(test_pos_ids, sentences):\n",
    "    length = len(sentence.split(' '))\n",
    "    final_pos.extend(\n",
    "        list(map(id_to_label.get, [x for x in pos_ids if x not in [-100, 17]]))[1:length+1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab63a58c-f2ec-43d8-8f79-fbc90939c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Pos'] = final_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55dd1deb-7091-4db3-8cf9-8cd0147a6346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOUN     6739\n",
       "VERB     5061\n",
       "ADP      4254\n",
       "PROPN    3042\n",
       "PUNCT    2997\n",
       "AUX      2755\n",
       "PRON     2042\n",
       "SCONJ    1175\n",
       "ADV       898\n",
       "DET       722\n",
       "CCONJ     699\n",
       "ADJ       539\n",
       "NUM       536\n",
       "PART      533\n",
       "X          26\n",
       "INTJ       15\n",
       "SYM        12\n",
       "Name: Pos, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Pos.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd03bca7-647e-41ce-84c7-6ffbf1096b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['Id', 'Pos']].to_csv(f'submissions/pos-ner-{model_name.split(\"/\")[-1]}-{avg_score:.3f}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ff57974-cd0b-47c0-9356-e08ceb7af5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "      <th>Language</th>\n",
       "      <th>Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Id00qog2f11n_0</td>\n",
       "      <td>Ne</td>\n",
       "      <td>luo</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Id00qog2f11n_1</td>\n",
       "      <td>otim</td>\n",
       "      <td>luo</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Id00qog2f11n_2</td>\n",
       "      <td>penj</td>\n",
       "      <td>luo</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Id00qog2f11n_3</td>\n",
       "      <td>e</td>\n",
       "      <td>luo</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Id00qog2f11n_4</td>\n",
       "      <td>kind</td>\n",
       "      <td>luo</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  Word Language   Pos\n",
       "0  Id00qog2f11n_0    Ne      luo   AUX\n",
       "1  Id00qog2f11n_1  otim      luo  VERB\n",
       "2  Id00qog2f11n_2  penj      luo  NOUN\n",
       "3  Id00qog2f11n_3     e      luo   ADP\n",
       "4  Id00qog2f11n_4  kind      luo  NOUN"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "93c85bbd-29a8-40f8-933b-5e45111e84be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test[test.Pos == 'SYM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b6f72-85c4-4881-8319-c59d5dd08de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
