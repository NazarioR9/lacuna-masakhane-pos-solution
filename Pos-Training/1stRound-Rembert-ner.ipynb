{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a239eea-e414-412d-9242-d7316e9cc18d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a7fcd7-eb6b-4ee8-8f57-ff5a443c78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q torch==1.13.1 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6eb2fe-f515-471e-8ae3-1aae649d697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c866a9c-6192-4cf1-b8e7-1b96c98953f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1961ba0b-d202-4477-8004-63b1c5e5ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 16 12:29:02 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   51C    P8    32W / 300W |  19567MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2a20d7-05ea-4951-a079-08c8b8a71940",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16825bfc-8853-445c-9e6c-53395bea41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "import subprocess\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedGroupKFold, GroupKFold\n",
    "from sklearn.metrics import f1_score, mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f6e65d9-657c-4b4d-a20c-ccd0dc3f5509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b9b22b-0a41-44b0-beec-e050afbeaa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR\n",
    "from torch import LongTensor\n",
    "from torch import nn, optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4c3033-b975-4aac-bc6b-24346ea9c6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "# from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm, trange\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict, Dataset as HFDataset\n",
    "from datasets import concatenate_datasets, interleave_datasets\n",
    "from datasets import ClassLabel, load_metric\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a120b4-51ef-49dc-9748-873fbd66e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext watermark\n",
    "# %watermark --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563effdb-c378-45ba-8f7e-8d7cc8bf8b08",
   "metadata": {},
   "source": [
    "# Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db742f8-2be3-47d5-bd7b-82629ce77866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_warnings(strict=False):\n",
    "\twarnings.simplefilter('ignore')\n",
    "\tif strict:\n",
    "\t\tlogging.disable(logging.WARNING)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "\trandom.seed(seed)\n",
    "\tos.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c01b0cb9-83b7-46c0-8383-c9384772e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "disable_warnings()\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7fe6b-98d6-4d75-9912-bf1bf44b6643",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe357fad-adaf-4b32-8c8a-3bfba4d57170",
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = '../masakhane-pos/data/'\n",
    "add_path = '../masakhane-pos/transfer_corpus/'\n",
    "pseudo_path = './pseudos/pos-ner-afro-xlmr-large-75L-best.csv'\n",
    "\n",
    "langs = sorted(os.listdir(path))\n",
    "add_langs = os.listdir(add_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5bd3e9-2a48-4227-af3e-24fde850a994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lang2family = {\n",
    " 'bam': 'Mande',\n",
    " 'bbj': 'Grassfields',\n",
    " 'ewe': 'Kwa',\n",
    " 'fon': 'Volta-Niger',\n",
    " 'hau': 'Chadic',\n",
    " 'ibo': 'Volta-Niger',\n",
    " 'kin': 'Bantu',\n",
    " 'lug': 'Bantu',\n",
    " 'mos': 'Gur',\n",
    " 'nya': 'Bantu',\n",
    " 'pcm': 'English-Creole',\n",
    " 'sna': 'Bantu',\n",
    " 'swa': 'Bantu',\n",
    " 'twi': 'Kwa',\n",
    " 'wol': 'Senegambia',\n",
    " 'xho': 'Bantu',\n",
    " 'yor': 'Volta-Niger',\n",
    " 'zul': 'Bantu'\n",
    "}\n",
    "\n",
    "lang2region = {\n",
    "    'bam': 'West',\n",
    "    'bbj': 'Central',\n",
    "    'ewe': 'West',\n",
    "    'fon': 'West',\n",
    "    'hau': 'West',\n",
    "    'ibo': 'West',\n",
    "    'kin': 'East',\n",
    "    'lug': 'East',\n",
    "    'mos': 'West',\n",
    "    'nya': 'East',\n",
    "    'pcm': 'West',\n",
    "    'sna': 'South',\n",
    "    'swa': 'East',\n",
    "    'twi': 'South',\n",
    "    'wol': 'West',\n",
    "    'xho': 'South',\n",
    "    'yor': 'West',\n",
    "    'zul': 'South'\n",
    "}\n",
    "\n",
    "mapper = {'NOUN': 0,\n",
    " 'ADJ': 1,\n",
    " 'PUNCT': 2,\n",
    " 'CCONJ': 3,\n",
    " 'PRON': 4,\n",
    " 'ADV': 5,\n",
    " 'AUX': 6,\n",
    " 'VERB': 7,\n",
    " 'ADP': 8,\n",
    " 'PART': 9,\n",
    " 'SCONJ': 10,\n",
    " 'PROPN': 11,\n",
    " 'X': 12,\n",
    " 'NUM': 13,\n",
    " 'INTJ': 14,\n",
    " 'SYM': 15,\n",
    " 'DET': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d2ead2-2302-480b-82b6-37346e9e07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2fix = {\n",
    "    'yo': 'yor',\n",
    "    'bm': 'bam'\n",
    "}\n",
    "\n",
    "def load_lang_data(data_path, lang, split='train'):\n",
    "    data = []\n",
    "    language = lang.split('_')[0]\n",
    "    language = lang2fix.get(language, language)\n",
    "    \n",
    "    try:\n",
    "        with open(f'{data_path}{lang}/{split}.txt', 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                splits = line.split(' ')\n",
    "                if len(splits) != 2:\n",
    "                    continue\n",
    "                data.append(splits)\n",
    "        data = np.array(data).squeeze()\n",
    "        data = pd.DataFrame({\n",
    "            'Word': data[:, 0],\n",
    "            'Pos': data[:, 1],\n",
    "            'Language': language.split('_')[0]\n",
    "        })\n",
    "        data['family'] = lang2family.get(language, language)\n",
    "        data['region'] = lang2region.get(language, language)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d23f39a4-6053-418d-b865-aaf8f4af5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lang_data_ner(data_path, lang, split='train'):\n",
    "    data = []\n",
    "    language = lang.split('_')[0]\n",
    "    language = lang2fix.get(language, language)\n",
    "    \n",
    "    try:\n",
    "        with open(f'{data_path}{lang}/{split}.txt', 'r') as f:\n",
    "            sentences, labels = [], []\n",
    "            \n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                splits = line.split(' ')\n",
    "                \n",
    "                if len(splits) == 2:\n",
    "                    sentences.append(splits[0])\n",
    "                    labels.append(splits[1])\n",
    "                elif len(splits) == 1:\n",
    "                    data.append(\n",
    "                        [\n",
    "                            ' '.join(sentences),\n",
    "                            ' '.join(labels),\n",
    "                        ]\n",
    "                    )\n",
    "                    \n",
    "                    sentences, labels = [], []\n",
    "\n",
    "        data = np.array(data).squeeze()\n",
    "        data = pd.DataFrame({\n",
    "            'Word': data[:, 0],\n",
    "            'Pos': data[:, 1],\n",
    "            'Language': language.split('_')[0]\n",
    "        })\n",
    "        data['family'] = lang2family.get(language, language)\n",
    "        data['region'] = lang2region.get(language, language)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "378e3114-de98-4f91-a64f-e7ebf677b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):  \n",
    "    # pos = df.Pos.unique()\n",
    "    use_pos = ['PUNCT', 'SYM']\n",
    "    \n",
    "    df_others = df[~df.Pos.isin(use_pos)]\n",
    "    df_toclean = df[df.Pos.isin(use_pos)]\n",
    "    \n",
    "    dfs = [df_others]\n",
    "    \n",
    "    for p in use_pos:\n",
    "        df_p = df_toclean[df_toclean.Pos == p]\n",
    "        words = df_p.Word.value_counts()[df_p.Word.value_counts() < 2].index.values\n",
    "        df_p = df_p[~df_p.Word.isin(words)]\n",
    "        \n",
    "        dfs.append(df_p)\n",
    "        \n",
    "    return pd.concat(dfs).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f54e1e-1c04-40a8-b297-d7bf05d901d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_data(with_pseudo=False):\n",
    "    train = pd.concat([load_lang_data_ner(path, lang, split) for lang in langs for split in ['train', 'dev', 'test']], axis=0).dropna().reset_index(drop=True)\n",
    "    add_data = (\n",
    "        pd.concat([load_lang_data_ner(add_path, lang, split) for lang in add_langs for split in ['train', 'dev', 'test']], axis=0)\n",
    "        .dropna()\n",
    "        .drop_duplicates(subset=['Word', 'Pos'], keep='first')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    train = pd.concat([train, add_data])\n",
    "    if with_pseudo:\n",
    "        train = pd.concat([train, pd.read_csv(pseudo_path)])\n",
    "    train = train.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return train\n",
    "\n",
    "def load_pos_data():\n",
    "    train = pd.concat([load_lang_data(path, lang, split) for lang in langs for split in ['train', 'dev', 'test']], axis=0).dropna().reset_index(drop=True)\n",
    "    add_data = (\n",
    "        pd.concat([load_lang_data(add_path, lang, split) for lang in add_langs for split in ['train', 'dev', 'test']], axis=0)\n",
    "        .dropna()\n",
    "        .drop_duplicates(subset=['Word', 'Pos'], keep='first')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    train = pd.concat([train, add_data])\n",
    "    train = train[train.Pos.isin(list(mapper.keys()))]\n",
    "\n",
    "    train = train.drop_duplicates().drop_duplicates(subset=['Word', 'Language'], keep='first').reset_index(drop=True)\n",
    "    # train = train.drop_duplicates().reset_index(drop=True)\n",
    "    # train = clean_data(train)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd40436e-a4f4-4823-99b3-8ff6c05636de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muso</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ŋana</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afiriki</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tilebinyanfan</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word    Pos Language family region\n",
       "0           Muso   NOUN      bam  Mande   West\n",
       "1           ŋana    ADJ      bam  Mande   West\n",
       "2              ,  PUNCT      bam  Mande   West\n",
       "3        Afiriki   NOUN      bam  Mande   West\n",
       "4  tilebinyanfan   NOUN      bam  Mande   West"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_pseudo = True\n",
    "\n",
    "train_ner = load_ner_data(with_pseudo)\n",
    "train = load_pos_data()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fc60dcc-029f-4508-85ac-070e6a79b23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Muso ŋana , Afiriki tilebinyanfan n' a cɛmancɛ...</td>\n",
       "      <td>NOUN ADJ PUNCT NOUN NOUN CCONJ PRON NOUN ADV C...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ni mɔgɔ ka dɔgɔn kojugu , i bɛ mɔgɔw juguya mi...</td>\n",
       "      <td>CCONJ NOUN PART ADJ NOUN PUNCT PRON AUX NOUN N...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A kɔrɔtalen a ka taɲɛ fɛ kow ɲɛmɔgɔyabaaraw la...</td>\n",
       "      <td>PRON VERB PRON PART NOUN PART NOUN NOUN PART C...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ale y' a ( basikɛti ) to a ka se ka bɔ a ka so...</td>\n",
       "      <td>PRON PART PRON PUNCT NOUN PUNCT VERB PRON PART...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sannayɛlɛn galabukɛnɛya a sera ka min fiyɛ Afi...</td>\n",
       "      <td>NOUN NOUN PRON VERB PART PRON VERB PROPN NOUN ...</td>\n",
       "      <td>bam</td>\n",
       "      <td>Mande</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Muso ŋana , Afiriki tilebinyanfan n' a cɛmancɛ...   \n",
       "1  Ni mɔgɔ ka dɔgɔn kojugu , i bɛ mɔgɔw juguya mi...   \n",
       "2  A kɔrɔtalen a ka taɲɛ fɛ kow ɲɛmɔgɔyabaaraw la...   \n",
       "3  Ale y' a ( basikɛti ) to a ka se ka bɔ a ka so...   \n",
       "4  Sannayɛlɛn galabukɛnɛya a sera ka min fiyɛ Afi...   \n",
       "\n",
       "                                                 Pos Language family region  \n",
       "0  NOUN ADJ PUNCT NOUN NOUN CCONJ PRON NOUN ADV C...      bam  Mande   West  \n",
       "1  CCONJ NOUN PART ADJ NOUN PUNCT PRON AUX NOUN N...      bam  Mande   West  \n",
       "2  PRON VERB PRON PART NOUN PART NOUN NOUN PART C...      bam  Mande   West  \n",
       "3  PRON PART PRON PUNCT NOUN PUNCT VERB PRON PART...      bam  Mande   West  \n",
       "4  NOUN NOUN PRON VERB PART PRON VERB PROPN NOUN ...      bam  Mande   West  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1e73767-4a99-4f9f-b4dc-7c7d6eeb8ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "      <th>Language</th>\n",
       "      <th>Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Id00qog2f11n_0</td>\n",
       "      <td>Ne</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Id00qog2f11n_1</td>\n",
       "      <td>otim</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Id00qog2f11n_2</td>\n",
       "      <td>penj</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Id00qog2f11n_3</td>\n",
       "      <td>e</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Id00qog2f11n_4</td>\n",
       "      <td>kind</td>\n",
       "      <td>luo</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  Word Language  Pos\n",
       "0  Id00qog2f11n_0    Ne      luo  NaN\n",
       "1  Id00qog2f11n_1  otim      luo  NaN\n",
       "2  Id00qog2f11n_2  penj      luo  NaN\n",
       "3  Id00qog2f11n_3     e      luo  NaN\n",
       "4  Id00qog2f11n_4  kind      luo  NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../Test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07d27197-1217-4279-bffa-8052260878dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((255140, 5), (104039, 5), (32045, 4))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, train_ner.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2114cb1d-b750-4c81-b4e9-cdf1b4d7bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = train_ner[train_ner.Language.isin(langs)].reset_index(drop=True)\n",
    "add_train = train_ner[~train_ner.Language.isin(langs)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "989646fd-faa8-4d30-92a5-db49b4e2518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['fold'] = -1\n",
    "train_ner['fold'] = -1\n",
    "\n",
    "\n",
    "# val_langs = ['fon', 'pcm', 'twi', 'xho']\n",
    "# train.loc[train.Language.isin(val_langs), 'fold'] = 0\n",
    "\n",
    "base_train['fold'] = base_train['Language'].map(dict(zip(langs, range(len(langs)))))\n",
    "base_train.loc[base_train['fold'].isna(), 'fold'] = -1\n",
    "base_train['fold'] = base_train['fold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12da99ae-93ad-4f4a-9f4d-87e539c55ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    10338\n",
       "1     6908\n",
       "2     6541\n",
       "3     6133\n",
       "4     7515\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_fold = 5\n",
    "# Fold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "# Fold = StratifiedGroupKFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "Fold = GroupKFold(n_splits=n_fold)\n",
    "\n",
    "base_train['fold'] = -1\n",
    "# for n, (train_index, val_index) in enumerate(Fold.split(train, train['region'], groups=train['Language'])):\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(base_train, base_train['Pos'], groups=base_train['Language'])):\n",
    "    base_train.loc[val_index, 'fold'] = int(n)\n",
    "    \n",
    "base_train['fold'] = base_train['fold'].astype(int)\n",
    "\n",
    "display(base_train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b0cabda-60df-4c10-bb88-93fcd7cddafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bam', 'bbj', 'ewe', 'fon', 'hau', 'ibo', 'kin', 'lug', 'mos',\n",
       "       'nya', 'pcm', 'sna', 'swa', 'twi', 'wol', 'xho', 'yor', 'zul',\n",
       "       'en', 'fr', 'eng-ron-wol-sna', 'ar', 'af', 'luo', 'tsn'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.Language.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6f2731d-47d6-4b01-814b-bd5a8d97bf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_train['fold'] = -1\n",
    "base_train.loc[base_train.Language.isin(['sna', 'wol']), 'fold'] = -1\n",
    "# base_train.loc[base_train.Language.isin(['sna', 'bam', 'pcm', 'yor', 'wol']), 'fold'] = -1\n",
    "\n",
    "train_ner = pd.concat([base_train, add_train]).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef6675b1-ada8-436b-829d-7d24d62d9382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Make anibodi wey dey call say make dem impeach...</td>\n",
       "      <td>VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...</td>\n",
       "      <td>pcm</td>\n",
       "      <td>English-Creole</td>\n",
       "      <td>West</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وزير كورى : حل القضية النووية مع كوريا الشمالي...</td>\n",
       "      <td>NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mano nyalo konyi ahinya e yudo tich .</td>\n",
       "      <td>PRON VERB VERB ADV ADP VERB NOUN PUNCT</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Son nom d' espèce , composé de loyalti et de l...</td>\n",
       "      <td>DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les lemmings sont des êtres expressifs , capab...</td>\n",
       "      <td>DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Make anibodi wey dey call say make dem impeach...   \n",
       "1  وزير كورى : حل القضية النووية مع كوريا الشمالي...   \n",
       "2              Mano nyalo konyi ahinya e yudo tich .   \n",
       "3  Son nom d' espèce , composé de loyalti et de l...   \n",
       "4  Les lemmings sont des êtres expressifs , capab...   \n",
       "\n",
       "                                                 Pos Language          family  \\\n",
       "0  VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...      pcm  English-Creole   \n",
       "1  NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...       ar              ar   \n",
       "2             PRON VERB VERB ADV ADP VERB NOUN PUNCT      luo             luo   \n",
       "3  DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...       fr              fr   \n",
       "4  DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...       fr              fr   \n",
       "\n",
       "  region  fold  \n",
       "0   West     0  \n",
       "1     ar    -1  \n",
       "2    luo    -1  \n",
       "3     fr    -1  \n",
       "4     fr    -1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7a768de-2e21-4900-b470-ea68408a58ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th>Language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">-1</th>\n",
       "      <th>af</th>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "      <td>7534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "      <td>15532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eng-ron-wol-sna</th>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "      <td>13120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "      <td>16339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luo</th>\n",
       "      <td>7244</td>\n",
       "      <td>7244</td>\n",
       "      <td>7244</td>\n",
       "      <td>7244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sna</th>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "      <td>1492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tsn</th>\n",
       "      <td>4936</td>\n",
       "      <td>4936</td>\n",
       "      <td>4936</td>\n",
       "      <td>4936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wol</th>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>pcm</th>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "      <td>10338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>bam</th>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "      <td>2481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bbj</th>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nya</th>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zul</th>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2</th>\n",
       "      <th>ewe</th>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "      <td>1453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hau</th>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mos</th>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yor</th>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "      <td>2096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>fon</th>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kin</th>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">4</th>\n",
       "      <th>ibo</th>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lug</th>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swa</th>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "      <td>1383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twi</th>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "      <td>1567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xho</th>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Word    Pos  family  region\n",
       "fold Language                                     \n",
       "-1   af                1899   1899    1899    1899\n",
       "     ar                7534   7534    7534    7534\n",
       "     en               15532  15532   15532   15532\n",
       "     eng-ron-wol-sna  13120  13120   13120   13120\n",
       "     fr               16339  16339   16339   16339\n",
       "     luo               7244   7244    7244    7244\n",
       "     sna               1492   1492    1492    1492\n",
       "     tsn               4936   4936    4936    4936\n",
       "     wol               1563   1563    1563    1563\n",
       " 0   pcm              10338  10338   10338   10338\n",
       " 1   bam               2481   2481    2481    2481\n",
       "     bbj               1484   1484    1484    1484\n",
       "     nya               1440   1440    1440    1440\n",
       "     zul               1503   1503    1503    1503\n",
       " 2   ewe               1453   1453    1453    1453\n",
       "     hau               1484   1484    1484    1484\n",
       "     mos               1508   1508    1508    1508\n",
       "     yor               2096   2096    2096    2096\n",
       " 3   fon               1617   1617    1617    1617\n",
       "     kin               1461   1461    1461    1461\n",
       " 4   ibo               1603   1603    1603    1603\n",
       "     lug               1461   1461    1461    1461\n",
       "     swa               1383   1383    1383    1383\n",
       "     twi               1567   1567    1567    1567\n",
       "     xho               1501   1501    1501    1501"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.groupby(['fold', 'Language']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c4eea-ebba-4afe-8bde-2f37646bfb13",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b46cb191-2de4-4698-ace9-95cabd8475a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def paralellize(fct, data, verbose=0, with_tqdm=True):\n",
    "    fn = map(delayed(fct), data)\n",
    "    if with_tqdm:\n",
    "        fn = tqdm(fn, total=len(data))\n",
    "    return Parallel(n_jobs=-1, verbose=verbose, backend=\"multiprocessing\")(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccd222f9-e3e1-4398-994b-33637be80614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self._time = 0\n",
    "\n",
    "    def start(self):\n",
    "        self._time = time()\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        return (time() - self._time) / 60\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de588c-aabf-4134-b3a3-c208e25b642b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8677ddf8-cd35-411b-bb67-397c2f501348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Language</th>\n",
       "      <th>family</th>\n",
       "      <th>region</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Make anibodi wey dey call say make dem impeach...</td>\n",
       "      <td>VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...</td>\n",
       "      <td>pcm</td>\n",
       "      <td>English-Creole</td>\n",
       "      <td>West</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>وزير كورى : حل القضية النووية مع كوريا الشمالي...</td>\n",
       "      <td>NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>ar</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mano nyalo konyi ahinya e yudo tich .</td>\n",
       "      <td>PRON VERB VERB ADV ADP VERB NOUN PUNCT</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>luo</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Son nom d' espèce , composé de loyalti et de l...</td>\n",
       "      <td>DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les lemmings sont des êtres expressifs , capab...</td>\n",
       "      <td>DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>fr</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Word  \\\n",
       "0  Make anibodi wey dey call say make dem impeach...   \n",
       "1  وزير كورى : حل القضية النووية مع كوريا الشمالي...   \n",
       "2              Mano nyalo konyi ahinya e yudo tich .   \n",
       "3  Son nom d' espèce , composé de loyalti et de l...   \n",
       "4  Les lemmings sont des êtres expressifs , capab...   \n",
       "\n",
       "                                                 Pos Language          family  \\\n",
       "0  VERB PRON PRON AUX VERB SCONJ VERB PRON VERB D...      pcm  English-Creole   \n",
       "1  NOUN ADJ PUNCT NOUN NOUN ADJ ADP X ADJ VERB SC...       ar              ar   \n",
       "2             PRON VERB VERB ADV ADP VERB NOUN PUNCT      luo             luo   \n",
       "3  DET NOUN ADP NOUN PUNCT VERB ADP PROPN CCONJ A...       fr              fr   \n",
       "4  DET NOUN AUX DET NOUN ADJ PUNCT ADJ ADP NOUN C...       fr              fr   \n",
       "\n",
       "  region  fold  \n",
       "0   West     0  \n",
       "1     ar    -1  \n",
       "2    luo    -1  \n",
       "3     fr    -1  \n",
       "4     fr    -1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ner.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4e4278d-e53b-4675-a97f-1643fac183bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJ': 0,\n",
       " 'ADP': 1,\n",
       " 'ADV': 2,\n",
       " 'AUX': 3,\n",
       " 'CCONJ': 4,\n",
       " 'DET': 5,\n",
       " 'INTJ': 6,\n",
       " 'NOUN': 7,\n",
       " 'NUM': 8,\n",
       " 'PART': 9,\n",
       " 'PRON': 10,\n",
       " 'PROPN': 11,\n",
       " 'PUNCT': 12,\n",
       " 'SCONJ': 13,\n",
       " 'SYM': 14,\n",
       " 'VERB': 15,\n",
       " 'X': 16,\n",
       " 'NAW': 17}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_column_name = 'Word'\n",
    "label_column_name = 'Pos'\n",
    "label_list = sorted(train[label_column_name].unique()) + ['NAW']\n",
    "\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "label_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9478b6bc-ce17-412b-9bac-ef7b581460f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ADJ',\n",
       " 1: 'ADP',\n",
       " 2: 'ADV',\n",
       " 3: 'AUX',\n",
       " 4: 'CCONJ',\n",
       " 5: 'DET',\n",
       " 6: 'INTJ',\n",
       " 7: 'NOUN',\n",
       " 8: 'NUM',\n",
       " 9: 'PART',\n",
       " 10: 'PRON',\n",
       " 11: 'PROPN',\n",
       " 12: 'PUNCT',\n",
       " 13: 'SCONJ',\n",
       " 14: 'SYM',\n",
       " 15: 'VERB',\n",
       " 16: 'X',\n",
       " 17: 'NAW'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label = {v:k for k,v in label_to_id.items()}\n",
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a78a58e7-22ef-49cc-857e-9811cf748e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # model_path = glob(f'./pos-tagging-ner/{model_name.replace(\"/\", \"-\")}/{fold}/checkpoint-*')[0]\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        # model_path,\n",
    "        # config=config,\n",
    "        num_labels=num_labels, id2label=id_to_label, label2id=label_to_id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04218f8f-11fc-429e-b70e-c59931217409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'Davlan/afro-xlmr-base'\n",
    "# model_name = 'Davlan/afro-xlmr-large-61L'\n",
    "# model_name = 'Davlan/afro-xlmr-large-75L'\n",
    "# model_name = 'masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0'\n",
    "model_name = 'google/rembert'\n",
    "# model_name = 'bonadossou/afrolm_active_learning'\n",
    "\n",
    "max_seq_length = 256\n",
    "padding = False\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "tokenizer_name_or_path = model_name\n",
    "if config.model_type in {\"gpt2\", \"roberta\"}:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name_or_path,\n",
    "        use_fast=True,\n",
    "        add_prefix_space=True,\n",
    "    )\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name_or_path,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    \n",
    "# data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, max_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54b5558a-0961-4dc5-ba9e-8f4103accfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(examples):\n",
    "    is_test = examples.get(label_column_name) is None\n",
    "    \n",
    "    for idx in range(len(examples[text_column_name])):\n",
    "        if not is_test:\n",
    "            examples[label_column_name][idx] = examples[label_column_name][idx].split()\n",
    "        examples[text_column_name][idx] = examples[text_column_name][idx].split()\n",
    "        \n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length if not is_test else None,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    \n",
    "    if not is_test:\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[label_column_name]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label_to_id[label[word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(label_to_id['NAW'])\n",
    "                previous_word_idx = word_idx\n",
    "            labels.append(label_ids)\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8f08892-823c-474a-a3f1-43480ec1213a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7192d89000414f2c9734e2737bfbfeea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cee4234b-51b0-4f63-9050-7c226b1596d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l not in [-100, 17]]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    lengths = list(map(len, true_labels))\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l not in [-100, 17]]\n",
    "        for prediction, label, length in zip(predictions, labels, lengths)\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f668b84f-ac49-4378-8cea-b0c9c3ecad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32045/32045 [00:00<00:00, 309426.40it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "words = []\n",
    "\n",
    "last_lang = test.Language.values[0]\n",
    "for i, row in enumerate(tqdm(test.itertuples(), total=len(test))):\n",
    "    words.append(row.Word)\n",
    "    \n",
    "    if row.Word == '.' or i == len(test)-1 or row.Language != last_lang:\n",
    "        sentences.append(' '.join(words))\n",
    "        words = []\n",
    "        \n",
    "    last_lang = row.Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00797992-b49d-47d1-bc47-6cbffc86ccf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b83f816c5fd4d88984381ae30bb2832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1974 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1974\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_dataset = HFDataset.from_pandas(test).remove_columns(column_names=['Id', 'Language', 'Pos'])\n",
    "# test_dataset = HFDataset.from_pandas(test_ner).remove_columns(column_names=['Language', 'family', 'region'])\n",
    "test_dataset = HFDataset.from_pandas(pd.DataFrame({'Word': sentences}))\n",
    "test_dataset = test_dataset.map(\n",
    "    process_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=['Word'],\n",
    ")\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94e34428-b18d-4949-83cc-e625b0ecf5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AWP:\n",
    "    def __init__(self, model, optimizer, *, adv_param='weight',\n",
    "                 adv_lr=0.001, adv_eps=0.001):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.backup = {}\n",
    "\n",
    "    def perturb(self):\n",
    "        \"\"\"\n",
    "        Perturb model parameters for AWP gradient\n",
    "        Call before loss and loss.backward()\n",
    "        \"\"\"\n",
    "        self._save()  # save model parameters\n",
    "        self._attack_step()  # perturb weights\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                grad = self.optimizer.state[param]['exp_avg']\n",
    "                norm_grad = torch.norm(grad)\n",
    "                norm_data = torch.norm(param.detach())\n",
    "\n",
    "                if norm_grad != 0 and not torch.isnan(norm_grad):\n",
    "                    # Set lower and upper limit in change\n",
    "                    limit_eps = self.adv_eps * param.detach().abs()\n",
    "                    param_min = param.data - limit_eps\n",
    "                    param_max = param.data + limit_eps\n",
    "\n",
    "                    # Perturb along gradient\n",
    "                    # w += (adv_lr * |w| / |grad|) * grad\n",
    "                    param.data.add_(grad, alpha=(self.adv_lr * (norm_data + e) / (norm_grad + e)))\n",
    "\n",
    "                    # Apply the limit to the change\n",
    "                    param.data.clamp_(param_min, param_max)\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.clone().detach()\n",
    "                else:\n",
    "                    self.backup[name].copy_(param.data)\n",
    "\n",
    "    def restore(self):\n",
    "        \"\"\"\n",
    "        Restore model parameter to correct position; AWP do not perturbe weights, it perturb gradients\n",
    "        Call after loss.backward(), before optimizer.step()\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data.copy_(self.backup[name])\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.awp = AWP(self.model, self.optimizer, adv_lr=0.001, adv_eps=0.001)\n",
    "    \n",
    "    def training_step(self, model: nn.Module, inputs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            `torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        # if is_sagemaker_mp_enabled():\n",
    "        #     loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n",
    "        #     return loss_mb.reduce_mean().detach().to(self.args.device)\n",
    "        \n",
    "        self.awp.perturb() # \n",
    "        \n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        if self.do_grad_scaling:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            self.accelerator.backward(loss)\n",
    "            \n",
    "        self.awp.restore()\n",
    "\n",
    "        return loss.detach() / self.args.gradient_accumulation_steps\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            if is_peft_available() and isinstance(model, PeftModel):\n",
    "                model_name = unwrap_model(model.base_model)._get_name()\n",
    "            else:\n",
    "                model_name = unwrap_model(model)._get_name()\n",
    "            if model_name in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n",
    "                loss = self.label_smoother(outputs, labels, shift_labels=True)\n",
    "            else:\n",
    "                loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
    "                raise ValueError(\n",
    "                    \"The model did not return a loss from the inputs, only the following keys: \"\n",
    "                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
    "                )\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c3ab5b5-1122-4b5a-bf90-b3c07e34c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(fold):\n",
    "    raw_dataset = DatasetDict({\n",
    "        'train': HFDataset.from_pandas(train_ner[train_ner.fold != fold]),\n",
    "        'validation': HFDataset.from_pandas(train_ner[train_ner.fold == fold])\n",
    "    }).remove_columns(column_names=['fold', '__index_level_0__', 'Language', 'family', 'region'])\n",
    "\n",
    "    train_dataset = raw_dataset[\"train\"]\n",
    "    column_names = train_dataset.column_names\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        process_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "    )\n",
    "    if max_seq_length is not None:\n",
    "        train_dataset = train_dataset.filter(\n",
    "            lambda example: len(example['input_ids']) <= max_seq_length\n",
    "        )\n",
    "\n",
    "    eval_dataset = raw_dataset['validation']\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        process_dataset,\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "    )\n",
    "    if max_seq_length is not None:\n",
    "        eval_dataset = eval_dataset.filter(\n",
    "            lambda example: len(example['input_ids']) <= max_seq_length\n",
    "        )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'pos-tagging-ner/{model_name.replace(\"/\", \"-\")}/{fold}',\n",
    "        learning_rate=5e-5,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        gradient_accumulation_steps=1,\n",
    "        group_by_length=True,\n",
    "        overwrite_output_dir=True,\n",
    "        warmup_steps=0.15,\n",
    "        num_train_epochs=5,\n",
    "        lr_scheduler_type ='cosine',\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=1,\n",
    "        fp16=True,\n",
    "        report_to='none'\n",
    "    )\n",
    "\n",
    "    model = load_model()\n",
    "\n",
    "    trainer = Trainer(\n",
    "    # trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    results = trainer.evaluate(eval_dataset)\n",
    "    \n",
    "    test_pos_ids = trainer.predict(test_dataset)\n",
    "    \n",
    "    return results['eval_accuracy'], test_pos_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65bf22a7-cfe7-454b-a739-20bfc29d1430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c63bb1328cf4d3bb457a8c3de8bf214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/97131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff768bc8c394e668a70b2d610dea893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/97131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ea19462e924b9ab333a1e66bdd2b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e483bdf69b4c6ebb2fd63a86d2c96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RemBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5690797151020079\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a87d8e2d0694cfbaa3d9b1153ffb21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/97498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550aa58dba7c408d92f26886ffa5afbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/97498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cf734da7df4aada659c4a79c84d9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2056597dbd475da2807f3522ee961e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6031341201993853\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a302b7c44efe4a39944e39ac69d8f65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/100961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b133e31f7c4fa0a0c1d70409211abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca327022b9a4014b4b7f4569b246862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd2feaab0c24e66bce4b3e6d45b2975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3078 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6271357913669064\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80bd7980ffc4c9cb23832164ba70b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/96524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fa5518aaa446cb889a62045368fe0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/96524 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203f88dbf4404632a8a68c7e3473c52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32818b9053e4333a2e8a32a8ed36deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6577001294139397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "all_preds = []\n",
    "\n",
    "for fold in [1, 2, 3, 4]:\n",
    "    score, fold_pred = train_fold(fold)\n",
    "    \n",
    "    scores.append(score)\n",
    "    all_preds.append(fold_pred)\n",
    "    \n",
    "    print(score)\n",
    "    print()\n",
    "    \n",
    "avg_score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b509e41f-8169-484e-af92-3853030b1f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5690797151020079,\n",
       " 0.6031341201993853,\n",
       " 0.6271357913669064,\n",
       " 0.6577001294139397]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f41ddd1d-b6b7-40fb-b1f2-7f5b58e361ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6142624390205598"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5423319-fc1d-4e9c-ab53-0eb0122803dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 168, 18)\n"
     ]
    }
   ],
   "source": [
    "test_pos_ids = np.mean([p.predictions for p in all_preds], axis=0)\n",
    "print(test_pos_ids.shape)\n",
    "test_pos_ids = test_pos_ids.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45ce2daa-b304-4065-8ab6-40fe16f91f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pos = []\n",
    "for pos_ids, sentence in zip(test_pos_ids, sentences):\n",
    "    length = len(sentence.split(' '))\n",
    "    final_pos.extend(\n",
    "        list(map(id_to_label.get, [x for x in pos_ids if x not in [-100, 17]]))[:length]\n",
    "        # list(map(id_to_label.get, [x for x in pos_ids if x not in [-100, 17]]))[1:length+1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab63a58c-f2ec-43d8-8f79-fbc90939c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Pos'] = final_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55dd1deb-7091-4db3-8cf9-8cd0147a6346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOUN     6797\n",
       "VERB     4979\n",
       "ADP      4414\n",
       "PROPN    3095\n",
       "PUNCT    2953\n",
       "AUX      2531\n",
       "PRON     1784\n",
       "SCONJ    1175\n",
       "DET       908\n",
       "ADV       808\n",
       "CCONJ     719\n",
       "PART      708\n",
       "ADJ       555\n",
       "NUM       550\n",
       "INTJ       27\n",
       "X          27\n",
       "SYM        15\n",
       "Name: Pos, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Pos.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ff57974-cd0b-47c0-9356-e08ceb7af5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Word</th>\n",
       "      <th>Language</th>\n",
       "      <th>Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Id00qog2f11n_0</td>\n",
       "      <td>Ne</td>\n",
       "      <td>luo</td>\n",
       "      <td>AUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Id00qog2f11n_1</td>\n",
       "      <td>otim</td>\n",
       "      <td>luo</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Id00qog2f11n_2</td>\n",
       "      <td>penj</td>\n",
       "      <td>luo</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Id00qog2f11n_3</td>\n",
       "      <td>e</td>\n",
       "      <td>luo</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Id00qog2f11n_4</td>\n",
       "      <td>kind</td>\n",
       "      <td>luo</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id  Word Language   Pos\n",
       "0  Id00qog2f11n_0    Ne      luo   AUX\n",
       "1  Id00qog2f11n_1  otim      luo  VERB\n",
       "2  Id00qog2f11n_2  penj      luo  NOUN\n",
       "3  Id00qog2f11n_3     e      luo   ADP\n",
       "4  Id00qog2f11n_4  kind      luo  NOUN"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd03bca7-647e-41ce-84c7-6ffbf1096b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['Id', 'Pos']].to_csv(f'submissions/ps-round1-pos-ner-{model_name.split(\"/\")[-1]}-{avg_score:.3f}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c85bbd-29a8-40f8-933b-5e45111e84be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test[test.Pos == 'SYM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b6f72-85c4-4881-8319-c59d5dd08de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
